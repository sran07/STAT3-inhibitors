{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre deepchem\n",
        "!pip install rdkit-pypi\n",
        "from rdkit import rdBase\n",
        "from rdkit import RDConfig\n",
        "!pip install --pre deepchem\n",
        "!pip install rdkit-pypi\n",
        "!pip install rdkit-pypi\n",
        "!pip install mordred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6AZhg2bJRgB",
        "outputId": "e3842f28-69d4-4b3d-b076-b32ba917ef48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[K     |████████████████████████████████| 693 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.2.0)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2022.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.3 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.0.2)\n",
            "Requirement already satisfied: scipy<1.9 in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepchem) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit->deepchem) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Installing collected packages: rdkit, deepchem\n",
            "Successfully installed deepchem-2.7.1 rdkit-2022.9.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.3 MB 81.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.9.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: scipy<1.9 in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.7.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.3.5)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.8/dist-packages (from deepchem) (2022.9.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepchem) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit->deepchem) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.8/dist-packages (2022.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.8/dist-packages (2022.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mordred\n",
            "  Downloading mordred-1.2.0.tar.gz (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.* in /usr/local/lib/python3.8/dist-packages (from mordred) (1.15.0)\n",
            "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.8/dist-packages (from mordred) (1.21.6)\n",
            "Requirement already satisfied: networkx==2.* in /usr/local/lib/python3.8/dist-packages (from mordred) (2.8.8)\n",
            "Building wheels for collected packages: mordred\n",
            "  Building wheel for mordred (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mordred: filename=mordred-1.2.0-py3-none-any.whl size=176721 sha256=66dc2a134e6f4460ad161383b306126e80ad9d70159acf4435ca45dc0bb29690\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/88/41/5d873c9b55dc7479f0b9951c2161d7b09be193e7228ea27309\n",
            "Successfully built mordred\n",
            "Installing collected packages: mordred\n",
            "Successfully installed mordred-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre deepchem\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBRBF9ktJUP5",
        "outputId": "e7931c3c-1272-42db-d30d-bd9e4a335a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.3.5)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.8/dist-packages (from deepchem) (2022.9.3)\n",
            "Requirement already satisfied: scipy<1.9 in /usr/local/lib/python3.8/dist-packages (from deepchem) (1.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepchem) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit->deepchem) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.8/dist-packages (2022.9.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import deepchem as dc\n",
        "dc.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "-B006QFRJUS6",
        "outputId": "24170528-34a2-4b8c-9faf-6643fbad59d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.8/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.7.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit.Chem import AllChem\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mordred import Calculator, descriptors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Input, Activation\n",
        "from keras.layers import BatchNormalization,Add\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "from keras import callbacks\n",
        "from keras import backend as K\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
        "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n"
      ],
      "metadata": {
        "id": "-HZ5kJSeJ9lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/dataset_covid/file1.csv')"
      ],
      "metadata": {
        "id": "mt-sS1Oy1Dgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def canonical_smiles(smiles):\n",
        "    mols = [Chem.MolFromSmiles(smi) for smi in smiles] \n",
        "    smiles = [Chem.MolToSmiles(mol) for mol in mols]\n",
        "    return smiles"
      ],
      "metadata": {
        "id": "qBQgY6iGJ9r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "\n",
        "Chem.MolFromSmiles('C=CCC')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "l4_pvs-tJ9ux",
        "outputId": "6cde3895-6b51-4db6-984e-7b7c013df0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x7f75f5426cf0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Canon_SMILES = canonical_smiles(dataset.SMILE)"
      ],
      "metadata": {
        "id": "lc_IvAKvJUVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['SMILE'] = Canon_SMILES\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "iBtRrikTJUYg",
        "outputId": "5c96ec55-38c2-4951-f677-eff030510348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  SMILE     logP       qed  \\\n",
              "0                 CCCS(=O)(=O)c1ccccc1C(=O)Nc1nnc(CC)s1  2.53650  0.871865   \n",
              "1        CNC(=O)[C@H](C)CN(C)Cc1cc(=O)n2cccc(C)c2[nH+]1  0.23592  0.880917   \n",
              "2                       COCCNC(=O)/C(C#N)=C/c1cccc(O)c1  1.06178  0.468379   \n",
              "3            CNC(=O)CN1c2ccccc2C(=O)N(C)[C@H]1c1ccccc1O  1.72900  0.908245   \n",
              "4           CC(=O)N[C@H](C)C(=O)Nc1ccc(Sc2nncs2)c(Cl)c1  2.80580  0.854635   \n",
              "...                                                 ...      ...       ...   \n",
              "2585           C[C@H]1CCC(=O)[C@H]([C@@H]2CCC[NH2+]2)C1  0.71750  0.647642   \n",
              "2586             COc1cccc(Cn2nc3c(N4CCOCC4)nccn3c2=O)c1  0.78450  0.709817   \n",
              "2587    COc1ccc2c(CN(C)[C@@H](C)Cc3ccccc3OC)cc(=O)oc2c1  3.87320  0.574334   \n",
              "2588  N#Cc1ccc(Cl)cc1NC(=O)CN1C(=O)NC(c2ccccc2)(c2cc...  3.64578  0.570734   \n",
              "2589  Cn1c(SCC(=O)NC(N)=O)nnc1-c1cc2c(s1)CC[C@@H](C(...  2.98160  0.751048   \n",
              "\n",
              "           SAS  label  \n",
              "0     2.227853      1  \n",
              "1     4.069782      1  \n",
              "2     2.213129      1  \n",
              "3     2.856964      1  \n",
              "4     2.884518      1  \n",
              "...        ...    ...  \n",
              "2585  4.986801      0  \n",
              "2586  2.366587      0  \n",
              "2587  2.660191      0  \n",
              "2588  2.364954      0  \n",
              "2589  3.183087      0  \n",
              "\n",
              "[2590 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-738a9561-2b78-48de-8cb6-959dddd75854\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMILE</th>\n",
              "      <th>logP</th>\n",
              "      <th>qed</th>\n",
              "      <th>SAS</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CCCS(=O)(=O)c1ccccc1C(=O)Nc1nnc(CC)s1</td>\n",
              "      <td>2.53650</td>\n",
              "      <td>0.871865</td>\n",
              "      <td>2.227853</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CNC(=O)[C@H](C)CN(C)Cc1cc(=O)n2cccc(C)c2[nH+]1</td>\n",
              "      <td>0.23592</td>\n",
              "      <td>0.880917</td>\n",
              "      <td>4.069782</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COCCNC(=O)/C(C#N)=C/c1cccc(O)c1</td>\n",
              "      <td>1.06178</td>\n",
              "      <td>0.468379</td>\n",
              "      <td>2.213129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CNC(=O)CN1c2ccccc2C(=O)N(C)[C@H]1c1ccccc1O</td>\n",
              "      <td>1.72900</td>\n",
              "      <td>0.908245</td>\n",
              "      <td>2.856964</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC(=O)N[C@H](C)C(=O)Nc1ccc(Sc2nncs2)c(Cl)c1</td>\n",
              "      <td>2.80580</td>\n",
              "      <td>0.854635</td>\n",
              "      <td>2.884518</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2585</th>\n",
              "      <td>C[C@H]1CCC(=O)[C@H]([C@@H]2CCC[NH2+]2)C1</td>\n",
              "      <td>0.71750</td>\n",
              "      <td>0.647642</td>\n",
              "      <td>4.986801</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2586</th>\n",
              "      <td>COc1cccc(Cn2nc3c(N4CCOCC4)nccn3c2=O)c1</td>\n",
              "      <td>0.78450</td>\n",
              "      <td>0.709817</td>\n",
              "      <td>2.366587</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2587</th>\n",
              "      <td>COc1ccc2c(CN(C)[C@@H](C)Cc3ccccc3OC)cc(=O)oc2c1</td>\n",
              "      <td>3.87320</td>\n",
              "      <td>0.574334</td>\n",
              "      <td>2.660191</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2588</th>\n",
              "      <td>N#Cc1ccc(Cl)cc1NC(=O)CN1C(=O)NC(c2ccccc2)(c2cc...</td>\n",
              "      <td>3.64578</td>\n",
              "      <td>0.570734</td>\n",
              "      <td>2.364954</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2589</th>\n",
              "      <td>Cn1c(SCC(=O)NC(N)=O)nnc1-c1cc2c(s1)CC[C@@H](C(...</td>\n",
              "      <td>2.98160</td>\n",
              "      <td>0.751048</td>\n",
              "      <td>3.183087</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2590 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-738a9561-2b78-48de-8cb6-959dddd75854')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-738a9561-2b78-48de-8cb6-959dddd75854 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-738a9561-2b78-48de-8cb6-959dddd75854');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates_smiles = dataset[dataset['SMILE'].duplicated()]['SMILE'].values\n",
        "len(duplicates_smiles)\n",
        "# Create a list for duplicate smiles\n",
        "dataset[dataset['SMILE'].isin(duplicates_smiles)].sort_values(by=['SMILE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "DK-cozZyJUbo",
        "outputId": "fee264a5-e2de-4423-9b6e-38ea5bcd68e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [SMILE, logP, qed, SAS, label]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7da22db0-09cc-41d2-9c94-f4f4efcc39e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMILE</th>\n",
              "      <th>logP</th>\n",
              "      <th>qed</th>\n",
              "      <th>SAS</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7da22db0-09cc-41d2-9c94-f4f4efcc39e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7da22db0-09cc-41d2-9c94-f4f4efcc39e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7da22db0-09cc-41d2-9c94-f4f4efcc39e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_new = dataset.drop_duplicates(subset=['SMILE'])\n",
        "\n",
        "def RDkit_descriptors(smiles):\n",
        "    mols = [Chem.MolFromSmiles(i) for i in smiles] \n",
        "    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])\n",
        "    desc_names = calc.GetDescriptorNames()\n",
        "    \n",
        "    Mol_descriptors =[]\n",
        "    for mol in mols:\n",
        "        # add hydrogens to molecules\n",
        "        mol=Chem.AddHs(mol)\n",
        "        # Calculate all 200 descriptors for each molecule\n",
        "        descriptors = calc.CalcDescriptors(mol)\n",
        "        Mol_descriptors.append(descriptors)\n",
        "    return Mol_descriptors,desc_names \n",
        "\n",
        "# Function call\n",
        "Mol_descriptors,desc_names = RDkit_descriptors(dataset_new['SMILE'])"
      ],
      "metadata": {
        "id": "0uocz9iyKRVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_200_descriptors = pd.DataFrame(Mol_descriptors,columns=desc_names)\n",
        "df_with_200_descriptors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "oioIe08zKRX_",
        "outputId": "6cbda1d2-06e7-4324-9bd5-d42ebd80cdc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      MaxEStateIndex  MinEStateIndex  MaxAbsEStateIndex  MinAbsEStateIndex  \\\n",
              "0          13.331848       -6.110897          13.331848           0.052180   \n",
              "1          13.233421       -4.751706          13.233421           0.055204   \n",
              "2          12.537473       -3.809998          12.537473           0.776355   \n",
              "3          13.818979       -4.217433          13.818979           0.407775   \n",
              "4          12.970445       -3.973922          12.970445           0.121629   \n",
              "...              ...             ...                ...                ...   \n",
              "2585       13.186895       -4.698958          13.186895           2.674537   \n",
              "2586       13.507964       -3.667948          13.507964           0.113899   \n",
              "2587       12.902296       -4.727469          12.902296           1.021646   \n",
              "2588       14.560487       -4.161604          14.560487           0.519680   \n",
              "2589       12.542670       -4.711017          12.542670           0.056368   \n",
              "\n",
              "           qed    MolWt  HeavyAtomMolWt  ExactMolWt  NumValenceElectrons  \\\n",
              "0     0.873366  339.442         322.306  339.071133                  118   \n",
              "1     0.859953  303.386         280.202  303.181552                  118   \n",
              "2     0.462133  246.266         232.154  246.100442                   94   \n",
              "3     0.901310  325.368         306.216  325.142641                  124   \n",
              "4     0.860138  356.860         343.756  356.016845                  116   \n",
              "...        ...      ...             ...         ...                  ...   \n",
              "2585  0.636654  182.287         162.127  182.153941                   74   \n",
              "2586  0.698327  341.371         322.219  341.148789                  130   \n",
              "2587  0.593457  367.445         342.245  367.178358                  142   \n",
              "2588  0.586170  444.878         427.742  444.098918                  158   \n",
              "2589  0.758605  407.565         382.365  407.144967                  146   \n",
              "\n",
              "      NumRadicalElectrons  ...  fr_sulfide  fr_sulfonamd  fr_sulfone  \\\n",
              "0                       0  ...           0             0           1   \n",
              "1                       0  ...           0             0           0   \n",
              "2                       0  ...           0             0           0   \n",
              "3                       0  ...           0             0           0   \n",
              "4                       0  ...           0             0           0   \n",
              "...                   ...  ...         ...           ...         ...   \n",
              "2585                    0  ...           0             0           0   \n",
              "2586                    0  ...           0             0           0   \n",
              "2587                    0  ...           0             0           0   \n",
              "2588                    0  ...           0             0           0   \n",
              "2589                    0  ...           1             0           0   \n",
              "\n",
              "      fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  \\\n",
              "0                     0             0            0            0             0   \n",
              "1                     0             0            0            0             0   \n",
              "2                     0             0            0            0             0   \n",
              "3                     0             0            0            0             0   \n",
              "4                     0             0            0            0             0   \n",
              "...                 ...           ...          ...          ...           ...   \n",
              "2585                  0             0            0            0             0   \n",
              "2586                  0             0            0            0             0   \n",
              "2587                  0             0            0            0             0   \n",
              "2588                  0             0            0            0             0   \n",
              "2589                  0             0            0            0             1   \n",
              "\n",
              "      fr_unbrch_alkane  fr_urea  \n",
              "0                    0        0  \n",
              "1                    0        0  \n",
              "2                    0        0  \n",
              "3                    0        0  \n",
              "4                    0        0  \n",
              "...                ...      ...  \n",
              "2585                 0        0  \n",
              "2586                 0        0  \n",
              "2587                 0        0  \n",
              "2588                 0        1  \n",
              "2589                 0        1  \n",
              "\n",
              "[2590 rows x 208 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2346ba8b-979f-48b9-af56-4a24088512d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MaxEStateIndex</th>\n",
              "      <th>MinEStateIndex</th>\n",
              "      <th>MaxAbsEStateIndex</th>\n",
              "      <th>MinAbsEStateIndex</th>\n",
              "      <th>qed</th>\n",
              "      <th>MolWt</th>\n",
              "      <th>HeavyAtomMolWt</th>\n",
              "      <th>ExactMolWt</th>\n",
              "      <th>NumValenceElectrons</th>\n",
              "      <th>NumRadicalElectrons</th>\n",
              "      <th>...</th>\n",
              "      <th>fr_sulfide</th>\n",
              "      <th>fr_sulfonamd</th>\n",
              "      <th>fr_sulfone</th>\n",
              "      <th>fr_term_acetylene</th>\n",
              "      <th>fr_tetrazole</th>\n",
              "      <th>fr_thiazole</th>\n",
              "      <th>fr_thiocyan</th>\n",
              "      <th>fr_thiophene</th>\n",
              "      <th>fr_unbrch_alkane</th>\n",
              "      <th>fr_urea</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13.331848</td>\n",
              "      <td>-6.110897</td>\n",
              "      <td>13.331848</td>\n",
              "      <td>0.052180</td>\n",
              "      <td>0.873366</td>\n",
              "      <td>339.442</td>\n",
              "      <td>322.306</td>\n",
              "      <td>339.071133</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.233421</td>\n",
              "      <td>-4.751706</td>\n",
              "      <td>13.233421</td>\n",
              "      <td>0.055204</td>\n",
              "      <td>0.859953</td>\n",
              "      <td>303.386</td>\n",
              "      <td>280.202</td>\n",
              "      <td>303.181552</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.537473</td>\n",
              "      <td>-3.809998</td>\n",
              "      <td>12.537473</td>\n",
              "      <td>0.776355</td>\n",
              "      <td>0.462133</td>\n",
              "      <td>246.266</td>\n",
              "      <td>232.154</td>\n",
              "      <td>246.100442</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.818979</td>\n",
              "      <td>-4.217433</td>\n",
              "      <td>13.818979</td>\n",
              "      <td>0.407775</td>\n",
              "      <td>0.901310</td>\n",
              "      <td>325.368</td>\n",
              "      <td>306.216</td>\n",
              "      <td>325.142641</td>\n",
              "      <td>124</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12.970445</td>\n",
              "      <td>-3.973922</td>\n",
              "      <td>12.970445</td>\n",
              "      <td>0.121629</td>\n",
              "      <td>0.860138</td>\n",
              "      <td>356.860</td>\n",
              "      <td>343.756</td>\n",
              "      <td>356.016845</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2585</th>\n",
              "      <td>13.186895</td>\n",
              "      <td>-4.698958</td>\n",
              "      <td>13.186895</td>\n",
              "      <td>2.674537</td>\n",
              "      <td>0.636654</td>\n",
              "      <td>182.287</td>\n",
              "      <td>162.127</td>\n",
              "      <td>182.153941</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2586</th>\n",
              "      <td>13.507964</td>\n",
              "      <td>-3.667948</td>\n",
              "      <td>13.507964</td>\n",
              "      <td>0.113899</td>\n",
              "      <td>0.698327</td>\n",
              "      <td>341.371</td>\n",
              "      <td>322.219</td>\n",
              "      <td>341.148789</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2587</th>\n",
              "      <td>12.902296</td>\n",
              "      <td>-4.727469</td>\n",
              "      <td>12.902296</td>\n",
              "      <td>1.021646</td>\n",
              "      <td>0.593457</td>\n",
              "      <td>367.445</td>\n",
              "      <td>342.245</td>\n",
              "      <td>367.178358</td>\n",
              "      <td>142</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2588</th>\n",
              "      <td>14.560487</td>\n",
              "      <td>-4.161604</td>\n",
              "      <td>14.560487</td>\n",
              "      <td>0.519680</td>\n",
              "      <td>0.586170</td>\n",
              "      <td>444.878</td>\n",
              "      <td>427.742</td>\n",
              "      <td>444.098918</td>\n",
              "      <td>158</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2589</th>\n",
              "      <td>12.542670</td>\n",
              "      <td>-4.711017</td>\n",
              "      <td>12.542670</td>\n",
              "      <td>0.056368</td>\n",
              "      <td>0.758605</td>\n",
              "      <td>407.565</td>\n",
              "      <td>382.365</td>\n",
              "      <td>407.144967</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2590 rows × 208 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2346ba8b-979f-48b9-af56-4a24088512d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2346ba8b-979f-48b9-af56-4a24088512d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2346ba8b-979f-48b9-af56-4a24088512d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def morgan_fpts(data):\n",
        "    Morgan_fpts = []\n",
        "    for i in data:\n",
        "        mol = Chem.MolFromSmiles(i) \n",
        "        fpts =  AllChem.GetMorganFingerprintAsBitVect(mol,2,2048)\n",
        "        mfpts = np.array(fpts)\n",
        "        Morgan_fpts.append(mfpts)  \n",
        "    return np.array(Morgan_fpts)\n",
        "   "
      ],
      "metadata": {
        "id": "xofw2ksjKRaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Morgan_fpts = morgan_fpts(dataset_new['SMILE'])"
      ],
      "metadata": {
        "id": "xoJDeB2XKX-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Morgan_fingerprints = pd.DataFrame(Morgan_fpts,columns=['Col_{}'.format(i) for i in range(Morgan_fpts.shape[1])])\n",
        "Morgan_fingerprints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "QVmi86-_KYBf",
        "outputId": "2e1173da-1a57-47e1-e027-72c9452ab761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Col_0  Col_1  Col_2  Col_3  Col_4  Col_5  Col_6  Col_7  Col_8  Col_9  \\\n",
              "0         0      0      0      0      0      0      0      0      0      0   \n",
              "1         0      1      0      0      0      0      0      0      0      0   \n",
              "2         0      0      0      0      0      0      0      0      0      0   \n",
              "3         0      0      0      0      0      0      0      0      0      0   \n",
              "4         0      1      0      0      0      0      0      0      0      0   \n",
              "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "2585      0      0      0      1      0      0      0      0      0      0   \n",
              "2586      0      0      0      0      0      0      0      0      0      0   \n",
              "2587      0      1      0      0      0      0      0      0      0      0   \n",
              "2588      0      0      0      0      0      0      0      0      0      0   \n",
              "2589      0      0      0      0      0      0      0      0      1      0   \n",
              "\n",
              "      ...  Col_2038  Col_2039  Col_2040  Col_2041  Col_2042  Col_2043  \\\n",
              "0     ...         0         0         0         0         0         0   \n",
              "1     ...         0         0         0         0         0         0   \n",
              "2     ...         0         0         0         0         0         0   \n",
              "3     ...         0         0         0         0         0         0   \n",
              "4     ...         0         1         0         0         0         0   \n",
              "...   ...       ...       ...       ...       ...       ...       ...   \n",
              "2585  ...         0         0         0         0         0         0   \n",
              "2586  ...         0         0         0         0         0         0   \n",
              "2587  ...         0         0         0         0         0         0   \n",
              "2588  ...         0         0         0         0         0         0   \n",
              "2589  ...         0         0         0         0         0         0   \n",
              "\n",
              "      Col_2044  Col_2045  Col_2046  Col_2047  \n",
              "0            0         0         0         0  \n",
              "1            0         0         0         0  \n",
              "2            0         0         0         0  \n",
              "3            0         0         0         0  \n",
              "4            0         0         0         0  \n",
              "...        ...       ...       ...       ...  \n",
              "2585         0         0         0         0  \n",
              "2586         0         0         0         0  \n",
              "2587         0         0         0         0  \n",
              "2588         0         0         0         0  \n",
              "2589         0         0         0         0  \n",
              "\n",
              "[2590 rows x 2048 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a578c18-fa92-47ad-b015-bd05ee53a80f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Col_0</th>\n",
              "      <th>Col_1</th>\n",
              "      <th>Col_2</th>\n",
              "      <th>Col_3</th>\n",
              "      <th>Col_4</th>\n",
              "      <th>Col_5</th>\n",
              "      <th>Col_6</th>\n",
              "      <th>Col_7</th>\n",
              "      <th>Col_8</th>\n",
              "      <th>Col_9</th>\n",
              "      <th>...</th>\n",
              "      <th>Col_2038</th>\n",
              "      <th>Col_2039</th>\n",
              "      <th>Col_2040</th>\n",
              "      <th>Col_2041</th>\n",
              "      <th>Col_2042</th>\n",
              "      <th>Col_2043</th>\n",
              "      <th>Col_2044</th>\n",
              "      <th>Col_2045</th>\n",
              "      <th>Col_2046</th>\n",
              "      <th>Col_2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2585</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2586</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2587</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2588</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2589</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2590 rows × 2048 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a578c18-fa92-47ad-b015-bd05ee53a80f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a578c18-fa92-47ad-b015-bd05ee53a80f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a578c18-fa92-47ad-b015-bd05ee53a80f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def All_Mordred_descriptors(data):\n",
        "    calc = Calculator(descriptors, ignore_3D=False)\n",
        "    mols = [Chem.MolFromSmiles(smi) for smi in data]\n",
        "    \n",
        "    # pandas df\n",
        "    df = calc.pandas(mols)\n",
        "    return df"
      ],
      "metadata": {
        "id": "rB-nbkQRLtLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mordred_descriptors = All_Mordred_descriptors(dataset_new['SMILE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HLWatuPLtO5",
        "outputId": "4a8726b7-309f-4130-bd46-57be44cd355b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2590/2590 [23:27<00:00,  1.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nwsqqBtJUeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Input, Activation\n",
        "from keras.layers import BatchNormalization,Add\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "from keras import callbacks\n",
        "from keras import backend as K\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
        "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-12-14T07:46:15.015368Z",
          "iopub.execute_input": "2022-12-14T07:46:15.015677Z",
          "iopub.status.idle": "2022-12-14T07:46:17.021282Z",
          "shell.execute_reply.started": "2022-12-14T07:46:15.015627Z",
          "shell.execute_reply": "2022-12-14T07:46:17.020217Z"
        },
        "trusted": true,
        "id": "-f7DOQx-HNz5",
        "outputId": "b8ae9780-40eb-49ec-8734-3a871672efd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Using TensorFlow backend.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct=pd.read_csv('/content/drive/MyDrive/dataset_new/molecule/structures.csv')\n",
        "df_train_sub_charge=pd.read_csv('/content/drive/MyDrive/dataset_new/molecule/mulliken_charges.csv')\n",
        "df_train_sub_tensor=pd.read_csv('/content/drive/MyDrive/dataset_new/molecule/magnetic_shielding_tensors.csv')\n"
      ],
      "metadata": {
        "id": "vOHFU9Y91VQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n"
      ],
      "metadata": {
        "id": "d-BMwZk-HNz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_atom_info(df_1,df_2, atom_idx):\n",
        "    df = pd.merge(df_1, df_2, how = 'left',\n",
        "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
        "                  right_on = ['molecule_name',  'atom_index'])\n",
        "    df = df.drop('atom_index', axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "for atom_idx in [0,1]:\n",
        "    df_train = map_atom_info(df_train,df_struct, atom_idx)\n",
        "    df_train = map_atom_info(df_train,df_train_sub_charge, atom_idx)\n",
        "    df_train = map_atom_info(df_train,df_train_sub_tensor, atom_idx)\n",
        "    df_train = df_train.rename(columns={'atom': f'atom_{atom_idx}',\n",
        "                                        'x': f'x_{atom_idx}',\n",
        "                                        'y': f'y_{atom_idx}',\n",
        "                                        'z': f'z_{atom_idx}',\n",
        "                                        'mulliken_charge': f'charge_{atom_idx}',\n",
        "                                        'XX': f'XX_{atom_idx}',\n",
        "                                        'YX': f'YX_{atom_idx}',\n",
        "                                        'ZX': f'ZX_{atom_idx}',\n",
        "                                        'XY': f'XY_{atom_idx}',\n",
        "                                        'YY': f'YY_{atom_idx}',\n",
        "                                        'ZY': f'ZY_{atom_idx}',\n",
        "                                        'XZ': f'XZ_{atom_idx}',\n",
        "                                        'YZ': f'YZ_{atom_idx}',\n",
        "                                        'ZZ': f'ZZ_{atom_idx}',})\n",
        "    df_test = map_atom_info(df_test,df_struct, atom_idx)\n",
        "    df_test = df_test.rename(columns={'atom': f'atom_{atom_idx}',\n",
        "                                'x': f'x_{atom_idx}',\n",
        "                                'y': f'y_{atom_idx}',\n",
        "                                'z': f'z_{atom_idx}'})\n",
        "    #add some features\n",
        "    df_struct['c_x']=df_struct.groupby('molecule_name')['x'].transform('mean')\n",
        "    df_struct['c_y']=df_struct.groupby('molecule_name')['y'].transform('mean')\n",
        "    df_struct['c_z']=df_struct.groupby('molecule_name')['z'].transform('mean')\n",
        "    df_struct['atom_n']=df_struct.groupby('molecule_name')['atom_index'].transform('max')\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T07:46:44.208956Z",
          "iopub.execute_input": "2022-12-14T07:46:44.209370Z",
          "iopub.status.idle": "2022-12-14T07:47:45.203588Z",
          "shell.execute_reply.started": "2022-12-14T07:46:44.209297Z",
          "shell.execute_reply": "2022-12-14T07:47:45.202760Z"
        },
        "trusted": true,
        "id": "zzRfjNCnHNz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance feature. "
      ],
      "metadata": {
        "id": "HpzmVPx9HNz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_features(df):\n",
        "    df['dx']=df['x_1']-df['x_0']\n",
        "    df['dy']=df['y_1']-df['y_0']\n",
        "    df['dz']=df['z_1']-df['z_0']\n",
        "    df['distance']=(df['dx']**2+df['dy']**2+df['dz']**2)**(1/2)\n",
        "    return df\n",
        "df_train=make_features(df_train)\n",
        "df_test=make_features(df_test) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T07:47:45.205275Z",
          "iopub.execute_input": "2022-12-14T07:47:45.205565Z",
          "iopub.status.idle": "2022-12-14T07:47:46.003962Z",
          "shell.execute_reply.started": "2022-12-14T07:47:45.205498Z",
          "shell.execute_reply": "2022-12-14T07:47:46.003254Z"
        },
        "trusted": true,
        "id": "WVKsRQrGHNz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_closest_farthest(df):\n",
        "    df_temp=df.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"distance\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
        "    df_temp_=df_temp.copy()\n",
        "    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
        "                                       'atom_index_1': 'atom_index_0',\n",
        "                                       'x_0': 'x_1',\n",
        "                                       'y_0': 'y_1',\n",
        "                                       'z_0': 'z_1',\n",
        "                                       'x_1': 'x_0',\n",
        "                                       'y_1': 'y_0',\n",
        "                                       'z_1': 'z_0'})\n",
        "    df_temp_all=pd.concat((df_temp,df_temp_),axis=0)\n",
        "\n",
        "    df_temp_all[\"min_distance\"]=df_temp_all.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('min')\n",
        "    df_temp_all[\"max_distance\"]=df_temp_all.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('max')\n",
        "    \n",
        "    df_temp= df_temp_all[df_temp_all[\"min_distance\"]==df_temp_all[\"distance\"]].copy()\n",
        "    df_temp=df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
        "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
        "                                         'atom_index_1': 'atom_index_closest',\n",
        "                                         'distance': 'distance_closest',\n",
        "                                         'x_1': 'x_closest',\n",
        "                                         'y_1': 'y_closest',\n",
        "                                         'z_1': 'z_closest'})\n",
        "\n",
        "    for atom_idx in [0,1]:\n",
        "        df = map_atom_info(df,df_temp, atom_idx)\n",
        "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
        "                                        'distance_closest': f'distance_closest_{atom_idx}',\n",
        "                                        'x_closest': f'x_closest_{atom_idx}',\n",
        "                                        'y_closest': f'y_closest_{atom_idx}',\n",
        "                                        'z_closest': f'z_closest_{atom_idx}'})\n",
        "\n",
        "    df_temp= df_temp_all[df_temp_all[\"max_distance\"]==df_temp_all[\"distance\"]].copy()\n",
        "    df_temp=df_temp.drop(['x_0','y_0','z_0','max_distance'], axis=1)\n",
        "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
        "                                         'atom_index_1': 'atom_index_farthest',\n",
        "                                         'distance': 'distance_farthest',\n",
        "                                         'x_1': 'x_farthest',\n",
        "                                         'y_1': 'y_farthest',\n",
        "                                         'z_1': 'z_farthest'})\n",
        "\n",
        "    for atom_idx in [0,1]:\n",
        "        df = map_atom_info(df,df_temp, atom_idx)\n",
        "        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n",
        "                                        'distance_farthest': f'distance_farthest_{atom_idx}',\n",
        "                                        'x_farthest': f'x_farthest_{atom_idx}',\n",
        "                                        'y_farthest': f'y_farthest_{atom_idx}',\n",
        "                                        'z_farthest': f'z_farthest_{atom_idx}'})\n",
        "    return df\n",
        "    \n",
        "df_train=get_closest_farthest(df_train)    \n",
        "df_test=get_closest_farthest(df_test)    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T07:47:46.011340Z",
          "iopub.execute_input": "2022-12-14T07:47:46.011755Z",
          "iopub.status.idle": "2022-12-14T07:50:16.499078Z",
          "shell.execute_reply.started": "2022-12-14T07:47:46.011709Z",
          "shell.execute_reply": "2022-12-14T07:50:16.498085Z"
        },
        "trusted": true,
        "id": "k3wQeK5NHN0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_cos_features(df):\n",
        "    df[\"distance_center0\"]=((df['x_0']-df['c_x'])**2+(df['y_0']-df['c_y'])**2+(df['z_0']-df['c_z'])**2)**(1/2)\n",
        "    df[\"distance_center1\"]=((df['x_0']-df['c_x'])**2+(df['y_0']-df['c_y'])**2+(df['z_0']-df['c_z'])**2)**(1/2)\n",
        "    df[\"distance_c0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
        "    df[\"distance_c1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
        "    df[\"distance_f0\"]=((df['x_0']-df['x_farthest_0'])**2+(df['y_0']-df['y_farthest_0'])**2+(df['z_0']-df['z_farthest_0'])**2)**(1/2)\n",
        "    df[\"distance_f1\"]=((df['x_1']-df['x_farthest_1'])**2+(df['y_1']-df['y_farthest_1'])**2+(df['z_1']-df['z_farthest_1'])**2)**(1/2)\n",
        "    df[\"vec_center0_x\"]=(df['x_0']-df['c_x'])/(df[\"distance_center0\"]+1e-10)\n",
        "    df[\"vec_center0_y\"]=(df['y_0']-df['c_y'])/(df[\"distance_center0\"]+1e-10)\n",
        "    df[\"vec_center0_z\"]=(df['z_0']-df['c_z'])/(df[\"distance_center0\"]+1e-10)\n",
        "    df[\"vec_center1_x\"]=(df['x_1']-df['c_x'])/(df[\"distance_center1\"]+1e-10)\n",
        "    df[\"vec_center1_y\"]=(df['y_1']-df['c_y'])/(df[\"distance_center1\"]+1e-10)\n",
        "    df[\"vec_center1_z\"]=(df['z_1']-df['c_z'])/(df[\"distance_center1\"]+1e-10)\n",
        "    df[\"vec_c0_x\"]=(df['x_0']-df['x_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
        "    df[\"vec_c0_y\"]=(df['y_0']-df['y_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
        "    df[\"vec_c0_z\"]=(df['z_0']-df['z_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
        "    df[\"vec_c1_x\"]=(df['x_1']-df['x_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
        "    df[\"vec_c1_y\"]=(df['y_1']-df['y_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
        "    df[\"vec_c1_z\"]=(df['z_1']-df['z_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
        "    df[\"vec_f0_x\"]=(df['x_0']-df['x_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
        "    df[\"vec_f0_y\"]=(df['y_0']-df['y_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
        "    df[\"vec_f0_z\"]=(df['z_0']-df['z_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
        "    df[\"vec_f1_x\"]=(df['x_1']-df['x_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
        "    df[\"vec_f1_y\"]=(df['y_1']-df['y_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
        "    df[\"vec_f1_z\"]=(df['z_1']-df['z_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
        "    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"distance\"]\n",
        "    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"distance\"]\n",
        "    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"distance\"]\n",
        "    df[\"cos_c0_c1\"]=df[\"vec_c0_x\"]*df[\"vec_c1_x\"]+df[\"vec_c0_y\"]*df[\"vec_c1_y\"]+df[\"vec_c0_z\"]*df[\"vec_c1_z\"]\n",
        "    df[\"cos_f0_f1\"]=df[\"vec_f0_x\"]*df[\"vec_f1_x\"]+df[\"vec_f0_y\"]*df[\"vec_f1_y\"]+df[\"vec_f0_z\"]*df[\"vec_f1_z\"]\n",
        "    df[\"cos_center0_center1\"]=df[\"vec_center0_x\"]*df[\"vec_center1_x\"]+df[\"vec_center0_y\"]*df[\"vec_center1_y\"]+df[\"vec_center0_z\"]*df[\"vec_center1_z\"]\n",
        "    df[\"cos_c0\"]=df[\"vec_c0_x\"]*df[\"vec_x\"]+df[\"vec_c0_y\"]*df[\"vec_y\"]+df[\"vec_c0_z\"]*df[\"vec_z\"]\n",
        "    df[\"cos_c1\"]=df[\"vec_c1_x\"]*df[\"vec_x\"]+df[\"vec_c1_y\"]*df[\"vec_y\"]+df[\"vec_c1_z\"]*df[\"vec_z\"]\n",
        "    df[\"cos_f0\"]=df[\"vec_f0_x\"]*df[\"vec_x\"]+df[\"vec_f0_y\"]*df[\"vec_y\"]+df[\"vec_f0_z\"]*df[\"vec_z\"]\n",
        "    df[\"cos_f1\"]=df[\"vec_f1_x\"]*df[\"vec_x\"]+df[\"vec_f1_y\"]*df[\"vec_y\"]+df[\"vec_f1_z\"]*df[\"vec_z\"]\n",
        "    df[\"cos_center0\"]=df[\"vec_center0_x\"]*df[\"vec_x\"]+df[\"vec_center0_y\"]*df[\"vec_y\"]+df[\"vec_center0_z\"]*df[\"vec_z\"]\n",
        "    df[\"cos_center1\"]=df[\"vec_center1_x\"]*df[\"vec_x\"]+df[\"vec_center1_y\"]*df[\"vec_y\"]+df[\"vec_center1_z\"]*df[\"vec_z\"]\n",
        "    df=df.drop(['vec_c0_x','vec_c0_y','vec_c0_z','vec_c1_x','vec_c1_y','vec_c1_z',\n",
        "                'vec_f0_x','vec_f0_y','vec_f0_z','vec_f1_x','vec_f1_y','vec_f1_z',\n",
        "                'vec_center0_x','vec_center0_y','vec_center0_z','vec_center1_x','vec_center1_y','vec_center1_z',\n",
        "                'vec_x','vec_y','vec_z'], axis=1)\n",
        "    return df\n",
        "    \n",
        "df_train=add_cos_features(df_train)\n",
        "df_test=add_cos_features(df_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T07:50:16.500708Z",
          "iopub.execute_input": "2022-12-14T07:50:16.501065Z",
          "iopub.status.idle": "2022-12-14T07:50:35.945911Z",
          "shell.execute_reply.started": "2022-12-14T07:50:16.500926Z",
          "shell.execute_reply": "2022-12-14T07:50:35.945066Z"
        },
        "trusted": true,
        "id": "QtfGpzmSHN0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T07:50:35.947161Z",
          "iopub.execute_input": "2022-12-14T07:50:35.947488Z",
          "iopub.status.idle": "2022-12-14T07:50:36.108409Z",
          "shell.execute_reply.started": "2022-12-14T07:50:35.947424Z",
          "shell.execute_reply": "2022-12-14T07:50:36.107367Z"
        },
        "trusted": true,
        "id": "9ZHAoBKIHN0C",
        "outputId": "db441c98-d1a1-46ff-c9a3-af35c2cc4f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   id     molecule_name     ...       cos_center0  cos_center1\n0   0  dsgdb9nsd_000001     ...         -1.000000     0.000007\n1   1  dsgdb9nsd_000001     ...         -0.816487     0.816488\n2   2  dsgdb9nsd_000001     ...         -0.816496     0.816505\n3   3  dsgdb9nsd_000001     ...         -0.816500     0.816509\n4   4  dsgdb9nsd_000001     ...         -1.000000     0.000005\n\n[5 rows x 81 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>molecule_name</th>\n      <th>atom_index_0</th>\n      <th>atom_index_1</th>\n      <th>type</th>\n      <th>scalar_coupling_constant</th>\n      <th>atom_0</th>\n      <th>x_0</th>\n      <th>y_0</th>\n      <th>z_0</th>\n      <th>charge_0</th>\n      <th>XX_0</th>\n      <th>YX_0</th>\n      <th>ZX_0</th>\n      <th>XY_0</th>\n      <th>YY_0</th>\n      <th>ZY_0</th>\n      <th>XZ_0</th>\n      <th>YZ_0</th>\n      <th>ZZ_0</th>\n      <th>atom_1</th>\n      <th>x_1</th>\n      <th>y_1</th>\n      <th>z_1</th>\n      <th>c_x</th>\n      <th>c_y</th>\n      <th>c_z</th>\n      <th>atom_n</th>\n      <th>charge_1</th>\n      <th>XX_1</th>\n      <th>YX_1</th>\n      <th>ZX_1</th>\n      <th>XY_1</th>\n      <th>YY_1</th>\n      <th>ZY_1</th>\n      <th>XZ_1</th>\n      <th>YZ_1</th>\n      <th>ZZ_1</th>\n      <th>dx</th>\n      <th>dy</th>\n      <th>...</th>\n      <th>distance</th>\n      <th>atom_index_closest_0</th>\n      <th>distance_closest_0</th>\n      <th>x_closest_0</th>\n      <th>y_closest_0</th>\n      <th>z_closest_0</th>\n      <th>max_distance_x</th>\n      <th>atom_index_closest_1</th>\n      <th>distance_closest_1</th>\n      <th>x_closest_1</th>\n      <th>y_closest_1</th>\n      <th>z_closest_1</th>\n      <th>max_distance_y</th>\n      <th>atom_index_farthest_0</th>\n      <th>distance_farthest_0</th>\n      <th>x_farthest_0</th>\n      <th>y_farthest_0</th>\n      <th>z_farthest_0</th>\n      <th>min_distance_x</th>\n      <th>atom_index_farthest_1</th>\n      <th>distance_farthest_1</th>\n      <th>x_farthest_1</th>\n      <th>y_farthest_1</th>\n      <th>z_farthest_1</th>\n      <th>min_distance_y</th>\n      <th>distance_center0</th>\n      <th>distance_center1</th>\n      <th>distance_c0</th>\n      <th>distance_c1</th>\n      <th>distance_f0</th>\n      <th>distance_f1</th>\n      <th>cos_c0_c1</th>\n      <th>cos_f0_f1</th>\n      <th>cos_center0_center1</th>\n      <th>cos_c0</th>\n      <th>cos_c1</th>\n      <th>cos_f0</th>\n      <th>cos_f1</th>\n      <th>cos_center0</th>\n      <th>cos_center1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1JHC</td>\n      <td>84.8076</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>C</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>-0.535689</td>\n      <td>195.3147</td>\n      <td>0.0000</td>\n      <td>-0.0001</td>\n      <td>0.0000</td>\n      <td>195.3171</td>\n      <td>0.0007</td>\n      <td>-0.0001</td>\n      <td>0.0007</td>\n      <td>195.3169</td>\n      <td>-0.014849</td>\n      <td>1.091835</td>\n      <td>...</td>\n      <td>1.091953</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>3</td>\n      <td>1.091946</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091953</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>1</td>\n      <td>1.091953</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>1.091946</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091946</td>\n      <td>1.783157</td>\n      <td>1.091953</td>\n      <td>0.333335</td>\n      <td>-0.816502</td>\n      <td>-0.000007</td>\n      <td>-1.000000</td>\n      <td>-0.333335</td>\n      <td>-0.816502</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2JHH</td>\n      <td>-11.2570</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>H</td>\n      <td>1.011731</td>\n      <td>1.463751</td>\n      <td>0.000277</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>0.133922</td>\n      <td>31.5814</td>\n      <td>1.2173</td>\n      <td>-4.1474</td>\n      <td>1.2173</td>\n      <td>28.9036</td>\n      <td>-1.6036</td>\n      <td>-4.1476</td>\n      <td>-1.6036</td>\n      <td>33.8967</td>\n      <td>1.009580</td>\n      <td>1.469782</td>\n      <td>...</td>\n      <td>1.783120</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091952</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783158</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>3</td>\n      <td>1.783158</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091952</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091952</td>\n      <td>1.783157</td>\n      <td>1.783158</td>\n      <td>-0.333287</td>\n      <td>0.000016</td>\n      <td>-0.333304</td>\n      <td>-0.816483</td>\n      <td>0.816482</td>\n      <td>-0.499994</td>\n      <td>0.499995</td>\n      <td>-0.816487</td>\n      <td>0.816488</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2JHH</td>\n      <td>-11.2548</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>H</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>0.133923</td>\n      <td>31.5172</td>\n      <td>4.1086</td>\n      <td>1.2723</td>\n      <td>4.1088</td>\n      <td>33.9068</td>\n      <td>1.6950</td>\n      <td>1.2724</td>\n      <td>1.6951</td>\n      <td>28.9579</td>\n      <td>-0.542965</td>\n      <td>1.453558</td>\n      <td>...</td>\n      <td>1.783147</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091946</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783158</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>2</td>\n      <td>1.783158</td>\n      <td>1.011731</td>\n      <td>1.463751</td>\n      <td>0.000277</td>\n      <td>1.091946</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091946</td>\n      <td>1.783157</td>\n      <td>1.783158</td>\n      <td>-0.333335</td>\n      <td>-0.000016</td>\n      <td>-0.333338</td>\n      <td>-0.816498</td>\n      <td>0.816496</td>\n      <td>-0.500002</td>\n      <td>0.500018</td>\n      <td>-0.816496</td>\n      <td>0.816505</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2JHH</td>\n      <td>-11.2543</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>H</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>0.133923</td>\n      <td>31.4029</td>\n      <td>-4.0942</td>\n      <td>-1.1793</td>\n      <td>-4.0944</td>\n      <td>34.0776</td>\n      <td>1.6259</td>\n      <td>-1.1795</td>\n      <td>1.6260</td>\n      <td>28.9013</td>\n      <td>-0.525964</td>\n      <td>1.443964</td>\n      <td>...</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091948</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>1</td>\n      <td>1.783157</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>1.091948</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091948</td>\n      <td>1.783157</td>\n      <td>1.783157</td>\n      <td>-0.333347</td>\n      <td>-1.000000</td>\n      <td>-0.333351</td>\n      <td>-0.816502</td>\n      <td>0.816500</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>-0.816500</td>\n      <td>0.816509</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1JHC</td>\n      <td>84.8074</td>\n      <td>H</td>\n      <td>1.011731</td>\n      <td>1.463751</td>\n      <td>0.000277</td>\n      <td>0.133922</td>\n      <td>31.5814</td>\n      <td>1.2173</td>\n      <td>-4.1474</td>\n      <td>1.2173</td>\n      <td>28.9036</td>\n      <td>-1.6036</td>\n      <td>-4.1476</td>\n      <td>-1.6036</td>\n      <td>33.8967</td>\n      <td>C</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>-0.535689</td>\n      <td>195.3147</td>\n      <td>0.0000</td>\n      <td>-0.0001</td>\n      <td>0.0000</td>\n      <td>195.3171</td>\n      <td>0.0007</td>\n      <td>-0.0001</td>\n      <td>0.0007</td>\n      <td>195.3169</td>\n      <td>-1.024429</td>\n      <td>-0.377947</td>\n      <td>...</td>\n      <td>1.091952</td>\n      <td>0</td>\n      <td>1.091952</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783158</td>\n      <td>3</td>\n      <td>1.091946</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091953</td>\n      <td>3</td>\n      <td>1.783158</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091952</td>\n      <td>1</td>\n      <td>1.091953</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>1.091946</td>\n      <td>1.091946</td>\n      <td>1.091946</td>\n      <td>1.091952</td>\n      <td>1.091946</td>\n      <td>1.783158</td>\n      <td>1.091953</td>\n      <td>0.333352</td>\n      <td>-0.000028</td>\n      <td>-0.000005</td>\n      <td>-1.000000</td>\n      <td>-0.333352</td>\n      <td>-0.816503</td>\n      <td>-0.333287</td>\n      <td>-1.000000</td>\n      <td>0.000005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Neural Network Model\n"
      ],
      "metadata": {
        "id": "MNZChEJhHN0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_nn_model(input_shape):\n",
        "    inp = Input(shape=(input_shape,))\n",
        "    x = Dense(256, activation=\"relu\")(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    out1 = Dense(2, activation=\"linear\")(x)#mulliken charge 2\n",
        "    out2 = Dense(6, activation=\"linear\")(x)#tensor 6(xx,yy,zz)\n",
        "    out3 = Dense(12, activation=\"linear\")(x)#tensor 12(others) \n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    out = Dense(1, activation=\"linear\")(x)#scalar_coupling_constant    \n",
        "    model = Model(inputs=inp, outputs=[out,out1,out2,out3])\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T07:50:36.110012Z",
          "iopub.execute_input": "2022-12-14T07:50:36.110382Z",
          "iopub.status.idle": "2022-12-14T07:50:36.118661Z",
          "shell.execute_reply.started": "2022-12-14T07:50:36.110308Z",
          "shell.execute_reply": "2022-12-14T07:50:36.117675Z"
        },
        "trusted": true,
        "id": "GC8msBjPHN0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create neural networks for each molecule type. (calculate 8 models in total) "
      ],
      "metadata": {
        "id": "AelUChcvHN0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mol_types=df_train[\"type\"].unique()\n",
        "cv_score=[]\n",
        "cv_score_total=0\n",
        "test_prediction=np.zeros(len(df_test))\n",
        "\n",
        "for mol_type in mol_types:\n",
        "    df_train_=df_train[df_train[\"type\"]==mol_type]\n",
        "    df_test_=df_test[df_test[\"type\"]==mol_type]\n",
        "    input_features=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\",\"c_x\",\"c_y\",\"c_z\",\n",
        "                    'x_closest_0','y_closest_0','z_closest_0','x_closest_1','y_closest_1','z_closest_1',\n",
        "                    \"distance\",\"distance_center0\",\"distance_center1\",\"distance_c0\",\"distance_c1\",\"distance_f0\",\"distance_f1\",\n",
        "                    \"cos_c0_c1\",\"cos_f0_f1\",\"cos_center0_center1\",\"cos_c0\",\"cos_c1\",\"cos_f0\",\"cos_f1\",\"cos_center0\",\"cos_center1\",\n",
        "                    \"atom_n\"\n",
        "                   ]\n",
        "    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))\n",
        "    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n",
        "    target_data_1=df_train_.loc[:,[\"charge_0\",\"charge_1\"]]\n",
        "    target_data_2=df_train_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n",
        "    target_data_3=df_train_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n",
        "    \n",
        "\n",
        "    m1=1\n",
        "    m2=4\n",
        "    m3=1\n",
        "    target_data_1=m1*(StandardScaler().fit_transform(target_data_1))\n",
        "    target_data_2=m2*(StandardScaler().fit_transform(target_data_2))\n",
        "    target_data_3=m3*(StandardScaler().fit_transform(target_data_3))\n",
        "    \n",
        "    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.2)\n",
        "    \n",
        "    train_input=input_data[train_index]\n",
        "    cv_input=input_data[cv_index]\n",
        "    train_target=target_data[train_index]\n",
        "    cv_target=target_data[cv_index]\n",
        "    train_target_1=target_data_1[train_index]\n",
        "    cv_target_1=target_data_1[cv_index]\n",
        "    train_target_2=target_data_2[train_index]\n",
        "    cv_target_2=target_data_2[cv_index]\n",
        "    train_target_3=target_data_3[train_index]\n",
        "    cv_target_3=target_data_3[cv_index]\n",
        "    test_input=input_data[len(df_train_):,:]\n",
        "\n",
        "    epoch_n=50\n",
        "    verbose=1\n",
        "    nn_model=create_nn_model(train_input.shape[1])\n",
        "    nn_model.compile(loss='mae', optimizer=Adam(lr=1e-3))#, metrics=[auc])\n",
        "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0., patience=12,#val_auc\n",
        "                                 verbose=verbose, mode='min', baseline=None, restore_best_weights=True)\n",
        "    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n",
        "                                      patience=4, min_lr=2e-6, mode='min', verbose=verbose)\n",
        "    nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n",
        "            validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n",
        "            callbacks=[es, rlr], epochs=epoch_n, batch_size=64, verbose=verbose)\n",
        "    cv_predict=nn_model.predict(cv_input)\n",
        "    \n",
        "    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n",
        "    cv_score.append(np.log(accuracy))\n",
        "    cv_score_total+=np.log(accuracy)\n",
        "    #print(mol_type,\": log score is \",np.log(accuracy))    \n",
        "    test_predict=nn_model.predict(test_input)\n",
        "    test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\n",
        "    K.clear_session()\n",
        "\n",
        "cv_score_total/=len(mol_types)\n",
        "#print(\"total score is\",cv_score_total)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T07:50:36.120150Z",
          "iopub.execute_input": "2022-12-14T07:50:36.120714Z",
          "iopub.status.idle": "2022-12-14T16:24:05.174060Z",
          "shell.execute_reply.started": "2022-12-14T07:50:36.120631Z",
          "shell.execute_reply": "2022-12-14T16:24:05.172240Z"
        },
        "trusted": true,
        "id": "29MaQnhwHN0D",
        "outputId": "212e28e7-41ae-43e1-d200-40eb31be1418"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 567306 samples, validate on 141827 samples\nEpoch 1/50\n567306/567306 [==============================] - 94s 166us/step - loss: 12.2164 - dense_10_loss: 9.0254 - dense_5_loss: 0.4137 - dense_6_loss: 2.0539 - dense_7_loss: 0.7235 - val_loss: 6.1961 - val_dense_10_loss: 3.2267 - val_dense_5_loss: 0.3535 - val_dense_6_loss: 1.8991 - val_dense_7_loss: 0.7168\nEpoch 2/50\n567306/567306 [==============================] - 93s 164us/step - loss: 6.6298 - dense_10_loss: 3.6319 - dense_5_loss: 0.3673 - dense_6_loss: 1.9144 - dense_7_loss: 0.7161 - val_loss: 5.8396 - val_dense_10_loss: 2.9476 - val_dense_5_loss: 0.3421 - val_dense_6_loss: 1.8337 - val_dense_7_loss: 0.7163\nEpoch 3/50\n567306/567306 [==============================] - 92s 162us/step - loss: 6.4023 - dense_10_loss: 3.4714 - dense_5_loss: 0.3521 - dense_6_loss: 1.8631 - dense_7_loss: 0.7158 - val_loss: 5.6717 - val_dense_10_loss: 2.8446 - val_dense_5_loss: 0.3238 - val_dense_6_loss: 1.7874 - val_dense_7_loss: 0.7159\nEpoch 4/50\n567306/567306 [==============================] - 91s 160us/step - loss: 6.2689 - dense_10_loss: 3.3801 - dense_5_loss: 0.3428 - dense_6_loss: 1.8304 - dense_7_loss: 0.7156 - val_loss: 5.4481 - val_dense_10_loss: 2.6591 - val_dense_5_loss: 0.3129 - val_dense_6_loss: 1.7604 - val_dense_7_loss: 0.7158\nEpoch 5/50\n567306/567306 [==============================] - 91s 160us/step - loss: 6.1971 - dense_10_loss: 3.3360 - dense_5_loss: 0.3372 - dense_6_loss: 1.8085 - dense_7_loss: 0.7154 - val_loss: 5.4644 - val_dense_10_loss: 2.6971 - val_dense_5_loss: 0.3121 - val_dense_6_loss: 1.7398 - val_dense_7_loss: 0.7154\nEpoch 6/50\n567306/567306 [==============================] - 90s 159us/step - loss: 6.1033 - dense_10_loss: 3.2640 - dense_5_loss: 0.3327 - dense_6_loss: 1.7914 - dense_7_loss: 0.7153 - val_loss: 5.3436 - val_dense_10_loss: 2.6010 - val_dense_5_loss: 0.3058 - val_dense_6_loss: 1.7218 - val_dense_7_loss: 0.7149\nEpoch 7/50\n567306/567306 [==============================] - 94s 166us/step - loss: 6.0521 - dense_10_loss: 3.2326 - dense_5_loss: 0.3287 - dense_6_loss: 1.7756 - dense_7_loss: 0.7152 - val_loss: 5.3542 - val_dense_10_loss: 2.6200 - val_dense_5_loss: 0.3024 - val_dense_6_loss: 1.7165 - val_dense_7_loss: 0.7153\nEpoch 8/50\n567306/567306 [==============================] - 91s 160us/step - loss: 5.9907 - dense_10_loss: 3.1868 - dense_5_loss: 0.3256 - dense_6_loss: 1.7631 - dense_7_loss: 0.7152 - val_loss: 5.2928 - val_dense_10_loss: 2.5896 - val_dense_5_loss: 0.2991 - val_dense_6_loss: 1.6886 - val_dense_7_loss: 0.7155\nEpoch 9/50\n567306/567306 [==============================] - 91s 160us/step - loss: 5.9480 - dense_10_loss: 3.1573 - dense_5_loss: 0.3236 - dense_6_loss: 1.7520 - dense_7_loss: 0.7150 - val_loss: 5.1523 - val_dense_10_loss: 2.4566 - val_dense_5_loss: 0.2945 - val_dense_6_loss: 1.6864 - val_dense_7_loss: 0.7148\nEpoch 10/50\n567306/567306 [==============================] - 91s 161us/step - loss: 5.8949 - dense_10_loss: 3.1155 - dense_5_loss: 0.3213 - dense_6_loss: 1.7431 - dense_7_loss: 0.7150 - val_loss: 5.1298 - val_dense_10_loss: 2.4506 - val_dense_5_loss: 0.2935 - val_dense_6_loss: 1.6703 - val_dense_7_loss: 0.7154\nEpoch 11/50\n567306/567306 [==============================] - 90s 159us/step - loss: 5.8648 - dense_10_loss: 3.0943 - dense_5_loss: 0.3196 - dense_6_loss: 1.7360 - dense_7_loss: 0.7149 - val_loss: 5.2263 - val_dense_10_loss: 2.5471 - val_dense_5_loss: 0.2955 - val_dense_6_loss: 1.6684 - val_dense_7_loss: 0.7154\nEpoch 12/50\n567306/567306 [==============================] - 90s 159us/step - loss: 5.8428 - dense_10_loss: 3.0841 - dense_5_loss: 0.3175 - dense_6_loss: 1.7263 - dense_7_loss: 0.7148 - val_loss: 5.2676 - val_dense_10_loss: 2.6000 - val_dense_5_loss: 0.2956 - val_dense_6_loss: 1.6578 - val_dense_7_loss: 0.7142\nEpoch 13/50\n567306/567306 [==============================] - 91s 161us/step - loss: 5.8277 - dense_10_loss: 3.0739 - dense_5_loss: 0.3167 - dense_6_loss: 1.7224 - dense_7_loss: 0.7148 - val_loss: 5.2270 - val_dense_10_loss: 2.5740 - val_dense_5_loss: 0.2867 - val_dense_6_loss: 1.6520 - val_dense_7_loss: 0.7143\nEpoch 14/50\n567306/567306 [==============================] - 91s 160us/step - loss: 5.7931 - dense_10_loss: 3.0493 - dense_5_loss: 0.3145 - dense_6_loss: 1.7146 - dense_7_loss: 0.7147 - val_loss: 5.0676 - val_dense_10_loss: 2.4164 - val_dense_5_loss: 0.2935 - val_dense_6_loss: 1.6435 - val_dense_7_loss: 0.7141\nEpoch 15/50\n567306/567306 [==============================] - 91s 161us/step - loss: 5.7501 - dense_10_loss: 3.0116 - dense_5_loss: 0.3136 - dense_6_loss: 1.7103 - dense_7_loss: 0.7147 - val_loss: 5.0857 - val_dense_10_loss: 2.4373 - val_dense_5_loss: 0.2943 - val_dense_6_loss: 1.6399 - val_dense_7_loss: 0.7141\nEpoch 16/50\n567306/567306 [==============================] - 90s 159us/step - loss: 5.7462 - dense_10_loss: 3.0154 - dense_5_loss: 0.3123 - dense_6_loss: 1.7038 - dense_7_loss: 0.7146 - val_loss: 4.9557 - val_dense_10_loss: 2.3254 - val_dense_5_loss: 0.2842 - val_dense_6_loss: 1.6316 - val_dense_7_loss: 0.7145\nEpoch 17/50\n567306/567306 [==============================] - 91s 161us/step - loss: 5.7305 - dense_10_loss: 3.0048 - dense_5_loss: 0.3113 - dense_6_loss: 1.6998 - dense_7_loss: 0.7146 - val_loss: 4.9871 - val_dense_10_loss: 2.3668 - val_dense_5_loss: 0.2802 - val_dense_6_loss: 1.6257 - val_dense_7_loss: 0.7145\nEpoch 18/50\n567306/567306 [==============================] - 91s 160us/step - loss: 5.7076 - dense_10_loss: 2.9877 - dense_5_loss: 0.3101 - dense_6_loss: 1.6952 - dense_7_loss: 0.7146 - val_loss: 4.9041 - val_dense_10_loss: 2.2781 - val_dense_5_loss: 0.2879 - val_dense_6_loss: 1.6235 - val_dense_7_loss: 0.7147\nEpoch 19/50\n567306/567306 [==============================] - 90s 158us/step - loss: 5.6949 - dense_10_loss: 2.9790 - dense_5_loss: 0.3093 - dense_6_loss: 1.6921 - dense_7_loss: 0.7145 - val_loss: 4.9286 - val_dense_10_loss: 2.3148 - val_dense_5_loss: 0.2812 - val_dense_6_loss: 1.6181 - val_dense_7_loss: 0.7145\nEpoch 20/50\n567306/567306 [==============================] - 91s 161us/step - loss: 5.6849 - dense_10_loss: 2.9755 - dense_5_loss: 0.3078 - dense_6_loss: 1.6871 - dense_7_loss: 0.7145 - val_loss: 5.1354 - val_dense_10_loss: 2.5218 - val_dense_5_loss: 0.2873 - val_dense_6_loss: 1.6118 - val_dense_7_loss: 0.7145\nEpoch 21/50\n567306/567306 [==============================] - 91s 160us/step - loss: 5.6602 - dense_10_loss: 2.9553 - dense_5_loss: 0.3073 - dense_6_loss: 1.6832 - dense_7_loss: 0.7145 - val_loss: 4.8849 - val_dense_10_loss: 2.2784 - val_dense_5_loss: 0.2820 - val_dense_6_loss: 1.6097 - val_dense_7_loss: 0.7148\nEpoch 22/50\n567306/567306 [==============================] - 90s 159us/step - loss: 5.6592 - dense_10_loss: 2.9568 - dense_5_loss: 0.3066 - dense_6_loss: 1.6814 - dense_7_loss: 0.7144 - val_loss: 4.9173 - val_dense_10_loss: 2.3180 - val_dense_5_loss: 0.2844 - val_dense_6_loss: 1.6006 - val_dense_7_loss: 0.7143\nEpoch 23/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.6448 - dense_10_loss: 2.9471 - dense_5_loss: 0.3063 - dense_6_loss: 1.6769 - dense_7_loss: 0.7144 - val_loss: 4.9311 - val_dense_10_loss: 2.3304 - val_dense_5_loss: 0.2822 - val_dense_6_loss: 1.6043 - val_dense_7_loss: 0.7142\nEpoch 24/50\n567306/567306 [==============================] - 92s 163us/step - loss: 5.6248 - dense_10_loss: 2.9314 - dense_5_loss: 0.3052 - dense_6_loss: 1.6739 - dense_7_loss: 0.7144 - val_loss: 4.8774 - val_dense_10_loss: 2.2815 - val_dense_5_loss: 0.2781 - val_dense_6_loss: 1.6033 - val_dense_7_loss: 0.7145\nEpoch 25/50\n567306/567306 [==============================] - 92s 161us/step - loss: 5.5981 - dense_10_loss: 2.9089 - dense_5_loss: 0.3041 - dense_6_loss: 1.6707 - dense_7_loss: 0.7143 - val_loss: 4.9613 - val_dense_10_loss: 2.3689 - val_dense_5_loss: 0.2786 - val_dense_6_loss: 1.5999 - val_dense_7_loss: 0.7139\nEpoch 26/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.6085 - dense_10_loss: 2.9216 - dense_5_loss: 0.3044 - dense_6_loss: 1.6681 - dense_7_loss: 0.7143 - val_loss: 4.8439 - val_dense_10_loss: 2.2598 - val_dense_5_loss: 0.2759 - val_dense_6_loss: 1.5938 - val_dense_7_loss: 0.7144\nEpoch 27/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.6000 - dense_10_loss: 2.9175 - dense_5_loss: 0.3032 - dense_6_loss: 1.6649 - dense_7_loss: 0.7143 - val_loss: 4.7848 - val_dense_10_loss: 2.2022 - val_dense_5_loss: 0.2773 - val_dense_6_loss: 1.5910 - val_dense_7_loss: 0.7143\nEpoch 28/50\n567306/567306 [==============================] - 95s 167us/step - loss: 5.5830 - dense_10_loss: 2.9036 - dense_5_loss: 0.3026 - dense_6_loss: 1.6625 - dense_7_loss: 0.7143 - val_loss: 4.7804 - val_dense_10_loss: 2.2085 - val_dense_5_loss: 0.2726 - val_dense_6_loss: 1.5853 - val_dense_7_loss: 0.7139\nEpoch 29/50\n567306/567306 [==============================] - 94s 167us/step - loss: 5.5814 - dense_10_loss: 2.9042 - dense_5_loss: 0.3026 - dense_6_loss: 1.6604 - dense_7_loss: 0.7143 - val_loss: 4.7692 - val_dense_10_loss: 2.1936 - val_dense_5_loss: 0.2754 - val_dense_6_loss: 1.5860 - val_dense_7_loss: 0.7142\nEpoch 30/50\n567306/567306 [==============================] - 94s 165us/step - loss: 5.5565 - dense_10_loss: 2.8821 - dense_5_loss: 0.3019 - dense_6_loss: 1.6583 - dense_7_loss: 0.7142 - val_loss: 4.8751 - val_dense_10_loss: 2.2923 - val_dense_5_loss: 0.2756 - val_dense_6_loss: 1.5925 - val_dense_7_loss: 0.7148\nEpoch 31/50\n567306/567306 [==============================] - 95s 167us/step - loss: 5.5666 - dense_10_loss: 2.8944 - dense_5_loss: 0.3018 - dense_6_loss: 1.6562 - dense_7_loss: 0.7142 - val_loss: 4.7353 - val_dense_10_loss: 2.1674 - val_dense_5_loss: 0.2778 - val_dense_6_loss: 1.5759 - val_dense_7_loss: 0.7142\nEpoch 32/50\n567306/567306 [==============================] - 95s 167us/step - loss: 5.5483 - dense_10_loss: 2.8795 - dense_5_loss: 0.3007 - dense_6_loss: 1.6539 - dense_7_loss: 0.7142 - val_loss: 4.7706 - val_dense_10_loss: 2.2081 - val_dense_5_loss: 0.2730 - val_dense_6_loss: 1.5755 - val_dense_7_loss: 0.7140\nEpoch 33/50\n567306/567306 [==============================] - 93s 165us/step - loss: 5.5543 - dense_10_loss: 2.8878 - dense_5_loss: 0.3005 - dense_6_loss: 1.6518 - dense_7_loss: 0.7142 - val_loss: 4.8650 - val_dense_10_loss: 2.2962 - val_dense_5_loss: 0.2722 - val_dense_6_loss: 1.5824 - val_dense_7_loss: 0.7143\nEpoch 34/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.5246 - dense_10_loss: 2.8597 - dense_5_loss: 0.3007 - dense_6_loss: 1.6501 - dense_7_loss: 0.7142 - val_loss: 4.8318 - val_dense_10_loss: 2.2669 - val_dense_5_loss: 0.2738 - val_dense_6_loss: 1.5768 - val_dense_7_loss: 0.7142\nEpoch 35/50\n567306/567306 [==============================] - 91s 160us/step - loss: 5.5183 - dense_10_loss: 2.8565 - dense_5_loss: 0.2995 - dense_6_loss: 1.6482 - dense_7_loss: 0.7141 - val_loss: 4.7356 - val_dense_10_loss: 2.1809 - val_dense_5_loss: 0.2723 - val_dense_6_loss: 1.5681 - val_dense_7_loss: 0.7143\n\nEpoch 00035: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 36/50\n567306/567306 [==============================] - 93s 164us/step - loss: 5.3986 - dense_10_loss: 2.7690 - dense_5_loss: 0.2929 - dense_6_loss: 1.6243 - dense_7_loss: 0.7124 - val_loss: 4.5945 - val_dense_10_loss: 2.0714 - val_dense_5_loss: 0.2644 - val_dense_6_loss: 1.5459 - val_dense_7_loss: 0.7127\nEpoch 37/50\n567306/567306 [==============================] - 94s 166us/step - loss: 5.3808 - dense_10_loss: 2.7565 - dense_5_loss: 0.2924 - dense_6_loss: 1.6196 - dense_7_loss: 0.7123 - val_loss: 4.5617 - val_dense_10_loss: 2.0459 - val_dense_5_loss: 0.2630 - val_dense_6_loss: 1.5401 - val_dense_7_loss: 0.7127\nEpoch 38/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.3638 - dense_10_loss: 2.7427 - dense_5_loss: 0.2913 - dense_6_loss: 1.6174 - dense_7_loss: 0.7123 - val_loss: 4.5394 - val_dense_10_loss: 2.0215 - val_dense_5_loss: 0.2636 - val_dense_6_loss: 1.5418 - val_dense_7_loss: 0.7125\nEpoch 39/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.3467 - dense_10_loss: 2.7273 - dense_5_loss: 0.2911 - dense_6_loss: 1.6159 - dense_7_loss: 0.7124 - val_loss: 4.5854 - val_dense_10_loss: 2.0689 - val_dense_5_loss: 0.2648 - val_dense_6_loss: 1.5392 - val_dense_7_loss: 0.7125\nEpoch 40/50\n567306/567306 [==============================] - 92s 161us/step - loss: 5.3480 - dense_10_loss: 2.7307 - dense_5_loss: 0.2908 - dense_6_loss: 1.6142 - dense_7_loss: 0.7123 - val_loss: 4.5642 - val_dense_10_loss: 2.0541 - val_dense_5_loss: 0.2618 - val_dense_6_loss: 1.5358 - val_dense_7_loss: 0.7126\nEpoch 41/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.3436 - dense_10_loss: 2.7281 - dense_5_loss: 0.2901 - dense_6_loss: 1.6130 - dense_7_loss: 0.7123 - val_loss: 4.6567 - val_dense_10_loss: 2.1485 - val_dense_5_loss: 0.2616 - val_dense_6_loss: 1.5341 - val_dense_7_loss: 0.7125\nEpoch 42/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.3466 - dense_10_loss: 2.7329 - dense_5_loss: 0.2900 - dense_6_loss: 1.6113 - dense_7_loss: 0.7123 - val_loss: 4.5656 - val_dense_10_loss: 2.0496 - val_dense_5_loss: 0.2648 - val_dense_6_loss: 1.5387 - val_dense_7_loss: 0.7125\n\nEpoch 00042: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\nEpoch 43/50\n567306/567306 [==============================] - 93s 165us/step - loss: 5.2982 - dense_10_loss: 2.6927 - dense_5_loss: 0.2880 - dense_6_loss: 1.6056 - dense_7_loss: 0.7118 - val_loss: 4.4989 - val_dense_10_loss: 1.9941 - val_dense_5_loss: 0.2621 - val_dense_6_loss: 1.5305 - val_dense_7_loss: 0.7121\nEpoch 44/50\n567306/567306 [==============================] - 92s 163us/step - loss: 5.2950 - dense_10_loss: 2.6934 - dense_5_loss: 0.2879 - dense_6_loss: 1.6019 - dense_7_loss: 0.7118 - val_loss: 4.4919 - val_dense_10_loss: 1.9920 - val_dense_5_loss: 0.2606 - val_dense_6_loss: 1.5272 - val_dense_7_loss: 0.7121\nEpoch 45/50\n567306/567306 [==============================] - 92s 163us/step - loss: 5.2947 - dense_10_loss: 2.6918 - dense_5_loss: 0.2882 - dense_6_loss: 1.6029 - dense_7_loss: 0.7118 - val_loss: 4.5228 - val_dense_10_loss: 2.0229 - val_dense_5_loss: 0.2614 - val_dense_6_loss: 1.5265 - val_dense_7_loss: 0.7121\nEpoch 46/50\n567306/567306 [==============================] - 93s 163us/step - loss: 5.2982 - dense_10_loss: 2.6959 - dense_5_loss: 0.2878 - dense_6_loss: 1.6027 - dense_7_loss: 0.7118 - val_loss: 4.4988 - val_dense_10_loss: 1.9995 - val_dense_5_loss: 0.2609 - val_dense_6_loss: 1.5264 - val_dense_7_loss: 0.7121\nEpoch 47/50\n567306/567306 [==============================] - 92s 163us/step - loss: 5.2804 - dense_10_loss: 2.6801 - dense_5_loss: 0.2875 - dense_6_loss: 1.6010 - dense_7_loss: 0.7118 - val_loss: 4.4893 - val_dense_10_loss: 1.9918 - val_dense_5_loss: 0.2596 - val_dense_6_loss: 1.5258 - val_dense_7_loss: 0.7121\nEpoch 48/50\n567306/567306 [==============================] - 93s 163us/step - loss: 5.2978 - dense_10_loss: 2.6963 - dense_5_loss: 0.2875 - dense_6_loss: 1.6022 - dense_7_loss: 0.7118 - val_loss: 4.4983 - val_dense_10_loss: 2.0001 - val_dense_5_loss: 0.2605 - val_dense_6_loss: 1.5257 - val_dense_7_loss: 0.7121\nEpoch 49/50\n567306/567306 [==============================] - 92s 162us/step - loss: 5.2821 - dense_10_loss: 2.6819 - dense_5_loss: 0.2870 - dense_6_loss: 1.6014 - dense_7_loss: 0.7118 - val_loss: 4.5097 - val_dense_10_loss: 2.0109 - val_dense_5_loss: 0.2606 - val_dense_6_loss: 1.5262 - val_dense_7_loss: 0.7121\nEpoch 50/50\n567306/567306 [==============================] - 91s 161us/step - loss: 5.2705 - dense_10_loss: 2.6721 - dense_5_loss: 0.2873 - dense_6_loss: 1.5994 - dense_7_loss: 0.7118 - val_loss: 4.4771 - val_dense_10_loss: 1.9814 - val_dense_5_loss: 0.2602 - val_dense_6_loss: 1.5234 - val_dense_7_loss: 0.7121\nTrain on 302390 samples, validate on 75598 samples\nEpoch 1/50\n302390/302390 [==============================] - 51s 169us/step - loss: 4.8000 - dense_10_loss: 1.1773 - dense_5_loss: 0.3213 - dense_6_loss: 2.4891 - dense_7_loss: 0.8123 - val_loss: 4.0637 - val_dense_10_loss: 0.6701 - val_dense_5_loss: 0.2742 - val_dense_6_loss: 2.3171 - val_dense_7_loss: 0.8023\nEpoch 2/50\n302390/302390 [==============================] - 49s 163us/step - loss: 4.1222 - dense_10_loss: 0.7419 - dense_5_loss: 0.2779 - dense_6_loss: 2.3017 - dense_7_loss: 0.8007 - val_loss: 3.8065 - val_dense_10_loss: 0.5563 - val_dense_5_loss: 0.2449 - val_dense_6_loss: 2.2045 - val_dense_7_loss: 0.8009\nEpoch 3/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.9976 - dense_10_loss: 0.7070 - dense_5_loss: 0.2652 - dense_6_loss: 2.2259 - dense_7_loss: 0.7995 - val_loss: 3.6772 - val_dense_10_loss: 0.5116 - val_dense_5_loss: 0.2327 - val_dense_6_loss: 2.1333 - val_dense_7_loss: 0.7996\nEpoch 4/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.9043 - dense_10_loss: 0.6723 - dense_5_loss: 0.2578 - dense_6_loss: 2.1753 - dense_7_loss: 0.7988 - val_loss: 3.6306 - val_dense_10_loss: 0.5050 - val_dense_5_loss: 0.2292 - val_dense_6_loss: 2.0979 - val_dense_7_loss: 0.7985\nEpoch 5/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.8457 - dense_10_loss: 0.6568 - dense_5_loss: 0.2536 - dense_6_loss: 2.1368 - dense_7_loss: 0.7986 - val_loss: 3.5620 - val_dense_10_loss: 0.4786 - val_dense_5_loss: 0.2216 - val_dense_6_loss: 2.0638 - val_dense_7_loss: 0.7981\nEpoch 6/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.7942 - dense_10_loss: 0.6407 - dense_5_loss: 0.2498 - dense_6_loss: 2.1055 - dense_7_loss: 0.7982 - val_loss: 3.5088 - val_dense_10_loss: 0.4601 - val_dense_5_loss: 0.2181 - val_dense_6_loss: 2.0324 - val_dense_7_loss: 0.7982\nEpoch 7/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.7555 - dense_10_loss: 0.6321 - dense_5_loss: 0.2458 - dense_6_loss: 2.0798 - dense_7_loss: 0.7977 - val_loss: 3.4737 - val_dense_10_loss: 0.4588 - val_dense_5_loss: 0.2146 - val_dense_6_loss: 2.0019 - val_dense_7_loss: 0.7983\nEpoch 8/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.7233 - dense_10_loss: 0.6245 - dense_5_loss: 0.2441 - dense_6_loss: 2.0573 - dense_7_loss: 0.7974 - val_loss: 3.4613 - val_dense_10_loss: 0.4498 - val_dense_5_loss: 0.2239 - val_dense_6_loss: 1.9895 - val_dense_7_loss: 0.7980\nEpoch 9/50\n302390/302390 [==============================] - 50s 165us/step - loss: 3.6882 - dense_10_loss: 0.6134 - dense_5_loss: 0.2416 - dense_6_loss: 2.0361 - dense_7_loss: 0.7971 - val_loss: 3.4336 - val_dense_10_loss: 0.4483 - val_dense_5_loss: 0.2158 - val_dense_6_loss: 1.9723 - val_dense_7_loss: 0.7972\nEpoch 10/50\n302390/302390 [==============================] - 50s 164us/step - loss: 3.6659 - dense_10_loss: 0.6094 - dense_5_loss: 0.2405 - dense_6_loss: 2.0191 - dense_7_loss: 0.7969 - val_loss: 3.3816 - val_dense_10_loss: 0.4236 - val_dense_5_loss: 0.2106 - val_dense_6_loss: 1.9506 - val_dense_7_loss: 0.7967\nEpoch 11/50\n302390/302390 [==============================] - 50s 165us/step - loss: 3.6478 - dense_10_loss: 0.6066 - dense_5_loss: 0.2396 - dense_6_loss: 2.0049 - dense_7_loss: 0.7967 - val_loss: 3.3648 - val_dense_10_loss: 0.4204 - val_dense_5_loss: 0.2111 - val_dense_6_loss: 1.9369 - val_dense_7_loss: 0.7965\nEpoch 12/50\n302390/302390 [==============================] - 50s 164us/step - loss: 3.6298 - dense_10_loss: 0.6028 - dense_5_loss: 0.2380 - dense_6_loss: 1.9924 - dense_7_loss: 0.7966 - val_loss: 3.3786 - val_dense_10_loss: 0.4406 - val_dense_5_loss: 0.2101 - val_dense_6_loss: 1.9312 - val_dense_7_loss: 0.7966\nEpoch 13/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.6119 - dense_10_loss: 0.5976 - dense_5_loss: 0.2370 - dense_6_loss: 1.9811 - dense_7_loss: 0.7962 - val_loss: 3.3483 - val_dense_10_loss: 0.4249 - val_dense_5_loss: 0.2088 - val_dense_6_loss: 1.9178 - val_dense_7_loss: 0.7968\nEpoch 14/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.5961 - dense_10_loss: 0.5937 - dense_5_loss: 0.2368 - dense_6_loss: 1.9696 - dense_7_loss: 0.7960 - val_loss: 3.3232 - val_dense_10_loss: 0.4094 - val_dense_5_loss: 0.2075 - val_dense_6_loss: 1.9103 - val_dense_7_loss: 0.7960\nEpoch 15/50\n302390/302390 [==============================] - 49s 164us/step - loss: 3.5794 - dense_10_loss: 0.5886 - dense_5_loss: 0.2349 - dense_6_loss: 1.9600 - dense_7_loss: 0.7959 - val_loss: 3.3359 - val_dense_10_loss: 0.4452 - val_dense_5_loss: 0.2030 - val_dense_6_loss: 1.8918 - val_dense_7_loss: 0.7959\nEpoch 16/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.5703 - dense_10_loss: 0.5866 - dense_5_loss: 0.2354 - dense_6_loss: 1.9527 - dense_7_loss: 0.7956 - val_loss: 3.2940 - val_dense_10_loss: 0.4112 - val_dense_5_loss: 0.2029 - val_dense_6_loss: 1.8845 - val_dense_7_loss: 0.7954\nEpoch 17/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.5562 - dense_10_loss: 0.5833 - dense_5_loss: 0.2339 - dense_6_loss: 1.9436 - dense_7_loss: 0.7954 - val_loss: 3.3169 - val_dense_10_loss: 0.4351 - val_dense_5_loss: 0.2062 - val_dense_6_loss: 1.8800 - val_dense_7_loss: 0.7955\nEpoch 18/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.5456 - dense_10_loss: 0.5812 - dense_5_loss: 0.2336 - dense_6_loss: 1.9355 - dense_7_loss: 0.7953 - val_loss: 3.3071 - val_dense_10_loss: 0.4307 - val_dense_5_loss: 0.2008 - val_dense_6_loss: 1.8797 - val_dense_7_loss: 0.7959\nEpoch 19/50\n302390/302390 [==============================] - 48s 160us/step - loss: 3.5359 - dense_10_loss: 0.5785 - dense_5_loss: 0.2334 - dense_6_loss: 1.9288 - dense_7_loss: 0.7952 - val_loss: 3.4763 - val_dense_10_loss: 0.5169 - val_dense_5_loss: 0.2493 - val_dense_6_loss: 1.9142 - val_dense_7_loss: 0.7959\nEpoch 20/50\n302390/302390 [==============================] - 49s 160us/step - loss: 3.5262 - dense_10_loss: 0.5767 - dense_5_loss: 0.2325 - dense_6_loss: 1.9220 - dense_7_loss: 0.7950 - val_loss: 3.2525 - val_dense_10_loss: 0.3842 - val_dense_5_loss: 0.2055 - val_dense_6_loss: 1.8678 - val_dense_7_loss: 0.7950\nEpoch 21/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.5199 - dense_10_loss: 0.5777 - dense_5_loss: 0.2323 - dense_6_loss: 1.9150 - dense_7_loss: 0.7949 - val_loss: 3.3082 - val_dense_10_loss: 0.4345 - val_dense_5_loss: 0.2162 - val_dense_6_loss: 1.8626 - val_dense_7_loss: 0.7949\nEpoch 22/50\n302390/302390 [==============================] - 50s 164us/step - loss: 3.5123 - dense_10_loss: 0.5775 - dense_5_loss: 0.2316 - dense_6_loss: 1.9085 - dense_7_loss: 0.7947 - val_loss: 3.2432 - val_dense_10_loss: 0.4026 - val_dense_5_loss: 0.1992 - val_dense_6_loss: 1.8462 - val_dense_7_loss: 0.7952\nEpoch 23/50\n302390/302390 [==============================] - 48s 160us/step - loss: 3.4982 - dense_10_loss: 0.5692 - dense_5_loss: 0.2310 - dense_6_loss: 1.9032 - dense_7_loss: 0.7947 - val_loss: 3.2557 - val_dense_10_loss: 0.4130 - val_dense_5_loss: 0.2033 - val_dense_6_loss: 1.8448 - val_dense_7_loss: 0.7947\nEpoch 24/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4957 - dense_10_loss: 0.5709 - dense_5_loss: 0.2308 - dense_6_loss: 1.8994 - dense_7_loss: 0.7946 - val_loss: 3.2335 - val_dense_10_loss: 0.3902 - val_dense_5_loss: 0.1993 - val_dense_6_loss: 1.8491 - val_dense_7_loss: 0.7948\nEpoch 25/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4925 - dense_10_loss: 0.5724 - dense_5_loss: 0.2300 - dense_6_loss: 1.8957 - dense_7_loss: 0.7945 - val_loss: 3.3220 - val_dense_10_loss: 0.4455 - val_dense_5_loss: 0.2284 - val_dense_6_loss: 1.8534 - val_dense_7_loss: 0.7947\nEpoch 26/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.4818 - dense_10_loss: 0.5699 - dense_5_loss: 0.2299 - dense_6_loss: 1.8877 - dense_7_loss: 0.7943 - val_loss: 3.2186 - val_dense_10_loss: 0.3856 - val_dense_5_loss: 0.2041 - val_dense_6_loss: 1.8342 - val_dense_7_loss: 0.7947\nEpoch 27/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4759 - dense_10_loss: 0.5669 - dense_5_loss: 0.2306 - dense_6_loss: 1.8843 - dense_7_loss: 0.7941 - val_loss: 3.2136 - val_dense_10_loss: 0.3890 - val_dense_5_loss: 0.2019 - val_dense_6_loss: 1.8282 - val_dense_7_loss: 0.7945\nEpoch 28/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.4680 - dense_10_loss: 0.5629 - dense_5_loss: 0.2297 - dense_6_loss: 1.8812 - dense_7_loss: 0.7942 - val_loss: 3.1920 - val_dense_10_loss: 0.3757 - val_dense_5_loss: 0.1981 - val_dense_6_loss: 1.8241 - val_dense_7_loss: 0.7941\nEpoch 29/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.4623 - dense_10_loss: 0.5627 - dense_5_loss: 0.2291 - dense_6_loss: 1.8765 - dense_7_loss: 0.7939 - val_loss: 3.2112 - val_dense_10_loss: 0.3880 - val_dense_5_loss: 0.1992 - val_dense_6_loss: 1.8298 - val_dense_7_loss: 0.7942\nEpoch 30/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4557 - dense_10_loss: 0.5631 - dense_5_loss: 0.2278 - dense_6_loss: 1.8709 - dense_7_loss: 0.7939 - val_loss: 3.1753 - val_dense_10_loss: 0.3680 - val_dense_5_loss: 0.1963 - val_dense_6_loss: 1.8169 - val_dense_7_loss: 0.7940\nEpoch 31/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.4506 - dense_10_loss: 0.5616 - dense_5_loss: 0.2277 - dense_6_loss: 1.8675 - dense_7_loss: 0.7938 - val_loss: 3.1927 - val_dense_10_loss: 0.3932 - val_dense_5_loss: 0.1966 - val_dense_6_loss: 1.8085 - val_dense_7_loss: 0.7943\nEpoch 32/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4484 - dense_10_loss: 0.5628 - dense_5_loss: 0.2286 - dense_6_loss: 1.8634 - dense_7_loss: 0.7936 - val_loss: 3.3448 - val_dense_10_loss: 0.4600 - val_dense_5_loss: 0.2383 - val_dense_6_loss: 1.8521 - val_dense_7_loss: 0.7944\nEpoch 33/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.4450 - dense_10_loss: 0.5614 - dense_5_loss: 0.2279 - dense_6_loss: 1.8621 - dense_7_loss: 0.7936 - val_loss: 3.1677 - val_dense_10_loss: 0.3695 - val_dense_5_loss: 0.1989 - val_dense_6_loss: 1.8057 - val_dense_7_loss: 0.7937\nEpoch 34/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4345 - dense_10_loss: 0.5582 - dense_5_loss: 0.2274 - dense_6_loss: 1.8554 - dense_7_loss: 0.7934 - val_loss: 3.2078 - val_dense_10_loss: 0.4072 - val_dense_5_loss: 0.2027 - val_dense_6_loss: 1.8039 - val_dense_7_loss: 0.7940\nEpoch 35/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.4354 - dense_10_loss: 0.5595 - dense_5_loss: 0.2275 - dense_6_loss: 1.8550 - dense_7_loss: 0.7934 - val_loss: 3.2093 - val_dense_10_loss: 0.4150 - val_dense_5_loss: 0.1956 - val_dense_6_loss: 1.8050 - val_dense_7_loss: 0.7937\nEpoch 36/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.4339 - dense_10_loss: 0.5616 - dense_5_loss: 0.2277 - dense_6_loss: 1.8513 - dense_7_loss: 0.7933 - val_loss: 3.3779 - val_dense_10_loss: 0.5000 - val_dense_5_loss: 0.2384 - val_dense_6_loss: 1.8454 - val_dense_7_loss: 0.7941\nEpoch 37/50\n302390/302390 [==============================] - 48s 160us/step - loss: 3.4292 - dense_10_loss: 0.5612 - dense_5_loss: 0.2266 - dense_6_loss: 1.8483 - dense_7_loss: 0.7931 - val_loss: 3.1428 - val_dense_10_loss: 0.3669 - val_dense_5_loss: 0.1941 - val_dense_6_loss: 1.7884 - val_dense_7_loss: 0.7935\nEpoch 38/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.4231 - dense_10_loss: 0.5578 - dense_5_loss: 0.2267 - dense_6_loss: 1.8456 - dense_7_loss: 0.7931 - val_loss: 3.1678 - val_dense_10_loss: 0.3793 - val_dense_5_loss: 0.2012 - val_dense_6_loss: 1.7936 - val_dense_7_loss: 0.7937\nEpoch 39/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4181 - dense_10_loss: 0.5551 - dense_5_loss: 0.2278 - dense_6_loss: 1.8422 - dense_7_loss: 0.7929 - val_loss: 3.3874 - val_dense_10_loss: 0.4987 - val_dense_5_loss: 0.2442 - val_dense_6_loss: 1.8508 - val_dense_7_loss: 0.7937\nEpoch 40/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4169 - dense_10_loss: 0.5603 - dense_5_loss: 0.2256 - dense_6_loss: 1.8382 - dense_7_loss: 0.7928 - val_loss: 3.6667 - val_dense_10_loss: 0.6382 - val_dense_5_loss: 0.2921 - val_dense_6_loss: 1.9421 - val_dense_7_loss: 0.7942\nEpoch 41/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.4161 - dense_10_loss: 0.5609 - dense_5_loss: 0.2255 - dense_6_loss: 1.8369 - dense_7_loss: 0.7928 - val_loss: 3.1505 - val_dense_10_loss: 0.3755 - val_dense_5_loss: 0.1967 - val_dense_6_loss: 1.7851 - val_dense_7_loss: 0.7932\n\nEpoch 00041: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 42/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.3361 - dense_10_loss: 0.5342 - dense_5_loss: 0.2203 - dense_6_loss: 1.7903 - dense_7_loss: 0.7913 - val_loss: 3.0733 - val_dense_10_loss: 0.3507 - val_dense_5_loss: 0.1901 - val_dense_6_loss: 1.7407 - val_dense_7_loss: 0.7919\nEpoch 43/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.3308 - dense_10_loss: 0.5350 - dense_5_loss: 0.2205 - dense_6_loss: 1.7842 - dense_7_loss: 0.7910 - val_loss: 3.0613 - val_dense_10_loss: 0.3420 - val_dense_5_loss: 0.1902 - val_dense_6_loss: 1.7373 - val_dense_7_loss: 0.7918\nEpoch 44/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.3187 - dense_10_loss: 0.5278 - dense_5_loss: 0.2198 - dense_6_loss: 1.7802 - dense_7_loss: 0.7909 - val_loss: 3.0753 - val_dense_10_loss: 0.3573 - val_dense_5_loss: 0.1902 - val_dense_6_loss: 1.7362 - val_dense_7_loss: 0.7917\nEpoch 45/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.3197 - dense_10_loss: 0.5320 - dense_5_loss: 0.2199 - dense_6_loss: 1.7771 - dense_7_loss: 0.7908 - val_loss: 3.0718 - val_dense_10_loss: 0.3603 - val_dense_5_loss: 0.1884 - val_dense_6_loss: 1.7314 - val_dense_7_loss: 0.7918\nEpoch 46/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.3120 - dense_10_loss: 0.5263 - dense_5_loss: 0.2193 - dense_6_loss: 1.7756 - dense_7_loss: 0.7909 - val_loss: 3.0600 - val_dense_10_loss: 0.3474 - val_dense_5_loss: 0.1886 - val_dense_6_loss: 1.7323 - val_dense_7_loss: 0.7916\nEpoch 47/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.3132 - dense_10_loss: 0.5296 - dense_5_loss: 0.2189 - dense_6_loss: 1.7739 - dense_7_loss: 0.7908 - val_loss: 3.0908 - val_dense_10_loss: 0.3681 - val_dense_5_loss: 0.1964 - val_dense_6_loss: 1.7349 - val_dense_7_loss: 0.7915\nEpoch 48/50\n302390/302390 [==============================] - 49s 163us/step - loss: 3.3121 - dense_10_loss: 0.5277 - dense_5_loss: 0.2194 - dense_6_loss: 1.7742 - dense_7_loss: 0.7908 - val_loss: 3.2774 - val_dense_10_loss: 0.4573 - val_dense_5_loss: 0.2376 - val_dense_6_loss: 1.7905 - val_dense_7_loss: 0.7920\nEpoch 49/50\n302390/302390 [==============================] - 49s 162us/step - loss: 3.3106 - dense_10_loss: 0.5299 - dense_5_loss: 0.2189 - dense_6_loss: 1.7710 - dense_7_loss: 0.7908 - val_loss: 3.3405 - val_dense_10_loss: 0.4898 - val_dense_5_loss: 0.2471 - val_dense_6_loss: 1.8115 - val_dense_7_loss: 0.7922\nEpoch 50/50\n302390/302390 [==============================] - 49s 161us/step - loss: 3.3070 - dense_10_loss: 0.5257 - dense_5_loss: 0.2194 - dense_6_loss: 1.7712 - dense_7_loss: 0.7906 - val_loss: 3.0508 - val_dense_10_loss: 0.3410 - val_dense_5_loss: 0.1892 - val_dense_6_loss: 1.7291 - val_dense_7_loss: 0.7915\nTrain on 34944 samples, validate on 8736 samples\nEpoch 1/50\n34944/34944 [==============================] - 8s 218us/step - loss: 33.5846 - dense_10_loss: 30.3167 - dense_5_loss: 0.4343 - dense_6_loss: 2.0866 - dense_7_loss: 0.7469 - val_loss: 11.8449 - val_dense_10_loss: 8.6531 - val_dense_5_loss: 0.4355 - val_dense_6_loss: 2.0310 - val_dense_7_loss: 0.7252\nEpoch 2/50\n34944/34944 [==============================] - 5s 155us/step - loss: 5.1686 - dense_10_loss: 2.2627 - dense_5_loss: 0.3539 - dense_6_loss: 1.8588 - dense_7_loss: 0.6932 - val_loss: 4.4743 - val_dense_10_loss: 1.7271 - val_dense_5_loss: 0.3147 - val_dense_6_loss: 1.7460 - val_dense_7_loss: 0.6865\nEpoch 3/50\n34944/34944 [==============================] - 6s 163us/step - loss: 4.7922 - dense_10_loss: 2.0356 - dense_5_loss: 0.3222 - dense_6_loss: 1.7484 - dense_7_loss: 0.6859 - val_loss: 4.2136 - val_dense_10_loss: 1.5757 - val_dense_5_loss: 0.2967 - val_dense_6_loss: 1.6592 - val_dense_7_loss: 0.6820\nEpoch 4/50\n34944/34944 [==============================] - 5s 157us/step - loss: 4.6203 - dense_10_loss: 1.9482 - dense_5_loss: 0.3069 - dense_6_loss: 1.6835 - dense_7_loss: 0.6817 - val_loss: 4.0073 - val_dense_10_loss: 1.4552 - val_dense_5_loss: 0.2771 - val_dense_6_loss: 1.5956 - val_dense_7_loss: 0.6793\nEpoch 5/50\n34944/34944 [==============================] - 6s 157us/step - loss: 4.4712 - dense_10_loss: 1.8623 - dense_5_loss: 0.2959 - dense_6_loss: 1.6337 - dense_7_loss: 0.6793 - val_loss: 4.0193 - val_dense_10_loss: 1.4981 - val_dense_5_loss: 0.2699 - val_dense_6_loss: 1.5729 - val_dense_7_loss: 0.6784\nEpoch 6/50\n34944/34944 [==============================] - 5s 155us/step - loss: 4.3712 - dense_10_loss: 1.8013 - dense_5_loss: 0.2906 - dense_6_loss: 1.6018 - dense_7_loss: 0.6775 - val_loss: 3.9180 - val_dense_10_loss: 1.4246 - val_dense_5_loss: 0.2628 - val_dense_6_loss: 1.5539 - val_dense_7_loss: 0.6766\nEpoch 7/50\n34944/34944 [==============================] - 5s 156us/step - loss: 4.3159 - dense_10_loss: 1.7892 - dense_5_loss: 0.2819 - dense_6_loss: 1.5686 - dense_7_loss: 0.6762 - val_loss: 3.7187 - val_dense_10_loss: 1.2767 - val_dense_5_loss: 0.2524 - val_dense_6_loss: 1.5138 - val_dense_7_loss: 0.6759\nEpoch 8/50\n34944/34944 [==============================] - 6s 161us/step - loss: 4.2565 - dense_10_loss: 1.7631 - dense_5_loss: 0.2768 - dense_6_loss: 1.5411 - dense_7_loss: 0.6755 - val_loss: 3.7471 - val_dense_10_loss: 1.3328 - val_dense_5_loss: 0.2449 - val_dense_6_loss: 1.4941 - val_dense_7_loss: 0.6754\nEpoch 9/50\n34944/34944 [==============================] - 6s 159us/step - loss: 4.1933 - dense_10_loss: 1.7268 - dense_5_loss: 0.2744 - dense_6_loss: 1.5177 - dense_7_loss: 0.6744 - val_loss: 3.6748 - val_dense_10_loss: 1.2805 - val_dense_5_loss: 0.2482 - val_dense_6_loss: 1.4710 - val_dense_7_loss: 0.6751\nEpoch 10/50\n34944/34944 [==============================] - 6s 158us/step - loss: 4.1537 - dense_10_loss: 1.7105 - dense_5_loss: 0.2687 - dense_6_loss: 1.5004 - dense_7_loss: 0.6741 - val_loss: 3.6278 - val_dense_10_loss: 1.2669 - val_dense_5_loss: 0.2424 - val_dense_6_loss: 1.4442 - val_dense_7_loss: 0.6743\nEpoch 11/50\n34944/34944 [==============================] - 6s 158us/step - loss: 4.0948 - dense_10_loss: 1.6736 - dense_5_loss: 0.2686 - dense_6_loss: 1.4789 - dense_7_loss: 0.6737 - val_loss: 3.6188 - val_dense_10_loss: 1.2604 - val_dense_5_loss: 0.2441 - val_dense_6_loss: 1.4402 - val_dense_7_loss: 0.6741\nEpoch 12/50\n34944/34944 [==============================] - 5s 154us/step - loss: 4.0403 - dense_10_loss: 1.6423 - dense_5_loss: 0.2635 - dense_6_loss: 1.4614 - dense_7_loss: 0.6731 - val_loss: 3.5744 - val_dense_10_loss: 1.2452 - val_dense_5_loss: 0.2397 - val_dense_6_loss: 1.4148 - val_dense_7_loss: 0.6747\nEpoch 13/50\n34944/34944 [==============================] - 5s 155us/step - loss: 4.0522 - dense_10_loss: 1.6604 - dense_5_loss: 0.2644 - dense_6_loss: 1.4544 - dense_7_loss: 0.6731 - val_loss: 3.5161 - val_dense_10_loss: 1.1971 - val_dense_5_loss: 0.2398 - val_dense_6_loss: 1.4069 - val_dense_7_loss: 0.6723\nEpoch 14/50\n34944/34944 [==============================] - 6s 161us/step - loss: 3.9889 - dense_10_loss: 1.6230 - dense_5_loss: 0.2589 - dense_6_loss: 1.4345 - dense_7_loss: 0.6725 - val_loss: 3.4553 - val_dense_10_loss: 1.1615 - val_dense_5_loss: 0.2291 - val_dense_6_loss: 1.3917 - val_dense_7_loss: 0.6730\nEpoch 15/50\n34944/34944 [==============================] - 5s 157us/step - loss: 3.9472 - dense_10_loss: 1.5970 - dense_5_loss: 0.2563 - dense_6_loss: 1.4217 - dense_7_loss: 0.6722 - val_loss: 3.4350 - val_dense_10_loss: 1.1514 - val_dense_5_loss: 0.2298 - val_dense_6_loss: 1.3811 - val_dense_7_loss: 0.6726\nEpoch 16/50\n34944/34944 [==============================] - 5s 154us/step - loss: 3.9103 - dense_10_loss: 1.5669 - dense_5_loss: 0.2587 - dense_6_loss: 1.4126 - dense_7_loss: 0.6721 - val_loss: 3.4770 - val_dense_10_loss: 1.2023 - val_dense_5_loss: 0.2291 - val_dense_6_loss: 1.3723 - val_dense_7_loss: 0.6733\nEpoch 17/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.8940 - dense_10_loss: 1.5688 - dense_5_loss: 0.2513 - dense_6_loss: 1.4019 - dense_7_loss: 0.6720 - val_loss: 3.4123 - val_dense_10_loss: 1.1578 - val_dense_5_loss: 0.2274 - val_dense_6_loss: 1.3547 - val_dense_7_loss: 0.6724\nEpoch 18/50\n34944/34944 [==============================] - 5s 155us/step - loss: 3.9184 - dense_10_loss: 1.6057 - dense_5_loss: 0.2503 - dense_6_loss: 1.3905 - dense_7_loss: 0.6720 - val_loss: 3.3684 - val_dense_10_loss: 1.1166 - val_dense_5_loss: 0.2235 - val_dense_6_loss: 1.3554 - val_dense_7_loss: 0.6729\nEpoch 19/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.8866 - dense_10_loss: 1.5786 - dense_5_loss: 0.2498 - dense_6_loss: 1.3863 - dense_7_loss: 0.6718 - val_loss: 3.4653 - val_dense_10_loss: 1.2132 - val_dense_5_loss: 0.2259 - val_dense_6_loss: 1.3546 - val_dense_7_loss: 0.6717\nEpoch 20/50\n34944/34944 [==============================] - 6s 161us/step - loss: 3.8162 - dense_10_loss: 1.5203 - dense_5_loss: 0.2484 - dense_6_loss: 1.3758 - dense_7_loss: 0.6716 - val_loss: 3.3440 - val_dense_10_loss: 1.1063 - val_dense_5_loss: 0.2231 - val_dense_6_loss: 1.3420 - val_dense_7_loss: 0.6726\nEpoch 21/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.8300 - dense_10_loss: 1.5437 - dense_5_loss: 0.2459 - dense_6_loss: 1.3688 - dense_7_loss: 0.6716 - val_loss: 3.3782 - val_dense_10_loss: 1.1464 - val_dense_5_loss: 0.2247 - val_dense_6_loss: 1.3348 - val_dense_7_loss: 0.6722\nEpoch 22/50\n34944/34944 [==============================] - 5s 157us/step - loss: 3.8200 - dense_10_loss: 1.5471 - dense_5_loss: 0.2437 - dense_6_loss: 1.3577 - dense_7_loss: 0.6716 - val_loss: 3.3108 - val_dense_10_loss: 1.1002 - val_dense_5_loss: 0.2133 - val_dense_6_loss: 1.3251 - val_dense_7_loss: 0.6721\nEpoch 23/50\n34944/34944 [==============================] - 5s 155us/step - loss: 3.8122 - dense_10_loss: 1.5420 - dense_5_loss: 0.2442 - dense_6_loss: 1.3548 - dense_7_loss: 0.6712 - val_loss: 3.3231 - val_dense_10_loss: 1.0842 - val_dense_5_loss: 0.2230 - val_dense_6_loss: 1.3434 - val_dense_7_loss: 0.6725\nEpoch 24/50\n34944/34944 [==============================] - 5s 155us/step - loss: 3.7947 - dense_10_loss: 1.5328 - dense_5_loss: 0.2435 - dense_6_loss: 1.3476 - dense_7_loss: 0.6709 - val_loss: 3.2786 - val_dense_10_loss: 1.0655 - val_dense_5_loss: 0.2165 - val_dense_6_loss: 1.3237 - val_dense_7_loss: 0.6729\nEpoch 25/50\n34944/34944 [==============================] - 6s 162us/step - loss: 3.7410 - dense_10_loss: 1.4905 - dense_5_loss: 0.2423 - dense_6_loss: 1.3370 - dense_7_loss: 0.6711 - val_loss: 3.3376 - val_dense_10_loss: 1.1311 - val_dense_5_loss: 0.2154 - val_dense_6_loss: 1.3188 - val_dense_7_loss: 0.6722\nEpoch 26/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.7479 - dense_10_loss: 1.5030 - dense_5_loss: 0.2403 - dense_6_loss: 1.3339 - dense_7_loss: 0.6708 - val_loss: 3.3878 - val_dense_10_loss: 1.1877 - val_dense_5_loss: 0.2245 - val_dense_6_loss: 1.3035 - val_dense_7_loss: 0.6721\nEpoch 27/50\n34944/34944 [==============================] - 6s 159us/step - loss: 3.7325 - dense_10_loss: 1.4943 - dense_5_loss: 0.2391 - dense_6_loss: 1.3284 - dense_7_loss: 0.6706 - val_loss: 3.2760 - val_dense_10_loss: 1.0716 - val_dense_5_loss: 0.2201 - val_dense_6_loss: 1.3108 - val_dense_7_loss: 0.6734\nEpoch 28/50\n34944/34944 [==============================] - 5s 155us/step - loss: 3.7037 - dense_10_loss: 1.4782 - dense_5_loss: 0.2378 - dense_6_loss: 1.3170 - dense_7_loss: 0.6707 - val_loss: 3.2312 - val_dense_10_loss: 1.0502 - val_dense_5_loss: 0.2152 - val_dense_6_loss: 1.2941 - val_dense_7_loss: 0.6716\nEpoch 29/50\n34944/34944 [==============================] - 5s 154us/step - loss: 3.7469 - dense_10_loss: 1.5251 - dense_5_loss: 0.2375 - dense_6_loss: 1.3139 - dense_7_loss: 0.6704 - val_loss: 3.2211 - val_dense_10_loss: 1.0447 - val_dense_5_loss: 0.2125 - val_dense_6_loss: 1.2923 - val_dense_7_loss: 0.6715\nEpoch 30/50\n34944/34944 [==============================] - 5s 157us/step - loss: 3.7331 - dense_10_loss: 1.5207 - dense_5_loss: 0.2380 - dense_6_loss: 1.3039 - dense_7_loss: 0.6704 - val_loss: 3.2649 - val_dense_10_loss: 1.0759 - val_dense_5_loss: 0.2149 - val_dense_6_loss: 1.3024 - val_dense_7_loss: 0.6717\nEpoch 31/50\n34944/34944 [==============================] - 6s 161us/step - loss: 3.7316 - dense_10_loss: 1.5261 - dense_5_loss: 0.2359 - dense_6_loss: 1.2993 - dense_7_loss: 0.6702 - val_loss: 3.4649 - val_dense_10_loss: 1.2846 - val_dense_5_loss: 0.2168 - val_dense_6_loss: 1.2919 - val_dense_7_loss: 0.6716\nEpoch 32/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.7093 - dense_10_loss: 1.5039 - dense_5_loss: 0.2351 - dense_6_loss: 1.2998 - dense_7_loss: 0.6704 - val_loss: 3.2684 - val_dense_10_loss: 1.1014 - val_dense_5_loss: 0.2107 - val_dense_6_loss: 1.2841 - val_dense_7_loss: 0.6721\nEpoch 33/50\n34944/34944 [==============================] - 6s 161us/step - loss: 3.6878 - dense_10_loss: 1.4912 - dense_5_loss: 0.2362 - dense_6_loss: 1.2904 - dense_7_loss: 0.6700 - val_loss: 3.2856 - val_dense_10_loss: 1.1043 - val_dense_5_loss: 0.2130 - val_dense_6_loss: 1.2964 - val_dense_7_loss: 0.6719\n\nEpoch 00033: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 34/50\n34944/34944 [==============================] - 6s 159us/step - loss: 3.5802 - dense_10_loss: 1.4414 - dense_5_loss: 0.2264 - dense_6_loss: 1.2452 - dense_7_loss: 0.6672 - val_loss: 3.1334 - val_dense_10_loss: 1.0232 - val_dense_5_loss: 0.2023 - val_dense_6_loss: 1.2387 - val_dense_7_loss: 0.6691\nEpoch 35/50\n34944/34944 [==============================] - 6s 160us/step - loss: 3.4812 - dense_10_loss: 1.3671 - dense_5_loss: 0.2200 - dense_6_loss: 1.2273 - dense_7_loss: 0.6668 - val_loss: 3.0542 - val_dense_10_loss: 0.9595 - val_dense_5_loss: 0.1985 - val_dense_6_loss: 1.2268 - val_dense_7_loss: 0.6694\nEpoch 36/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.5293 - dense_10_loss: 1.4176 - dense_5_loss: 0.2219 - dense_6_loss: 1.2231 - dense_7_loss: 0.6666 - val_loss: 3.0109 - val_dense_10_loss: 0.9239 - val_dense_5_loss: 0.1956 - val_dense_6_loss: 1.2224 - val_dense_7_loss: 0.6690\nEpoch 37/50\n34944/34944 [==============================] - 6s 166us/step - loss: 3.4574 - dense_10_loss: 1.3531 - dense_5_loss: 0.2219 - dense_6_loss: 1.2159 - dense_7_loss: 0.6665 - val_loss: 3.0335 - val_dense_10_loss: 0.9421 - val_dense_5_loss: 0.1977 - val_dense_6_loss: 1.2246 - val_dense_7_loss: 0.6690\nEpoch 38/50\n34944/34944 [==============================] - 6s 161us/step - loss: 3.4327 - dense_10_loss: 1.3405 - dense_5_loss: 0.2178 - dense_6_loss: 1.2079 - dense_7_loss: 0.6665 - val_loss: 3.0434 - val_dense_10_loss: 0.9508 - val_dense_5_loss: 0.2001 - val_dense_6_loss: 1.2236 - val_dense_7_loss: 0.6689\nEpoch 39/50\n34944/34944 [==============================] - 6s 161us/step - loss: 3.4359 - dense_10_loss: 1.3409 - dense_5_loss: 0.2173 - dense_6_loss: 1.2113 - dense_7_loss: 0.6664 - val_loss: 3.0865 - val_dense_10_loss: 1.0075 - val_dense_5_loss: 0.1969 - val_dense_6_loss: 1.2130 - val_dense_7_loss: 0.6691\nEpoch 40/50\n34944/34944 [==============================] - 6s 159us/step - loss: 3.4764 - dense_10_loss: 1.3792 - dense_5_loss: 0.2196 - dense_6_loss: 1.2114 - dense_7_loss: 0.6662 - val_loss: 3.0468 - val_dense_10_loss: 0.9659 - val_dense_5_loss: 0.1960 - val_dense_6_loss: 1.2159 - val_dense_7_loss: 0.6690\n\nEpoch 00040: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\nEpoch 41/50\n34944/34944 [==============================] - 6s 160us/step - loss: 3.4284 - dense_10_loss: 1.3520 - dense_5_loss: 0.2167 - dense_6_loss: 1.1943 - dense_7_loss: 0.6653 - val_loss: 2.9910 - val_dense_10_loss: 0.9232 - val_dense_5_loss: 0.1929 - val_dense_6_loss: 1.2066 - val_dense_7_loss: 0.6684\nEpoch 42/50\n34944/34944 [==============================] - 6s 163us/step - loss: 3.4067 - dense_10_loss: 1.3349 - dense_5_loss: 0.2150 - dense_6_loss: 1.1916 - dense_7_loss: 0.6652 - val_loss: 2.9977 - val_dense_10_loss: 0.9345 - val_dense_5_loss: 0.1931 - val_dense_6_loss: 1.2017 - val_dense_7_loss: 0.6683\nEpoch 43/50\n34944/34944 [==============================] - 6s 159us/step - loss: 3.4099 - dense_10_loss: 1.3472 - dense_5_loss: 0.2151 - dense_6_loss: 1.1824 - dense_7_loss: 0.6652 - val_loss: 2.9746 - val_dense_10_loss: 0.9136 - val_dense_5_loss: 0.1917 - val_dense_6_loss: 1.2009 - val_dense_7_loss: 0.6683\nEpoch 44/50\n34944/34944 [==============================] - 6s 160us/step - loss: 3.3841 - dense_10_loss: 1.3210 - dense_5_loss: 0.2127 - dense_6_loss: 1.1852 - dense_7_loss: 0.6651 - val_loss: 2.9949 - val_dense_10_loss: 0.9352 - val_dense_5_loss: 0.1924 - val_dense_6_loss: 1.1991 - val_dense_7_loss: 0.6683\nEpoch 45/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.3870 - dense_10_loss: 1.3288 - dense_5_loss: 0.2122 - dense_6_loss: 1.1811 - dense_7_loss: 0.6649 - val_loss: 2.9845 - val_dense_10_loss: 0.9255 - val_dense_5_loss: 0.1917 - val_dense_6_loss: 1.1991 - val_dense_7_loss: 0.6682\nEpoch 46/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.3976 - dense_10_loss: 1.3407 - dense_5_loss: 0.2111 - dense_6_loss: 1.1806 - dense_7_loss: 0.6652 - val_loss: 2.9732 - val_dense_10_loss: 0.9163 - val_dense_5_loss: 0.1916 - val_dense_6_loss: 1.1973 - val_dense_7_loss: 0.6681\nEpoch 47/50\n34944/34944 [==============================] - 6s 160us/step - loss: 3.4267 - dense_10_loss: 1.3729 - dense_5_loss: 0.2121 - dense_6_loss: 1.1765 - dense_7_loss: 0.6652 - val_loss: 2.9740 - val_dense_10_loss: 0.9175 - val_dense_5_loss: 0.1917 - val_dense_6_loss: 1.1964 - val_dense_7_loss: 0.6683\nEpoch 48/50\n34944/34944 [==============================] - 6s 168us/step - loss: 3.3358 - dense_10_loss: 1.2780 - dense_5_loss: 0.2153 - dense_6_loss: 1.1775 - dense_7_loss: 0.6650 - val_loss: 2.9945 - val_dense_10_loss: 0.9375 - val_dense_5_loss: 0.1911 - val_dense_6_loss: 1.1978 - val_dense_7_loss: 0.6682\nEpoch 49/50\n34944/34944 [==============================] - 6s 160us/step - loss: 3.3810 - dense_10_loss: 1.3248 - dense_5_loss: 0.2130 - dense_6_loss: 1.1782 - dense_7_loss: 0.6650 - val_loss: 2.9709 - val_dense_10_loss: 0.9154 - val_dense_5_loss: 0.1919 - val_dense_6_loss: 1.1954 - val_dense_7_loss: 0.6683\nEpoch 50/50\n34944/34944 [==============================] - 6s 158us/step - loss: 3.3975 - dense_10_loss: 1.3501 - dense_5_loss: 0.2119 - dense_6_loss: 1.1706 - dense_7_loss: 0.6649 - val_loss: 2.9605 - val_dense_10_loss: 0.9069 - val_dense_5_loss: 0.1907 - val_dense_6_loss: 1.1946 - val_dense_7_loss: 0.6683\nTrain on 95247 samples, validate on 23812 samples\nEpoch 1/50\n95247/95247 [==============================] - 18s 187us/step - loss: 4.3381 - dense_10_loss: 1.1217 - dense_5_loss: 0.3832 - dense_6_loss: 2.0784 - dense_7_loss: 0.7548 - val_loss: 3.5341 - val_dense_10_loss: 0.6785 - val_dense_5_loss: 0.2902 - val_dense_6_loss: 1.8459 - val_dense_7_loss: 0.7196\nEpoch 2/50\n95247/95247 [==============================] - 15s 156us/step - loss: 3.6244 - dense_10_loss: 0.7550 - dense_5_loss: 0.3025 - dense_6_loss: 1.8482 - dense_7_loss: 0.7187 - val_loss: 3.3051 - val_dense_10_loss: 0.5883 - val_dense_5_loss: 0.2670 - val_dense_6_loss: 1.7358 - val_dense_7_loss: 0.7141\nEpoch 3/50\n95247/95247 [==============================] - 15s 159us/step - loss: 3.4441 - dense_10_loss: 0.6843 - dense_5_loss: 0.2819 - dense_6_loss: 1.7630 - dense_7_loss: 0.7149 - val_loss: 3.1607 - val_dense_10_loss: 0.5322 - val_dense_5_loss: 0.2400 - val_dense_6_loss: 1.6753 - val_dense_7_loss: 0.7131\nEpoch 4/50\n95247/95247 [==============================] - 15s 160us/step - loss: 3.3303 - dense_10_loss: 0.6449 - dense_5_loss: 0.2680 - dense_6_loss: 1.7037 - dense_7_loss: 0.7137 - val_loss: 3.0437 - val_dense_10_loss: 0.4796 - val_dense_5_loss: 0.2365 - val_dense_6_loss: 1.6163 - val_dense_7_loss: 0.7114\nEpoch 5/50\n95247/95247 [==============================] - 15s 163us/step - loss: 3.2550 - dense_10_loss: 0.6182 - dense_5_loss: 0.2602 - dense_6_loss: 1.6634 - dense_7_loss: 0.7132 - val_loss: 3.0181 - val_dense_10_loss: 0.4858 - val_dense_5_loss: 0.2242 - val_dense_6_loss: 1.5977 - val_dense_7_loss: 0.7105\nEpoch 6/50\n95247/95247 [==============================] - 15s 161us/step - loss: 3.1924 - dense_10_loss: 0.5996 - dense_5_loss: 0.2523 - dense_6_loss: 1.6275 - dense_7_loss: 0.7130 - val_loss: 2.9212 - val_dense_10_loss: 0.4333 - val_dense_5_loss: 0.2192 - val_dense_6_loss: 1.5584 - val_dense_7_loss: 0.7104\nEpoch 7/50\n95247/95247 [==============================] - 16s 164us/step - loss: 3.1427 - dense_10_loss: 0.5805 - dense_5_loss: 0.2473 - dense_6_loss: 1.6021 - dense_7_loss: 0.7128 - val_loss: 2.8802 - val_dense_10_loss: 0.4220 - val_dense_5_loss: 0.2146 - val_dense_6_loss: 1.5331 - val_dense_7_loss: 0.7105\nEpoch 8/50\n95247/95247 [==============================] - 15s 158us/step - loss: 3.1034 - dense_10_loss: 0.5670 - dense_5_loss: 0.2439 - dense_6_loss: 1.5800 - dense_7_loss: 0.7126 - val_loss: 2.8465 - val_dense_10_loss: 0.4071 - val_dense_5_loss: 0.2084 - val_dense_6_loss: 1.5200 - val_dense_7_loss: 0.7110\nEpoch 9/50\n95247/95247 [==============================] - 15s 161us/step - loss: 3.0773 - dense_10_loss: 0.5631 - dense_5_loss: 0.2391 - dense_6_loss: 1.5624 - dense_7_loss: 0.7126 - val_loss: 2.8051 - val_dense_10_loss: 0.3911 - val_dense_5_loss: 0.2082 - val_dense_6_loss: 1.4949 - val_dense_7_loss: 0.7110\nEpoch 10/50\n95247/95247 [==============================] - 15s 158us/step - loss: 3.0436 - dense_10_loss: 0.5542 - dense_5_loss: 0.2371 - dense_6_loss: 1.5399 - dense_7_loss: 0.7124 - val_loss: 2.7799 - val_dense_10_loss: 0.3743 - val_dense_5_loss: 0.2092 - val_dense_6_loss: 1.4867 - val_dense_7_loss: 0.7097\nEpoch 11/50\n95247/95247 [==============================] - 15s 161us/step - loss: 3.0254 - dense_10_loss: 0.5534 - dense_5_loss: 0.2348 - dense_6_loss: 1.5249 - dense_7_loss: 0.7124 - val_loss: 2.7564 - val_dense_10_loss: 0.3887 - val_dense_5_loss: 0.1997 - val_dense_6_loss: 1.4574 - val_dense_7_loss: 0.7107\nEpoch 12/50\n95247/95247 [==============================] - 15s 159us/step - loss: 2.9932 - dense_10_loss: 0.5395 - dense_5_loss: 0.2307 - dense_6_loss: 1.5107 - dense_7_loss: 0.7123 - val_loss: 2.7422 - val_dense_10_loss: 0.3634 - val_dense_5_loss: 0.2069 - val_dense_6_loss: 1.4620 - val_dense_7_loss: 0.7100\nEpoch 13/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.9870 - dense_10_loss: 0.5438 - dense_5_loss: 0.2304 - dense_6_loss: 1.5006 - dense_7_loss: 0.7122 - val_loss: 2.7200 - val_dense_10_loss: 0.3658 - val_dense_5_loss: 0.2027 - val_dense_6_loss: 1.4418 - val_dense_7_loss: 0.7097\nEpoch 14/50\n95247/95247 [==============================] - 15s 160us/step - loss: 2.9606 - dense_10_loss: 0.5328 - dense_5_loss: 0.2276 - dense_6_loss: 1.4881 - dense_7_loss: 0.7120 - val_loss: 2.6933 - val_dense_10_loss: 0.3598 - val_dense_5_loss: 0.1938 - val_dense_6_loss: 1.4303 - val_dense_7_loss: 0.7094\nEpoch 15/50\n95247/95247 [==============================] - 16s 164us/step - loss: 2.9461 - dense_10_loss: 0.5283 - dense_5_loss: 0.2260 - dense_6_loss: 1.4797 - dense_7_loss: 0.7121 - val_loss: 2.7180 - val_dense_10_loss: 0.3857 - val_dense_5_loss: 0.1956 - val_dense_6_loss: 1.4276 - val_dense_7_loss: 0.7091\nEpoch 16/50\n95247/95247 [==============================] - 15s 159us/step - loss: 2.9414 - dense_10_loss: 0.5365 - dense_5_loss: 0.2239 - dense_6_loss: 1.4688 - dense_7_loss: 0.7121 - val_loss: 2.6670 - val_dense_10_loss: 0.3439 - val_dense_5_loss: 0.1966 - val_dense_6_loss: 1.4171 - val_dense_7_loss: 0.7095\nEpoch 17/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.9160 - dense_10_loss: 0.5213 - dense_5_loss: 0.2229 - dense_6_loss: 1.4598 - dense_7_loss: 0.7120 - val_loss: 2.6767 - val_dense_10_loss: 0.3647 - val_dense_5_loss: 0.1954 - val_dense_6_loss: 1.4072 - val_dense_7_loss: 0.7093\nEpoch 18/50\n95247/95247 [==============================] - 15s 160us/step - loss: 2.9050 - dense_10_loss: 0.5193 - dense_5_loss: 0.2229 - dense_6_loss: 1.4510 - dense_7_loss: 0.7118 - val_loss: 2.6292 - val_dense_10_loss: 0.3369 - val_dense_5_loss: 0.1923 - val_dense_6_loss: 1.3908 - val_dense_7_loss: 0.7092\nEpoch 19/50\n95247/95247 [==============================] - 15s 160us/step - loss: 2.8886 - dense_10_loss: 0.5140 - dense_5_loss: 0.2207 - dense_6_loss: 1.4422 - dense_7_loss: 0.7117 - val_loss: 2.6457 - val_dense_10_loss: 0.3508 - val_dense_5_loss: 0.1947 - val_dense_6_loss: 1.3908 - val_dense_7_loss: 0.7092\nEpoch 20/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.8798 - dense_10_loss: 0.5138 - dense_5_loss: 0.2200 - dense_6_loss: 1.4342 - dense_7_loss: 0.7118 - val_loss: 2.6177 - val_dense_10_loss: 0.3348 - val_dense_5_loss: 0.1877 - val_dense_6_loss: 1.3857 - val_dense_7_loss: 0.7095\nEpoch 21/50\n95247/95247 [==============================] - 16s 166us/step - loss: 2.8688 - dense_10_loss: 0.5123 - dense_5_loss: 0.2180 - dense_6_loss: 1.4268 - dense_7_loss: 0.7117 - val_loss: 2.6153 - val_dense_10_loss: 0.3345 - val_dense_5_loss: 0.1896 - val_dense_6_loss: 1.3817 - val_dense_7_loss: 0.7095\nEpoch 22/50\n95247/95247 [==============================] - 16s 164us/step - loss: 2.8656 - dense_10_loss: 0.5131 - dense_5_loss: 0.2178 - dense_6_loss: 1.4231 - dense_7_loss: 0.7116 - val_loss: 2.6164 - val_dense_10_loss: 0.3372 - val_dense_5_loss: 0.1841 - val_dense_6_loss: 1.3861 - val_dense_7_loss: 0.7090\nEpoch 23/50\n95247/95247 [==============================] - 16s 165us/step - loss: 2.8547 - dense_10_loss: 0.5089 - dense_5_loss: 0.2172 - dense_6_loss: 1.4169 - dense_7_loss: 0.7116 - val_loss: 2.6018 - val_dense_10_loss: 0.3284 - val_dense_5_loss: 0.1867 - val_dense_6_loss: 1.3768 - val_dense_7_loss: 0.7098\nEpoch 24/50\n95247/95247 [==============================] - 16s 164us/step - loss: 2.8540 - dense_10_loss: 0.5137 - dense_5_loss: 0.2152 - dense_6_loss: 1.4137 - dense_7_loss: 0.7115 - val_loss: 2.6035 - val_dense_10_loss: 0.3279 - val_dense_5_loss: 0.1902 - val_dense_6_loss: 1.3762 - val_dense_7_loss: 0.7092\nEpoch 25/50\n95247/95247 [==============================] - 16s 165us/step - loss: 2.8380 - dense_10_loss: 0.5072 - dense_5_loss: 0.2148 - dense_6_loss: 1.4045 - dense_7_loss: 0.7114 - val_loss: 2.5928 - val_dense_10_loss: 0.3362 - val_dense_5_loss: 0.1821 - val_dense_6_loss: 1.3652 - val_dense_7_loss: 0.7092\nEpoch 26/50\n95247/95247 [==============================] - 16s 166us/step - loss: 2.8240 - dense_10_loss: 0.5005 - dense_5_loss: 0.2147 - dense_6_loss: 1.3973 - dense_7_loss: 0.7114 - val_loss: 2.5964 - val_dense_10_loss: 0.3401 - val_dense_5_loss: 0.1828 - val_dense_6_loss: 1.3645 - val_dense_7_loss: 0.7091\nEpoch 27/50\n95247/95247 [==============================] - 16s 167us/step - loss: 2.8236 - dense_10_loss: 0.5052 - dense_5_loss: 0.2136 - dense_6_loss: 1.3934 - dense_7_loss: 0.7113 - val_loss: 2.6001 - val_dense_10_loss: 0.3389 - val_dense_5_loss: 0.1858 - val_dense_6_loss: 1.3657 - val_dense_7_loss: 0.7096\nEpoch 28/50\n95247/95247 [==============================] - 15s 162us/step - loss: 2.8172 - dense_10_loss: 0.5056 - dense_5_loss: 0.2126 - dense_6_loss: 1.3876 - dense_7_loss: 0.7113 - val_loss: 2.5915 - val_dense_10_loss: 0.3370 - val_dense_5_loss: 0.1839 - val_dense_6_loss: 1.3614 - val_dense_7_loss: 0.7092\nEpoch 29/50\n95247/95247 [==============================] - 16s 165us/step - loss: 2.8095 - dense_10_loss: 0.5025 - dense_5_loss: 0.2128 - dense_6_loss: 1.3829 - dense_7_loss: 0.7112 - val_loss: 2.5566 - val_dense_10_loss: 0.3258 - val_dense_5_loss: 0.1797 - val_dense_6_loss: 1.3420 - val_dense_7_loss: 0.7091\nEpoch 30/50\n95247/95247 [==============================] - 15s 159us/step - loss: 2.7980 - dense_10_loss: 0.4972 - dense_5_loss: 0.2118 - dense_6_loss: 1.3778 - dense_7_loss: 0.7111 - val_loss: 2.5420 - val_dense_10_loss: 0.3081 - val_dense_5_loss: 0.1832 - val_dense_6_loss: 1.3418 - val_dense_7_loss: 0.7089\nEpoch 31/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.7969 - dense_10_loss: 0.4995 - dense_5_loss: 0.2128 - dense_6_loss: 1.3735 - dense_7_loss: 0.7111 - val_loss: 2.5476 - val_dense_10_loss: 0.3111 - val_dense_5_loss: 0.1820 - val_dense_6_loss: 1.3456 - val_dense_7_loss: 0.7089\nEpoch 32/50\n95247/95247 [==============================] - 15s 158us/step - loss: 2.7847 - dense_10_loss: 0.4925 - dense_5_loss: 0.2111 - dense_6_loss: 1.3700 - dense_7_loss: 0.7110 - val_loss: 2.5738 - val_dense_10_loss: 0.3383 - val_dense_5_loss: 0.1831 - val_dense_6_loss: 1.3437 - val_dense_7_loss: 0.7087\nEpoch 33/50\n95247/95247 [==============================] - 16s 163us/step - loss: 2.7844 - dense_10_loss: 0.4973 - dense_5_loss: 0.2100 - dense_6_loss: 1.3660 - dense_7_loss: 0.7110 - val_loss: 2.5393 - val_dense_10_loss: 0.3131 - val_dense_5_loss: 0.1812 - val_dense_6_loss: 1.3361 - val_dense_7_loss: 0.7089\nEpoch 34/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.7721 - dense_10_loss: 0.4908 - dense_5_loss: 0.2108 - dense_6_loss: 1.3596 - dense_7_loss: 0.7109 - val_loss: 2.5647 - val_dense_10_loss: 0.3379 - val_dense_5_loss: 0.1834 - val_dense_6_loss: 1.3345 - val_dense_7_loss: 0.7090\nEpoch 35/50\n95247/95247 [==============================] - 16s 165us/step - loss: 2.7795 - dense_10_loss: 0.4967 - dense_5_loss: 0.2112 - dense_6_loss: 1.3608 - dense_7_loss: 0.7108 - val_loss: 2.5480 - val_dense_10_loss: 0.3316 - val_dense_5_loss: 0.1793 - val_dense_6_loss: 1.3284 - val_dense_7_loss: 0.7087\nEpoch 36/50\n95247/95247 [==============================] - 16s 166us/step - loss: 2.7579 - dense_10_loss: 0.4862 - dense_5_loss: 0.2090 - dense_6_loss: 1.3518 - dense_7_loss: 0.7109 - val_loss: 2.5461 - val_dense_10_loss: 0.3240 - val_dense_5_loss: 0.1798 - val_dense_6_loss: 1.3337 - val_dense_7_loss: 0.7086\nEpoch 37/50\n95247/95247 [==============================] - 16s 164us/step - loss: 2.7640 - dense_10_loss: 0.4934 - dense_5_loss: 0.2094 - dense_6_loss: 1.3506 - dense_7_loss: 0.7107 - val_loss: 2.5301 - val_dense_10_loss: 0.3093 - val_dense_5_loss: 0.1813 - val_dense_6_loss: 1.3309 - val_dense_7_loss: 0.7086\nEpoch 38/50\n95247/95247 [==============================] - 16s 165us/step - loss: 2.7610 - dense_10_loss: 0.4944 - dense_5_loss: 0.2079 - dense_6_loss: 1.3479 - dense_7_loss: 0.7107 - val_loss: 2.5457 - val_dense_10_loss: 0.3182 - val_dense_5_loss: 0.1827 - val_dense_6_loss: 1.3361 - val_dense_7_loss: 0.7087\nEpoch 39/50\n95247/95247 [==============================] - 16s 167us/step - loss: 2.7569 - dense_10_loss: 0.4961 - dense_5_loss: 0.2082 - dense_6_loss: 1.3419 - dense_7_loss: 0.7107 - val_loss: 2.5212 - val_dense_10_loss: 0.3076 - val_dense_5_loss: 0.1787 - val_dense_6_loss: 1.3259 - val_dense_7_loss: 0.7090\nEpoch 40/50\n95247/95247 [==============================] - 16s 166us/step - loss: 2.7496 - dense_10_loss: 0.4921 - dense_5_loss: 0.2084 - dense_6_loss: 1.3386 - dense_7_loss: 0.7105 - val_loss: 2.5415 - val_dense_10_loss: 0.3341 - val_dense_5_loss: 0.1825 - val_dense_6_loss: 1.3167 - val_dense_7_loss: 0.7083\nEpoch 41/50\n95247/95247 [==============================] - 15s 162us/step - loss: 2.7280 - dense_10_loss: 0.4753 - dense_5_loss: 0.2066 - dense_6_loss: 1.3356 - dense_7_loss: 0.7105 - val_loss: 2.5079 - val_dense_10_loss: 0.3060 - val_dense_5_loss: 0.1795 - val_dense_6_loss: 1.3137 - val_dense_7_loss: 0.7087\nEpoch 42/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.7363 - dense_10_loss: 0.4883 - dense_5_loss: 0.2066 - dense_6_loss: 1.3309 - dense_7_loss: 0.7105 - val_loss: 2.5304 - val_dense_10_loss: 0.3171 - val_dense_5_loss: 0.1809 - val_dense_6_loss: 1.3237 - val_dense_7_loss: 0.7087\nEpoch 43/50\n95247/95247 [==============================] - 15s 162us/step - loss: 2.7301 - dense_10_loss: 0.4831 - dense_5_loss: 0.2066 - dense_6_loss: 1.3302 - dense_7_loss: 0.7103 - val_loss: 2.5512 - val_dense_10_loss: 0.3545 - val_dense_5_loss: 0.1741 - val_dense_6_loss: 1.3136 - val_dense_7_loss: 0.7091\nEpoch 44/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.7403 - dense_10_loss: 0.4970 - dense_5_loss: 0.2062 - dense_6_loss: 1.3268 - dense_7_loss: 0.7104 - val_loss: 2.5333 - val_dense_10_loss: 0.3355 - val_dense_5_loss: 0.1770 - val_dense_6_loss: 1.3118 - val_dense_7_loss: 0.7090\nEpoch 45/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.7253 - dense_10_loss: 0.4835 - dense_5_loss: 0.2060 - dense_6_loss: 1.3254 - dense_7_loss: 0.7103 - val_loss: 2.5276 - val_dense_10_loss: 0.3242 - val_dense_5_loss: 0.1800 - val_dense_6_loss: 1.3150 - val_dense_7_loss: 0.7084\n\nEpoch 00045: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 46/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.6550 - dense_10_loss: 0.4694 - dense_5_loss: 0.1984 - dense_6_loss: 1.2789 - dense_7_loss: 0.7084 - val_loss: 2.4218 - val_dense_10_loss: 0.2813 - val_dense_5_loss: 0.1664 - val_dense_6_loss: 1.2676 - val_dense_7_loss: 0.7065\nEpoch 47/50\n95247/95247 [==============================] - 15s 160us/step - loss: 2.6286 - dense_10_loss: 0.4580 - dense_5_loss: 0.1968 - dense_6_loss: 1.2658 - dense_7_loss: 0.7081 - val_loss: 2.4068 - val_dense_10_loss: 0.2719 - val_dense_5_loss: 0.1656 - val_dense_6_loss: 1.2629 - val_dense_7_loss: 0.7063\nEpoch 48/50\n95247/95247 [==============================] - 16s 163us/step - loss: 2.6228 - dense_10_loss: 0.4548 - dense_5_loss: 0.1962 - dense_6_loss: 1.2637 - dense_7_loss: 0.7080 - val_loss: 2.4227 - val_dense_10_loss: 0.2867 - val_dense_5_loss: 0.1659 - val_dense_6_loss: 1.2637 - val_dense_7_loss: 0.7064\nEpoch 49/50\n95247/95247 [==============================] - 15s 161us/step - loss: 2.6183 - dense_10_loss: 0.4520 - dense_5_loss: 0.1968 - dense_6_loss: 1.2615 - dense_7_loss: 0.7079 - val_loss: 2.4204 - val_dense_10_loss: 0.2899 - val_dense_5_loss: 0.1663 - val_dense_6_loss: 1.2577 - val_dense_7_loss: 0.7066\nEpoch 50/50\n95247/95247 [==============================] - 16s 171us/step - loss: 2.6390 - dense_10_loss: 0.4744 - dense_5_loss: 0.1969 - dense_6_loss: 1.2598 - dense_7_loss: 0.7080 - val_loss: 2.4153 - val_dense_10_loss: 0.2830 - val_dense_5_loss: 0.1662 - val_dense_6_loss: 1.2595 - val_dense_7_loss: 0.7066\nTrain on 912693 samples, validate on 228174 samples\nEpoch 1/50\n912693/912693 [==============================] - 157s 172us/step - loss: 4.4550 - dense_10_loss: 1.4117 - dense_5_loss: 0.2891 - dense_6_loss: 2.0528 - dense_7_loss: 0.7014 - val_loss: 3.8375 - val_dense_10_loss: 1.0646 - val_dense_5_loss: 0.2254 - val_dense_6_loss: 1.8506 - val_dense_7_loss: 0.6969\nEpoch 2/50\n912693/912693 [==============================] - 153s 168us/step - loss: 3.9570 - dense_10_loss: 1.1454 - dense_5_loss: 0.2481 - dense_6_loss: 1.8674 - dense_7_loss: 0.6961 - val_loss: 3.6255 - val_dense_10_loss: 0.9582 - val_dense_5_loss: 0.2107 - val_dense_6_loss: 1.7600 - val_dense_7_loss: 0.6967\nEpoch 3/50\n912693/912693 [==============================] - 154s 169us/step - loss: 3.8198 - dense_10_loss: 1.0785 - dense_5_loss: 0.2381 - dense_6_loss: 1.8074 - dense_7_loss: 0.6958 - val_loss: 3.5047 - val_dense_10_loss: 0.8882 - val_dense_5_loss: 0.2029 - val_dense_6_loss: 1.7173 - val_dense_7_loss: 0.6964\nEpoch 4/50\n912693/912693 [==============================] - 155s 169us/step - loss: 3.7328 - dense_10_loss: 1.0393 - dense_5_loss: 0.2313 - dense_6_loss: 1.7668 - dense_7_loss: 0.6955 - val_loss: 3.4369 - val_dense_10_loss: 0.8675 - val_dense_5_loss: 0.1965 - val_dense_6_loss: 1.6770 - val_dense_7_loss: 0.6959\nEpoch 5/50\n912693/912693 [==============================] - 154s 169us/step - loss: 3.6693 - dense_10_loss: 1.0079 - dense_5_loss: 0.2267 - dense_6_loss: 1.7395 - dense_7_loss: 0.6953 - val_loss: 3.3556 - val_dense_10_loss: 0.8275 - val_dense_5_loss: 0.1906 - val_dense_6_loss: 1.6413 - val_dense_7_loss: 0.6962\nEpoch 6/50\n912693/912693 [==============================] - 153s 167us/step - loss: 3.6188 - dense_10_loss: 0.9850 - dense_5_loss: 0.2230 - dense_6_loss: 1.7157 - dense_7_loss: 0.6951 - val_loss: 3.3153 - val_dense_10_loss: 0.8194 - val_dense_5_loss: 0.1857 - val_dense_6_loss: 1.6144 - val_dense_7_loss: 0.6958\nEpoch 7/50\n912693/912693 [==============================] - 150s 165us/step - loss: 3.5857 - dense_10_loss: 0.9718 - dense_5_loss: 0.2204 - dense_6_loss: 1.6985 - dense_7_loss: 0.6950 - val_loss: 3.2721 - val_dense_10_loss: 0.7897 - val_dense_5_loss: 0.1816 - val_dense_6_loss: 1.6050 - val_dense_7_loss: 0.6957\nEpoch 8/50\n912693/912693 [==============================] - 150s 164us/step - loss: 3.5593 - dense_10_loss: 0.9619 - dense_5_loss: 0.2185 - dense_6_loss: 1.6841 - dense_7_loss: 0.6948 - val_loss: 3.2582 - val_dense_10_loss: 0.7938 - val_dense_5_loss: 0.1836 - val_dense_6_loss: 1.5852 - val_dense_7_loss: 0.6957\nEpoch 9/50\n912693/912693 [==============================] - 150s 164us/step - loss: 3.5306 - dense_10_loss: 0.9468 - dense_5_loss: 0.2171 - dense_6_loss: 1.6720 - dense_7_loss: 0.6947 - val_loss: 3.2329 - val_dense_10_loss: 0.7761 - val_dense_5_loss: 0.1812 - val_dense_6_loss: 1.5800 - val_dense_7_loss: 0.6956\nEpoch 10/50\n912693/912693 [==============================] - 152s 167us/step - loss: 3.5097 - dense_10_loss: 0.9365 - dense_5_loss: 0.2159 - dense_6_loss: 1.6626 - dense_7_loss: 0.6947 - val_loss: 3.2014 - val_dense_10_loss: 0.7581 - val_dense_5_loss: 0.1813 - val_dense_6_loss: 1.5666 - val_dense_7_loss: 0.6954\nEpoch 11/50\n912693/912693 [==============================] - 152s 166us/step - loss: 3.4924 - dense_10_loss: 0.9299 - dense_5_loss: 0.2147 - dense_6_loss: 1.6532 - dense_7_loss: 0.6946 - val_loss: 3.1967 - val_dense_10_loss: 0.7642 - val_dense_5_loss: 0.1790 - val_dense_6_loss: 1.5582 - val_dense_7_loss: 0.6952\nEpoch 12/50\n912693/912693 [==============================] - 150s 165us/step - loss: 3.4769 - dense_10_loss: 0.9225 - dense_5_loss: 0.2143 - dense_6_loss: 1.6457 - dense_7_loss: 0.6945 - val_loss: 3.1842 - val_dense_10_loss: 0.7608 - val_dense_5_loss: 0.1794 - val_dense_6_loss: 1.5486 - val_dense_7_loss: 0.6953\nEpoch 13/50\n912693/912693 [==============================] - 150s 165us/step - loss: 3.4635 - dense_10_loss: 0.9167 - dense_5_loss: 0.2131 - dense_6_loss: 1.6392 - dense_7_loss: 0.6945 - val_loss: 3.1736 - val_dense_10_loss: 0.7565 - val_dense_5_loss: 0.1776 - val_dense_6_loss: 1.5444 - val_dense_7_loss: 0.6951\nEpoch 14/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.4508 - dense_10_loss: 0.9118 - dense_5_loss: 0.2122 - dense_6_loss: 1.6324 - dense_7_loss: 0.6944 - val_loss: 3.1395 - val_dense_10_loss: 0.7307 - val_dense_5_loss: 0.1771 - val_dense_6_loss: 1.5366 - val_dense_7_loss: 0.6952\nEpoch 15/50\n912693/912693 [==============================] - 148s 163us/step - loss: 3.4407 - dense_10_loss: 0.9074 - dense_5_loss: 0.2118 - dense_6_loss: 1.6271 - dense_7_loss: 0.6944 - val_loss: 3.1332 - val_dense_10_loss: 0.7330 - val_dense_5_loss: 0.1750 - val_dense_6_loss: 1.5296 - val_dense_7_loss: 0.6956\nEpoch 16/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.4263 - dense_10_loss: 0.8993 - dense_5_loss: 0.2108 - dense_6_loss: 1.6219 - dense_7_loss: 0.6943 - val_loss: 3.1180 - val_dense_10_loss: 0.7243 - val_dense_5_loss: 0.1723 - val_dense_6_loss: 1.5260 - val_dense_7_loss: 0.6954\nEpoch 17/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.4164 - dense_10_loss: 0.8946 - dense_5_loss: 0.2102 - dense_6_loss: 1.6173 - dense_7_loss: 0.6943 - val_loss: 3.1270 - val_dense_10_loss: 0.7379 - val_dense_5_loss: 0.1727 - val_dense_6_loss: 1.5214 - val_dense_7_loss: 0.6950\nEpoch 18/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.4110 - dense_10_loss: 0.8945 - dense_5_loss: 0.2096 - dense_6_loss: 1.6127 - dense_7_loss: 0.6942 - val_loss: 3.1125 - val_dense_10_loss: 0.7279 - val_dense_5_loss: 0.1723 - val_dense_6_loss: 1.5171 - val_dense_7_loss: 0.6952\nEpoch 19/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.4039 - dense_10_loss: 0.8911 - dense_5_loss: 0.2095 - dense_6_loss: 1.6091 - dense_7_loss: 0.6942 - val_loss: 3.0907 - val_dense_10_loss: 0.7141 - val_dense_5_loss: 0.1733 - val_dense_6_loss: 1.5085 - val_dense_7_loss: 0.6949\nEpoch 20/50\n912693/912693 [==============================] - 146s 160us/step - loss: 3.3934 - dense_10_loss: 0.8849 - dense_5_loss: 0.2089 - dense_6_loss: 1.6055 - dense_7_loss: 0.6942 - val_loss: 3.1035 - val_dense_10_loss: 0.7288 - val_dense_5_loss: 0.1719 - val_dense_6_loss: 1.5079 - val_dense_7_loss: 0.6949\nEpoch 21/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.3862 - dense_10_loss: 0.8835 - dense_5_loss: 0.2081 - dense_6_loss: 1.6004 - dense_7_loss: 0.6942 - val_loss: 3.0759 - val_dense_10_loss: 0.7055 - val_dense_5_loss: 0.1723 - val_dense_6_loss: 1.5032 - val_dense_7_loss: 0.6949\nEpoch 22/50\n912693/912693 [==============================] - 151s 166us/step - loss: 3.3781 - dense_10_loss: 0.8779 - dense_5_loss: 0.2079 - dense_6_loss: 1.5981 - dense_7_loss: 0.6942 - val_loss: 3.0565 - val_dense_10_loss: 0.6901 - val_dense_5_loss: 0.1720 - val_dense_6_loss: 1.4993 - val_dense_7_loss: 0.6950\nEpoch 23/50\n912693/912693 [==============================] - 147s 162us/step - loss: 3.3734 - dense_10_loss: 0.8765 - dense_5_loss: 0.2074 - dense_6_loss: 1.5954 - dense_7_loss: 0.6942 - val_loss: 3.0543 - val_dense_10_loss: 0.6986 - val_dense_5_loss: 0.1691 - val_dense_6_loss: 1.4918 - val_dense_7_loss: 0.6948\nEpoch 24/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.3649 - dense_10_loss: 0.8721 - dense_5_loss: 0.2071 - dense_6_loss: 1.5915 - dense_7_loss: 0.6941 - val_loss: 3.0723 - val_dense_10_loss: 0.7098 - val_dense_5_loss: 0.1710 - val_dense_6_loss: 1.4967 - val_dense_7_loss: 0.6948\nEpoch 25/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.3614 - dense_10_loss: 0.8727 - dense_5_loss: 0.2067 - dense_6_loss: 1.5879 - dense_7_loss: 0.6941 - val_loss: 3.0424 - val_dense_10_loss: 0.6900 - val_dense_5_loss: 0.1697 - val_dense_6_loss: 1.4880 - val_dense_7_loss: 0.6947\nEpoch 26/50\n912693/912693 [==============================] - 148s 163us/step - loss: 3.3527 - dense_10_loss: 0.8664 - dense_5_loss: 0.2061 - dense_6_loss: 1.5862 - dense_7_loss: 0.6941 - val_loss: 3.0497 - val_dense_10_loss: 0.7026 - val_dense_5_loss: 0.1689 - val_dense_6_loss: 1.4833 - val_dense_7_loss: 0.6949\nEpoch 27/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.3504 - dense_10_loss: 0.8664 - dense_5_loss: 0.2063 - dense_6_loss: 1.5836 - dense_7_loss: 0.6941 - val_loss: 3.0336 - val_dense_10_loss: 0.6868 - val_dense_5_loss: 0.1689 - val_dense_6_loss: 1.4832 - val_dense_7_loss: 0.6947\nEpoch 28/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.3440 - dense_10_loss: 0.8633 - dense_5_loss: 0.2053 - dense_6_loss: 1.5813 - dense_7_loss: 0.6941 - val_loss: 3.0329 - val_dense_10_loss: 0.6852 - val_dense_5_loss: 0.1726 - val_dense_6_loss: 1.4803 - val_dense_7_loss: 0.6949\nEpoch 29/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.3406 - dense_10_loss: 0.8637 - dense_5_loss: 0.2054 - dense_6_loss: 1.5775 - dense_7_loss: 0.6941 - val_loss: 3.0358 - val_dense_10_loss: 0.6926 - val_dense_5_loss: 0.1698 - val_dense_6_loss: 1.4784 - val_dense_7_loss: 0.6951\nEpoch 30/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.3343 - dense_10_loss: 0.8607 - dense_5_loss: 0.2049 - dense_6_loss: 1.5746 - dense_7_loss: 0.6941 - val_loss: 3.0121 - val_dense_10_loss: 0.6762 - val_dense_5_loss: 0.1653 - val_dense_6_loss: 1.4759 - val_dense_7_loss: 0.6948\nEpoch 31/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.3305 - dense_10_loss: 0.8595 - dense_5_loss: 0.2048 - dense_6_loss: 1.5721 - dense_7_loss: 0.6940 - val_loss: 3.0090 - val_dense_10_loss: 0.6809 - val_dense_5_loss: 0.1669 - val_dense_6_loss: 1.4663 - val_dense_7_loss: 0.6948\nEpoch 32/50\n912693/912693 [==============================] - 150s 165us/step - loss: 3.3252 - dense_10_loss: 0.8574 - dense_5_loss: 0.2046 - dense_6_loss: 1.5693 - dense_7_loss: 0.6940 - val_loss: 3.0342 - val_dense_10_loss: 0.6961 - val_dense_5_loss: 0.1694 - val_dense_6_loss: 1.4738 - val_dense_7_loss: 0.6948\nEpoch 33/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.3150 - dense_10_loss: 0.8522 - dense_5_loss: 0.2037 - dense_6_loss: 1.5651 - dense_7_loss: 0.6940 - val_loss: 3.0009 - val_dense_10_loss: 0.6729 - val_dense_5_loss: 0.1661 - val_dense_6_loss: 1.4670 - val_dense_7_loss: 0.6948\nEpoch 34/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.3120 - dense_10_loss: 0.8512 - dense_5_loss: 0.2038 - dense_6_loss: 1.5631 - dense_7_loss: 0.6940 - val_loss: 2.9904 - val_dense_10_loss: 0.6718 - val_dense_5_loss: 0.1655 - val_dense_6_loss: 1.4586 - val_dense_7_loss: 0.6946\nEpoch 35/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.3059 - dense_10_loss: 0.8485 - dense_5_loss: 0.2031 - dense_6_loss: 1.5603 - dense_7_loss: 0.6940 - val_loss: 2.9885 - val_dense_10_loss: 0.6676 - val_dense_5_loss: 0.1667 - val_dense_6_loss: 1.4591 - val_dense_7_loss: 0.6951\nEpoch 36/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.3025 - dense_10_loss: 0.8478 - dense_5_loss: 0.2029 - dense_6_loss: 1.5578 - dense_7_loss: 0.6940 - val_loss: 2.9957 - val_dense_10_loss: 0.6794 - val_dense_5_loss: 0.1663 - val_dense_6_loss: 1.4552 - val_dense_7_loss: 0.6948\nEpoch 37/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.2978 - dense_10_loss: 0.8467 - dense_5_loss: 0.2021 - dense_6_loss: 1.5549 - dense_7_loss: 0.6940 - val_loss: 3.0031 - val_dense_10_loss: 0.6886 - val_dense_5_loss: 0.1653 - val_dense_6_loss: 1.4542 - val_dense_7_loss: 0.6950\nEpoch 38/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.2909 - dense_10_loss: 0.8413 - dense_5_loss: 0.2021 - dense_6_loss: 1.5535 - dense_7_loss: 0.6940 - val_loss: 2.9947 - val_dense_10_loss: 0.6782 - val_dense_5_loss: 0.1640 - val_dense_6_loss: 1.4575 - val_dense_7_loss: 0.6950\nEpoch 39/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.2933 - dense_10_loss: 0.8437 - dense_5_loss: 0.2025 - dense_6_loss: 1.5532 - dense_7_loss: 0.6940 - val_loss: 3.0477 - val_dense_10_loss: 0.6885 - val_dense_5_loss: 0.1724 - val_dense_6_loss: 1.4921 - val_dense_7_loss: 0.6947\n\nEpoch 00039: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 40/50\n912693/912693 [==============================] - 149s 163us/step - loss: 3.2263 - dense_10_loss: 0.8137 - dense_5_loss: 0.1968 - dense_6_loss: 1.5231 - dense_7_loss: 0.6928 - val_loss: 2.9000 - val_dense_10_loss: 0.6290 - val_dense_5_loss: 0.1592 - val_dense_6_loss: 1.4182 - val_dense_7_loss: 0.6936\nEpoch 41/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.2169 - dense_10_loss: 0.8080 - dense_5_loss: 0.1967 - dense_6_loss: 1.5195 - dense_7_loss: 0.6927 - val_loss: 2.9005 - val_dense_10_loss: 0.6297 - val_dense_5_loss: 0.1590 - val_dense_6_loss: 1.4181 - val_dense_7_loss: 0.6936\nEpoch 42/50\n912693/912693 [==============================] - 148s 163us/step - loss: 3.2141 - dense_10_loss: 0.8081 - dense_5_loss: 0.1965 - dense_6_loss: 1.5168 - dense_7_loss: 0.6927 - val_loss: 2.8979 - val_dense_10_loss: 0.6288 - val_dense_5_loss: 0.1598 - val_dense_6_loss: 1.4156 - val_dense_7_loss: 0.6937\nEpoch 43/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.2142 - dense_10_loss: 0.8078 - dense_5_loss: 0.1967 - dense_6_loss: 1.5170 - dense_7_loss: 0.6927 - val_loss: 2.8922 - val_dense_10_loss: 0.6272 - val_dense_5_loss: 0.1580 - val_dense_6_loss: 1.4133 - val_dense_7_loss: 0.6937\nEpoch 44/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.2124 - dense_10_loss: 0.8078 - dense_5_loss: 0.1969 - dense_6_loss: 1.5149 - dense_7_loss: 0.6927 - val_loss: 2.8931 - val_dense_10_loss: 0.6288 - val_dense_5_loss: 0.1578 - val_dense_6_loss: 1.4128 - val_dense_7_loss: 0.6937\nEpoch 45/50\n912693/912693 [==============================] - 147s 162us/step - loss: 3.2052 - dense_10_loss: 0.8027 - dense_5_loss: 0.1959 - dense_6_loss: 1.5139 - dense_7_loss: 0.6927 - val_loss: 2.8857 - val_dense_10_loss: 0.6225 - val_dense_5_loss: 0.1586 - val_dense_6_loss: 1.4108 - val_dense_7_loss: 0.6937\nEpoch 46/50\n912693/912693 [==============================] - 150s 165us/step - loss: 3.2076 - dense_10_loss: 0.8051 - dense_5_loss: 0.1962 - dense_6_loss: 1.5134 - dense_7_loss: 0.6927 - val_loss: 2.8751 - val_dense_10_loss: 0.6147 - val_dense_5_loss: 0.1576 - val_dense_6_loss: 1.4091 - val_dense_7_loss: 0.6937\nEpoch 47/50\n912693/912693 [==============================] - 148s 162us/step - loss: 3.2051 - dense_10_loss: 0.8040 - dense_5_loss: 0.1960 - dense_6_loss: 1.5124 - dense_7_loss: 0.6927 - val_loss: 2.8759 - val_dense_10_loss: 0.6168 - val_dense_5_loss: 0.1567 - val_dense_6_loss: 1.4088 - val_dense_7_loss: 0.6937\nEpoch 48/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.2021 - dense_10_loss: 0.8022 - dense_5_loss: 0.1958 - dense_6_loss: 1.5114 - dense_7_loss: 0.6927 - val_loss: 2.8849 - val_dense_10_loss: 0.6256 - val_dense_5_loss: 0.1576 - val_dense_6_loss: 1.4081 - val_dense_7_loss: 0.6936\nEpoch 49/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.2038 - dense_10_loss: 0.8045 - dense_5_loss: 0.1960 - dense_6_loss: 1.5105 - dense_7_loss: 0.6927 - val_loss: 2.8759 - val_dense_10_loss: 0.6137 - val_dense_5_loss: 0.1589 - val_dense_6_loss: 1.4097 - val_dense_7_loss: 0.6936\nEpoch 50/50\n912693/912693 [==============================] - 147s 161us/step - loss: 3.1992 - dense_10_loss: 0.8006 - dense_5_loss: 0.1957 - dense_6_loss: 1.5101 - dense_7_loss: 0.6927 - val_loss: 2.8725 - val_dense_10_loss: 0.6121 - val_dense_5_loss: 0.1584 - val_dense_6_loss: 1.4083 - val_dense_7_loss: 0.6937\nTrain on 472423 samples, validate on 118106 samples\nEpoch 1/50\n472423/472423 [==============================] - 79s 168us/step - loss: 4.5707 - dense_10_loss: 0.9987 - dense_5_loss: 0.3099 - dense_6_loss: 2.4985 - dense_7_loss: 0.7635 - val_loss: 3.9901 - val_dense_10_loss: 0.6979 - val_dense_5_loss: 0.2414 - val_dense_6_loss: 2.2955 - val_dense_7_loss: 0.7554\nEpoch 2/50\n472423/472423 [==============================] - 77s 162us/step - loss: 4.0190 - dense_10_loss: 0.7390 - dense_5_loss: 0.2514 - dense_6_loss: 2.2742 - dense_7_loss: 0.7544 - val_loss: 3.7106 - val_dense_10_loss: 0.5735 - val_dense_5_loss: 0.2098 - val_dense_6_loss: 2.1723 - val_dense_7_loss: 0.7550\nEpoch 3/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.8704 - dense_10_loss: 0.6877 - dense_5_loss: 0.2382 - dense_6_loss: 2.1907 - dense_7_loss: 0.7540 - val_loss: 3.5524 - val_dense_10_loss: 0.5036 - val_dense_5_loss: 0.2008 - val_dense_6_loss: 2.0940 - val_dense_7_loss: 0.7540\nEpoch 4/50\n472423/472423 [==============================] - 78s 164us/step - loss: 3.7835 - dense_10_loss: 0.6615 - dense_5_loss: 0.2313 - dense_6_loss: 2.1370 - dense_7_loss: 0.7537 - val_loss: 3.5093 - val_dense_10_loss: 0.4919 - val_dense_5_loss: 0.1970 - val_dense_6_loss: 2.0661 - val_dense_7_loss: 0.7544\nEpoch 5/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.7174 - dense_10_loss: 0.6406 - dense_5_loss: 0.2262 - dense_6_loss: 2.0971 - dense_7_loss: 0.7535 - val_loss: 3.4331 - val_dense_10_loss: 0.4756 - val_dense_5_loss: 0.1915 - val_dense_6_loss: 2.0115 - val_dense_7_loss: 0.7545\nEpoch 6/50\n472423/472423 [==============================] - 79s 166us/step - loss: 3.6675 - dense_10_loss: 0.6251 - dense_5_loss: 0.2227 - dense_6_loss: 2.0664 - dense_7_loss: 0.7533 - val_loss: 3.4464 - val_dense_10_loss: 0.5048 - val_dense_5_loss: 0.1888 - val_dense_6_loss: 1.9991 - val_dense_7_loss: 0.7538\nEpoch 7/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.6255 - dense_10_loss: 0.6135 - dense_5_loss: 0.2203 - dense_6_loss: 2.0386 - dense_7_loss: 0.7532 - val_loss: 3.3461 - val_dense_10_loss: 0.4434 - val_dense_5_loss: 0.1854 - val_dense_6_loss: 1.9634 - val_dense_7_loss: 0.7539\nEpoch 8/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.5906 - dense_10_loss: 0.6035 - dense_5_loss: 0.2181 - dense_6_loss: 2.0160 - dense_7_loss: 0.7530 - val_loss: 3.3308 - val_dense_10_loss: 0.4460 - val_dense_5_loss: 0.1847 - val_dense_6_loss: 1.9465 - val_dense_7_loss: 0.7536\nEpoch 9/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.5630 - dense_10_loss: 0.5970 - dense_5_loss: 0.2163 - dense_6_loss: 1.9968 - dense_7_loss: 0.7529 - val_loss: 3.3142 - val_dense_10_loss: 0.4571 - val_dense_5_loss: 0.1808 - val_dense_6_loss: 1.9224 - val_dense_7_loss: 0.7539\nEpoch 10/50\n472423/472423 [==============================] - 78s 165us/step - loss: 3.5348 - dense_10_loss: 0.5876 - dense_5_loss: 0.2142 - dense_6_loss: 1.9803 - dense_7_loss: 0.7528 - val_loss: 3.2561 - val_dense_10_loss: 0.4145 - val_dense_5_loss: 0.1821 - val_dense_6_loss: 1.9059 - val_dense_7_loss: 0.7536\nEpoch 11/50\n472423/472423 [==============================] - 77s 164us/step - loss: 3.5147 - dense_10_loss: 0.5839 - dense_5_loss: 0.2133 - dense_6_loss: 1.9649 - dense_7_loss: 0.7527 - val_loss: 3.2358 - val_dense_10_loss: 0.4254 - val_dense_5_loss: 0.1765 - val_dense_6_loss: 1.8802 - val_dense_7_loss: 0.7537\nEpoch 12/50\n472423/472423 [==============================] - 78s 166us/step - loss: 3.5003 - dense_10_loss: 0.5842 - dense_5_loss: 0.2122 - dense_6_loss: 1.9514 - dense_7_loss: 0.7526 - val_loss: 3.2476 - val_dense_10_loss: 0.4407 - val_dense_5_loss: 0.1745 - val_dense_6_loss: 1.8793 - val_dense_7_loss: 0.7531\nEpoch 13/50\n472423/472423 [==============================] - 78s 164us/step - loss: 3.4846 - dense_10_loss: 0.5798 - dense_5_loss: 0.2110 - dense_6_loss: 1.9412 - dense_7_loss: 0.7525 - val_loss: 3.2301 - val_dense_10_loss: 0.4418 - val_dense_5_loss: 0.1777 - val_dense_6_loss: 1.8577 - val_dense_7_loss: 0.7529\nEpoch 14/50\n472423/472423 [==============================] - 77s 164us/step - loss: 3.4687 - dense_10_loss: 0.5745 - dense_5_loss: 0.2104 - dense_6_loss: 1.9314 - dense_7_loss: 0.7524 - val_loss: 3.2090 - val_dense_10_loss: 0.4247 - val_dense_5_loss: 0.1736 - val_dense_6_loss: 1.8580 - val_dense_7_loss: 0.7527\nEpoch 15/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.4556 - dense_10_loss: 0.5731 - dense_5_loss: 0.2092 - dense_6_loss: 1.9210 - dense_7_loss: 0.7523 - val_loss: 3.1762 - val_dense_10_loss: 0.4116 - val_dense_5_loss: 0.1722 - val_dense_6_loss: 1.8394 - val_dense_7_loss: 0.7530\nEpoch 16/50\n472423/472423 [==============================] - 78s 164us/step - loss: 3.4415 - dense_10_loss: 0.5691 - dense_5_loss: 0.2082 - dense_6_loss: 1.9120 - dense_7_loss: 0.7522 - val_loss: 3.1569 - val_dense_10_loss: 0.3990 - val_dense_5_loss: 0.1753 - val_dense_6_loss: 1.8292 - val_dense_7_loss: 0.7535\nEpoch 17/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.4290 - dense_10_loss: 0.5662 - dense_5_loss: 0.2074 - dense_6_loss: 1.9033 - dense_7_loss: 0.7521 - val_loss: 3.1530 - val_dense_10_loss: 0.3990 - val_dense_5_loss: 0.1728 - val_dense_6_loss: 1.8285 - val_dense_7_loss: 0.7527\nEpoch 18/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.4181 - dense_10_loss: 0.5637 - dense_5_loss: 0.2069 - dense_6_loss: 1.8954 - dense_7_loss: 0.7521 - val_loss: 3.1493 - val_dense_10_loss: 0.4030 - val_dense_5_loss: 0.1721 - val_dense_6_loss: 1.8209 - val_dense_7_loss: 0.7533\nEpoch 19/50\n472423/472423 [==============================] - 78s 164us/step - loss: 3.4075 - dense_10_loss: 0.5595 - dense_5_loss: 0.2066 - dense_6_loss: 1.8894 - dense_7_loss: 0.7520 - val_loss: 3.1389 - val_dense_10_loss: 0.3940 - val_dense_5_loss: 0.1697 - val_dense_6_loss: 1.8226 - val_dense_7_loss: 0.7526\nEpoch 20/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.4035 - dense_10_loss: 0.5618 - dense_5_loss: 0.2058 - dense_6_loss: 1.8839 - dense_7_loss: 0.7520 - val_loss: 3.1254 - val_dense_10_loss: 0.3987 - val_dense_5_loss: 0.1666 - val_dense_6_loss: 1.8072 - val_dense_7_loss: 0.7530\nEpoch 21/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.3931 - dense_10_loss: 0.5584 - dense_5_loss: 0.2050 - dense_6_loss: 1.8778 - dense_7_loss: 0.7519 - val_loss: 3.1134 - val_dense_10_loss: 0.3886 - val_dense_5_loss: 0.1713 - val_dense_6_loss: 1.8008 - val_dense_7_loss: 0.7527\nEpoch 22/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.3853 - dense_10_loss: 0.5554 - dense_5_loss: 0.2046 - dense_6_loss: 1.8734 - dense_7_loss: 0.7519 - val_loss: 3.1039 - val_dense_10_loss: 0.3850 - val_dense_5_loss: 0.1725 - val_dense_6_loss: 1.7931 - val_dense_7_loss: 0.7533\nEpoch 23/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.3776 - dense_10_loss: 0.5550 - dense_5_loss: 0.2044 - dense_6_loss: 1.8664 - dense_7_loss: 0.7519 - val_loss: 3.1350 - val_dense_10_loss: 0.4282 - val_dense_5_loss: 0.1703 - val_dense_6_loss: 1.7834 - val_dense_7_loss: 0.7531\nEpoch 24/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.3692 - dense_10_loss: 0.5517 - dense_5_loss: 0.2041 - dense_6_loss: 1.8616 - dense_7_loss: 0.7518 - val_loss: 3.0950 - val_dense_10_loss: 0.3911 - val_dense_5_loss: 0.1692 - val_dense_6_loss: 1.7818 - val_dense_7_loss: 0.7530\nEpoch 25/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.3604 - dense_10_loss: 0.5492 - dense_5_loss: 0.2032 - dense_6_loss: 1.8562 - dense_7_loss: 0.7518 - val_loss: 3.0936 - val_dense_10_loss: 0.3919 - val_dense_5_loss: 0.1707 - val_dense_6_loss: 1.7785 - val_dense_7_loss: 0.7526\nEpoch 26/50\n472423/472423 [==============================] - 76s 162us/step - loss: 3.3525 - dense_10_loss: 0.5472 - dense_5_loss: 0.2024 - dense_6_loss: 1.8511 - dense_7_loss: 0.7517 - val_loss: 3.1191 - val_dense_10_loss: 0.4256 - val_dense_5_loss: 0.1676 - val_dense_6_loss: 1.7731 - val_dense_7_loss: 0.7528\nEpoch 27/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.3512 - dense_10_loss: 0.5493 - dense_5_loss: 0.2021 - dense_6_loss: 1.8480 - dense_7_loss: 0.7517 - val_loss: 3.0955 - val_dense_10_loss: 0.4027 - val_dense_5_loss: 0.1696 - val_dense_6_loss: 1.7709 - val_dense_7_loss: 0.7523\nEpoch 28/50\n472423/472423 [==============================] - 76s 161us/step - loss: 3.3457 - dense_10_loss: 0.5474 - dense_5_loss: 0.2019 - dense_6_loss: 1.8448 - dense_7_loss: 0.7516 - val_loss: 3.0856 - val_dense_10_loss: 0.3948 - val_dense_5_loss: 0.1669 - val_dense_6_loss: 1.7715 - val_dense_7_loss: 0.7525\nEpoch 29/50\n472423/472423 [==============================] - 76s 162us/step - loss: 3.3383 - dense_10_loss: 0.5452 - dense_5_loss: 0.2018 - dense_6_loss: 1.8396 - dense_7_loss: 0.7516 - val_loss: 3.0932 - val_dense_10_loss: 0.4070 - val_dense_5_loss: 0.1680 - val_dense_6_loss: 1.7656 - val_dense_7_loss: 0.7526\nEpoch 30/50\n472423/472423 [==============================] - 76s 162us/step - loss: 3.3327 - dense_10_loss: 0.5453 - dense_5_loss: 0.2009 - dense_6_loss: 1.8348 - dense_7_loss: 0.7516 - val_loss: 3.0605 - val_dense_10_loss: 0.3798 - val_dense_5_loss: 0.1671 - val_dense_6_loss: 1.7612 - val_dense_7_loss: 0.7524\nEpoch 31/50\n472423/472423 [==============================] - 76s 161us/step - loss: 3.3306 - dense_10_loss: 0.5474 - dense_5_loss: 0.2007 - dense_6_loss: 1.8308 - dense_7_loss: 0.7516 - val_loss: 3.0516 - val_dense_10_loss: 0.3807 - val_dense_5_loss: 0.1649 - val_dense_6_loss: 1.7536 - val_dense_7_loss: 0.7525\nEpoch 32/50\n472423/472423 [==============================] - 76s 162us/step - loss: 3.3243 - dense_10_loss: 0.5440 - dense_5_loss: 0.2004 - dense_6_loss: 1.8284 - dense_7_loss: 0.7516 - val_loss: 3.0660 - val_dense_10_loss: 0.3928 - val_dense_5_loss: 0.1693 - val_dense_6_loss: 1.7513 - val_dense_7_loss: 0.7525\nEpoch 33/50\n472423/472423 [==============================] - 76s 161us/step - loss: 3.3211 - dense_10_loss: 0.5450 - dense_5_loss: 0.2000 - dense_6_loss: 1.8245 - dense_7_loss: 0.7516 - val_loss: 3.0457 - val_dense_10_loss: 0.3768 - val_dense_5_loss: 0.1645 - val_dense_6_loss: 1.7521 - val_dense_7_loss: 0.7523\nEpoch 34/50\n472423/472423 [==============================] - 76s 161us/step - loss: 3.3196 - dense_10_loss: 0.5449 - dense_5_loss: 0.2013 - dense_6_loss: 1.8220 - dense_7_loss: 0.7515 - val_loss: 3.0574 - val_dense_10_loss: 0.3845 - val_dense_5_loss: 0.1650 - val_dense_6_loss: 1.7555 - val_dense_7_loss: 0.7524\nEpoch 35/50\n472423/472423 [==============================] - 76s 161us/step - loss: 3.3173 - dense_10_loss: 0.5442 - dense_5_loss: 0.2001 - dense_6_loss: 1.8215 - dense_7_loss: 0.7515 - val_loss: 3.0467 - val_dense_10_loss: 0.3841 - val_dense_5_loss: 0.1662 - val_dense_6_loss: 1.7441 - val_dense_7_loss: 0.7523\nEpoch 36/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.3077 - dense_10_loss: 0.5404 - dense_5_loss: 0.1997 - dense_6_loss: 1.8160 - dense_7_loss: 0.7515 - val_loss: 3.0808 - val_dense_10_loss: 0.4195 - val_dense_5_loss: 0.1650 - val_dense_6_loss: 1.7438 - val_dense_7_loss: 0.7524\nEpoch 37/50\n472423/472423 [==============================] - 79s 167us/step - loss: 3.3067 - dense_10_loss: 0.5404 - dense_5_loss: 0.2001 - dense_6_loss: 1.8147 - dense_7_loss: 0.7515 - val_loss: 3.0357 - val_dense_10_loss: 0.3743 - val_dense_5_loss: 0.1651 - val_dense_6_loss: 1.7439 - val_dense_7_loss: 0.7524\nEpoch 38/50\n472423/472423 [==============================] - 79s 168us/step - loss: 3.3065 - dense_10_loss: 0.5431 - dense_5_loss: 0.1997 - dense_6_loss: 1.8123 - dense_7_loss: 0.7514 - val_loss: 3.0452 - val_dense_10_loss: 0.3888 - val_dense_5_loss: 0.1669 - val_dense_6_loss: 1.7375 - val_dense_7_loss: 0.7520\nEpoch 39/50\n472423/472423 [==============================] - 78s 164us/step - loss: 3.2938 - dense_10_loss: 0.5362 - dense_5_loss: 0.1997 - dense_6_loss: 1.8065 - dense_7_loss: 0.7513 - val_loss: 3.0271 - val_dense_10_loss: 0.3766 - val_dense_5_loss: 0.1645 - val_dense_6_loss: 1.7336 - val_dense_7_loss: 0.7524\nEpoch 40/50\n472423/472423 [==============================] - 78s 165us/step - loss: 3.2948 - dense_10_loss: 0.5414 - dense_5_loss: 0.1988 - dense_6_loss: 1.8032 - dense_7_loss: 0.7514 - val_loss: 3.0206 - val_dense_10_loss: 0.3712 - val_dense_5_loss: 0.1635 - val_dense_6_loss: 1.7335 - val_dense_7_loss: 0.7523\nEpoch 41/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.2918 - dense_10_loss: 0.5395 - dense_5_loss: 0.1987 - dense_6_loss: 1.8023 - dense_7_loss: 0.7513 - val_loss: 3.0037 - val_dense_10_loss: 0.3644 - val_dense_5_loss: 0.1616 - val_dense_6_loss: 1.7255 - val_dense_7_loss: 0.7523\nEpoch 42/50\n472423/472423 [==============================] - 76s 162us/step - loss: 3.2790 - dense_10_loss: 0.5310 - dense_5_loss: 0.1984 - dense_6_loss: 1.7983 - dense_7_loss: 0.7513 - val_loss: 3.0150 - val_dense_10_loss: 0.3697 - val_dense_5_loss: 0.1636 - val_dense_6_loss: 1.7299 - val_dense_7_loss: 0.7518\nEpoch 43/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.2844 - dense_10_loss: 0.5376 - dense_5_loss: 0.1980 - dense_6_loss: 1.7975 - dense_7_loss: 0.7513 - val_loss: 2.9966 - val_dense_10_loss: 0.3593 - val_dense_5_loss: 0.1629 - val_dense_6_loss: 1.7222 - val_dense_7_loss: 0.7523\nEpoch 44/50\n472423/472423 [==============================] - 77s 164us/step - loss: 3.2770 - dense_10_loss: 0.5312 - dense_5_loss: 0.1985 - dense_6_loss: 1.7959 - dense_7_loss: 0.7513 - val_loss: 3.0192 - val_dense_10_loss: 0.3848 - val_dense_5_loss: 0.1627 - val_dense_6_loss: 1.7194 - val_dense_7_loss: 0.7523\nEpoch 45/50\n472423/472423 [==============================] - 78s 165us/step - loss: 3.2726 - dense_10_loss: 0.5311 - dense_5_loss: 0.1976 - dense_6_loss: 1.7927 - dense_7_loss: 0.7512 - val_loss: 2.9966 - val_dense_10_loss: 0.3630 - val_dense_5_loss: 0.1659 - val_dense_6_loss: 1.7156 - val_dense_7_loss: 0.7521\nEpoch 46/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.2739 - dense_10_loss: 0.5342 - dense_5_loss: 0.1977 - dense_6_loss: 1.7906 - dense_7_loss: 0.7513 - val_loss: 2.9833 - val_dense_10_loss: 0.3562 - val_dense_5_loss: 0.1616 - val_dense_6_loss: 1.7135 - val_dense_7_loss: 0.7520\nEpoch 47/50\n472423/472423 [==============================] - 77s 162us/step - loss: 3.2675 - dense_10_loss: 0.5297 - dense_5_loss: 0.1971 - dense_6_loss: 1.7895 - dense_7_loss: 0.7513 - val_loss: 3.0124 - val_dense_10_loss: 0.3860 - val_dense_5_loss: 0.1606 - val_dense_6_loss: 1.7134 - val_dense_7_loss: 0.7524\nEpoch 48/50\n472423/472423 [==============================] - 76s 162us/step - loss: 3.2714 - dense_10_loss: 0.5343 - dense_5_loss: 0.1969 - dense_6_loss: 1.7891 - dense_7_loss: 0.7512 - val_loss: 3.0189 - val_dense_10_loss: 0.3972 - val_dense_5_loss: 0.1605 - val_dense_6_loss: 1.7089 - val_dense_7_loss: 0.7523\nEpoch 49/50\n472423/472423 [==============================] - 77s 163us/step - loss: 3.2629 - dense_10_loss: 0.5307 - dense_5_loss: 0.1971 - dense_6_loss: 1.7839 - dense_7_loss: 0.7512 - val_loss: 3.0085 - val_dense_10_loss: 0.3816 - val_dense_5_loss: 0.1591 - val_dense_6_loss: 1.7155 - val_dense_7_loss: 0.7522\nEpoch 50/50\n472423/472423 [==============================] - 77s 164us/step - loss: 3.2646 - dense_10_loss: 0.5331 - dense_5_loss: 0.1965 - dense_6_loss: 1.7839 - dense_7_loss: 0.7512 - val_loss: 2.9833 - val_dense_10_loss: 0.3662 - val_dense_5_loss: 0.1608 - val_dense_6_loss: 1.7043 - val_dense_7_loss: 0.7520\n\nEpoch 00050: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nTrain on 1208965 samples, validate on 302242 samples\nEpoch 1/50\n1208965/1208965 [==============================] - 199s 164us/step - loss: 4.4149 - dense_10_loss: 1.2111 - dense_5_loss: 0.3243 - dense_6_loss: 2.1774 - dense_7_loss: 0.7021 - val_loss: 3.9146 - val_dense_10_loss: 0.9636 - val_dense_5_loss: 0.2647 - val_dense_6_loss: 1.9899 - val_dense_7_loss: 0.6964\nEpoch 2/50\n1208965/1208965 [==============================] - 198s 164us/step - loss: 3.9724 - dense_10_loss: 0.9887 - dense_5_loss: 0.2839 - dense_6_loss: 2.0019 - dense_7_loss: 0.6979 - val_loss: 3.7039 - val_dense_10_loss: 0.8657 - val_dense_5_loss: 0.2510 - val_dense_6_loss: 1.8911 - val_dense_7_loss: 0.6960\nEpoch 3/50\n1208965/1208965 [==============================] - 196s 162us/step - loss: 3.8349 - dense_10_loss: 0.9290 - dense_5_loss: 0.2723 - dense_6_loss: 1.9361 - dense_7_loss: 0.6976 - val_loss: 3.5750 - val_dense_10_loss: 0.8100 - val_dense_5_loss: 0.2376 - val_dense_6_loss: 1.8315 - val_dense_7_loss: 0.6959\nEpoch 4/50\n1208965/1208965 [==============================] - 195s 161us/step - loss: 3.7245 - dense_10_loss: 0.8899 - dense_5_loss: 0.2610 - dense_6_loss: 1.8762 - dense_7_loss: 0.6973 - val_loss: 3.4836 - val_dense_10_loss: 0.7818 - val_dense_5_loss: 0.2284 - val_dense_6_loss: 1.7778 - val_dense_7_loss: 0.6955\nEpoch 5/50\n1208965/1208965 [==============================] - 198s 164us/step - loss: 3.6510 - dense_10_loss: 0.8641 - dense_5_loss: 0.2539 - dense_6_loss: 1.8360 - dense_7_loss: 0.6971 - val_loss: 3.4062 - val_dense_10_loss: 0.7518 - val_dense_5_loss: 0.2212 - val_dense_6_loss: 1.7376 - val_dense_7_loss: 0.6956\nEpoch 6/50\n1208965/1208965 [==============================] - 196s 162us/step - loss: 3.6004 - dense_10_loss: 0.8438 - dense_5_loss: 0.2495 - dense_6_loss: 1.8101 - dense_7_loss: 0.6969 - val_loss: 3.3690 - val_dense_10_loss: 0.7335 - val_dense_5_loss: 0.2203 - val_dense_6_loss: 1.7198 - val_dense_7_loss: 0.6954\nEpoch 7/50\n1208965/1208965 [==============================] - 194s 161us/step - loss: 3.5621 - dense_10_loss: 0.8283 - dense_5_loss: 0.2462 - dense_6_loss: 1.7909 - dense_7_loss: 0.6968 - val_loss: 3.3403 - val_dense_10_loss: 0.7295 - val_dense_5_loss: 0.2168 - val_dense_6_loss: 1.6989 - val_dense_7_loss: 0.6951\nEpoch 8/50\n1208965/1208965 [==============================] - 195s 161us/step - loss: 3.5328 - dense_10_loss: 0.8168 - dense_5_loss: 0.2440 - dense_6_loss: 1.7752 - dense_7_loss: 0.6967 - val_loss: 3.3017 - val_dense_10_loss: 0.7146 - val_dense_5_loss: 0.2101 - val_dense_6_loss: 1.6817 - val_dense_7_loss: 0.6954\nEpoch 9/50\n1208965/1208965 [==============================] - 195s 162us/step - loss: 3.5079 - dense_10_loss: 0.8068 - dense_5_loss: 0.2417 - dense_6_loss: 1.7628 - dense_7_loss: 0.6966 - val_loss: 3.2665 - val_dense_10_loss: 0.6978 - val_dense_5_loss: 0.2090 - val_dense_6_loss: 1.6649 - val_dense_7_loss: 0.6948\nEpoch 10/50\n1208965/1208965 [==============================] - 197s 163us/step - loss: 3.4878 - dense_10_loss: 0.7999 - dense_5_loss: 0.2403 - dense_6_loss: 1.7511 - dense_7_loss: 0.6966 - val_loss: 3.2546 - val_dense_10_loss: 0.6887 - val_dense_5_loss: 0.2079 - val_dense_6_loss: 1.6632 - val_dense_7_loss: 0.6949\nEpoch 11/50\n1208965/1208965 [==============================] - 197s 163us/step - loss: 3.4691 - dense_10_loss: 0.7913 - dense_5_loss: 0.2392 - dense_6_loss: 1.7421 - dense_7_loss: 0.6965 - val_loss: 3.2629 - val_dense_10_loss: 0.7091 - val_dense_5_loss: 0.2079 - val_dense_6_loss: 1.6513 - val_dense_7_loss: 0.6947\nEpoch 12/50\n1208965/1208965 [==============================] - 198s 164us/step - loss: 3.4533 - dense_10_loss: 0.7862 - dense_5_loss: 0.2377 - dense_6_loss: 1.7329 - dense_7_loss: 0.6965 - val_loss: 3.2328 - val_dense_10_loss: 0.6857 - val_dense_5_loss: 0.2092 - val_dense_6_loss: 1.6430 - val_dense_7_loss: 0.6949\nEpoch 13/50\n1208965/1208965 [==============================] - 198s 164us/step - loss: 3.4363 - dense_10_loss: 0.7785 - dense_5_loss: 0.2366 - dense_6_loss: 1.7248 - dense_7_loss: 0.6964 - val_loss: 3.2117 - val_dense_10_loss: 0.6748 - val_dense_5_loss: 0.2031 - val_dense_6_loss: 1.6390 - val_dense_7_loss: 0.6949\nEpoch 14/50\n1208965/1208965 [==============================] - 200s 165us/step - loss: 3.4256 - dense_10_loss: 0.7752 - dense_5_loss: 0.2358 - dense_6_loss: 1.7182 - dense_7_loss: 0.6964 - val_loss: 3.1791 - val_dense_10_loss: 0.6558 - val_dense_5_loss: 0.2028 - val_dense_6_loss: 1.6259 - val_dense_7_loss: 0.6946\nEpoch 15/50\n1208965/1208965 [==============================] - 198s 164us/step - loss: 3.4151 - dense_10_loss: 0.7710 - dense_5_loss: 0.2348 - dense_6_loss: 1.7130 - dense_7_loss: 0.6963 - val_loss: 3.1922 - val_dense_10_loss: 0.6668 - val_dense_5_loss: 0.2032 - val_dense_6_loss: 1.6273 - val_dense_7_loss: 0.6948\nEpoch 16/50\n1208965/1208965 [==============================] - 198s 164us/step - loss: 3.4062 - dense_10_loss: 0.7675 - dense_5_loss: 0.2342 - dense_6_loss: 1.7082 - dense_7_loss: 0.6963 - val_loss: 3.1638 - val_dense_10_loss: 0.6566 - val_dense_5_loss: 0.2011 - val_dense_6_loss: 1.6115 - val_dense_7_loss: 0.6946\nEpoch 17/50\n1208965/1208965 [==============================] - 198s 163us/step - loss: 3.3979 - dense_10_loss: 0.7637 - dense_5_loss: 0.2339 - dense_6_loss: 1.7039 - dense_7_loss: 0.6963 - val_loss: 3.1529 - val_dense_10_loss: 0.6481 - val_dense_5_loss: 0.2015 - val_dense_6_loss: 1.6085 - val_dense_7_loss: 0.6948\nEpoch 18/50\n1208965/1208965 [==============================] - 200s 165us/step - loss: 3.3883 - dense_10_loss: 0.7605 - dense_5_loss: 0.2333 - dense_6_loss: 1.6982 - dense_7_loss: 0.6963 - val_loss: 3.1469 - val_dense_10_loss: 0.6540 - val_dense_5_loss: 0.1976 - val_dense_6_loss: 1.6006 - val_dense_7_loss: 0.6947\nEpoch 19/50\n1208965/1208965 [==============================] - 200s 165us/step - loss: 3.3815 - dense_10_loss: 0.7581 - dense_5_loss: 0.2324 - dense_6_loss: 1.6948 - dense_7_loss: 0.6963 - val_loss: 3.1463 - val_dense_10_loss: 0.6387 - val_dense_5_loss: 0.2023 - val_dense_6_loss: 1.6104 - val_dense_7_loss: 0.6948\nEpoch 20/50\n1208965/1208965 [==============================] - 198s 164us/step - loss: 3.3717 - dense_10_loss: 0.7540 - dense_5_loss: 0.2317 - dense_6_loss: 1.6898 - dense_7_loss: 0.6963 - val_loss: 3.1432 - val_dense_10_loss: 0.6539 - val_dense_5_loss: 0.1969 - val_dense_6_loss: 1.5978 - val_dense_7_loss: 0.6946\nEpoch 21/50\n1208965/1208965 [==============================] - 199s 165us/step - loss: 3.3648 - dense_10_loss: 0.7506 - dense_5_loss: 0.2309 - dense_6_loss: 1.6870 - dense_7_loss: 0.6962 - val_loss: 3.1255 - val_dense_10_loss: 0.6435 - val_dense_5_loss: 0.1970 - val_dense_6_loss: 1.5904 - val_dense_7_loss: 0.6945\nEpoch 22/50\n1208965/1208965 [==============================] - 199s 165us/step - loss: 3.3600 - dense_10_loss: 0.7485 - dense_5_loss: 0.2311 - dense_6_loss: 1.6842 - dense_7_loss: 0.6962 - val_loss: 3.1366 - val_dense_10_loss: 0.6459 - val_dense_5_loss: 0.2004 - val_dense_6_loss: 1.5955 - val_dense_7_loss: 0.6949\nEpoch 23/50\n1208965/1208965 [==============================] - 200s 166us/step - loss: 3.3521 - dense_10_loss: 0.7461 - dense_5_loss: 0.2300 - dense_6_loss: 1.6799 - dense_7_loss: 0.6962 - val_loss: 3.1096 - val_dense_10_loss: 0.6383 - val_dense_5_loss: 0.1949 - val_dense_6_loss: 1.5818 - val_dense_7_loss: 0.6945\nEpoch 24/50\n1208965/1208965 [==============================] - 202s 167us/step - loss: 3.3471 - dense_10_loss: 0.7442 - dense_5_loss: 0.2297 - dense_6_loss: 1.6771 - dense_7_loss: 0.6962 - val_loss: 3.1172 - val_dense_10_loss: 0.6380 - val_dense_5_loss: 0.1976 - val_dense_6_loss: 1.5870 - val_dense_7_loss: 0.6947\nEpoch 25/50\n1208965/1208965 [==============================] - 200s 166us/step - loss: 3.3419 - dense_10_loss: 0.7421 - dense_5_loss: 0.2299 - dense_6_loss: 1.6738 - dense_7_loss: 0.6962 - val_loss: 3.1259 - val_dense_10_loss: 0.6391 - val_dense_5_loss: 0.2031 - val_dense_6_loss: 1.5892 - val_dense_7_loss: 0.6945\nEpoch 26/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.3368 - dense_10_loss: 0.7400 - dense_5_loss: 0.2290 - dense_6_loss: 1.6716 - dense_7_loss: 0.6962 - val_loss: 3.1113 - val_dense_10_loss: 0.6354 - val_dense_5_loss: 0.1981 - val_dense_6_loss: 1.5831 - val_dense_7_loss: 0.6947\nEpoch 27/50\n1208965/1208965 [==============================] - 199s 165us/step - loss: 3.3313 - dense_10_loss: 0.7376 - dense_5_loss: 0.2289 - dense_6_loss: 1.6686 - dense_7_loss: 0.6962 - val_loss: 3.1167 - val_dense_10_loss: 0.6351 - val_dense_5_loss: 0.1994 - val_dense_6_loss: 1.5876 - val_dense_7_loss: 0.6946\n\nEpoch 00027: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 28/50\n1208965/1208965 [==============================] - 202s 167us/step - loss: 3.2672 - dense_10_loss: 0.7096 - dense_5_loss: 0.2235 - dense_6_loss: 1.6391 - dense_7_loss: 0.6950 - val_loss: 3.0173 - val_dense_10_loss: 0.5961 - val_dense_5_loss: 0.1895 - val_dense_6_loss: 1.5382 - val_dense_7_loss: 0.6935\nEpoch 29/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2567 - dense_10_loss: 0.7051 - dense_5_loss: 0.2228 - dense_6_loss: 1.6339 - dense_7_loss: 0.6949 - val_loss: 3.0169 - val_dense_10_loss: 0.5979 - val_dense_5_loss: 0.1877 - val_dense_6_loss: 1.5378 - val_dense_7_loss: 0.6935\nEpoch 30/50\n1208965/1208965 [==============================] - 200s 165us/step - loss: 3.2528 - dense_10_loss: 0.7042 - dense_5_loss: 0.2225 - dense_6_loss: 1.6312 - dense_7_loss: 0.6949 - val_loss: 3.0106 - val_dense_10_loss: 0.5921 - val_dense_5_loss: 0.1884 - val_dense_6_loss: 1.5366 - val_dense_7_loss: 0.6934\nEpoch 31/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2507 - dense_10_loss: 0.7025 - dense_5_loss: 0.2226 - dense_6_loss: 1.6306 - dense_7_loss: 0.6949 - val_loss: 3.0073 - val_dense_10_loss: 0.5898 - val_dense_5_loss: 0.1899 - val_dense_6_loss: 1.5342 - val_dense_7_loss: 0.6934\nEpoch 32/50\n1208965/1208965 [==============================] - 205s 170us/step - loss: 3.2487 - dense_10_loss: 0.7017 - dense_5_loss: 0.2224 - dense_6_loss: 1.6296 - dense_7_loss: 0.6949 - val_loss: 3.0021 - val_dense_10_loss: 0.5868 - val_dense_5_loss: 0.1894 - val_dense_6_loss: 1.5325 - val_dense_7_loss: 0.6934\nEpoch 33/50\n1208965/1208965 [==============================] - 202s 167us/step - loss: 3.2451 - dense_10_loss: 0.7005 - dense_5_loss: 0.2221 - dense_6_loss: 1.6276 - dense_7_loss: 0.6949 - val_loss: 2.9921 - val_dense_10_loss: 0.5823 - val_dense_5_loss: 0.1883 - val_dense_6_loss: 1.5280 - val_dense_7_loss: 0.6935\nEpoch 34/50\n1208965/1208965 [==============================] - 202s 167us/step - loss: 3.2457 - dense_10_loss: 0.7016 - dense_5_loss: 0.2220 - dense_6_loss: 1.6272 - dense_7_loss: 0.6950 - val_loss: 2.9979 - val_dense_10_loss: 0.5851 - val_dense_5_loss: 0.1884 - val_dense_6_loss: 1.5309 - val_dense_7_loss: 0.6935\nEpoch 35/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2437 - dense_10_loss: 0.7002 - dense_5_loss: 0.2220 - dense_6_loss: 1.6266 - dense_7_loss: 0.6950 - val_loss: 2.9989 - val_dense_10_loss: 0.5872 - val_dense_5_loss: 0.1879 - val_dense_6_loss: 1.5303 - val_dense_7_loss: 0.6934\nEpoch 36/50\n1208965/1208965 [==============================] - 201s 167us/step - loss: 3.2421 - dense_10_loss: 0.6993 - dense_5_loss: 0.2221 - dense_6_loss: 1.6258 - dense_7_loss: 0.6949 - val_loss: 3.0037 - val_dense_10_loss: 0.5934 - val_dense_5_loss: 0.1878 - val_dense_6_loss: 1.5290 - val_dense_7_loss: 0.6935\nEpoch 37/50\n1208965/1208965 [==============================] - 202s 167us/step - loss: 3.2399 - dense_10_loss: 0.6983 - dense_5_loss: 0.2218 - dense_6_loss: 1.6249 - dense_7_loss: 0.6950 - val_loss: 3.0006 - val_dense_10_loss: 0.5912 - val_dense_5_loss: 0.1893 - val_dense_6_loss: 1.5266 - val_dense_7_loss: 0.6935\n\nEpoch 00037: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\nEpoch 38/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2211 - dense_10_loss: 0.6900 - dense_5_loss: 0.2204 - dense_6_loss: 1.6161 - dense_7_loss: 0.6946 - val_loss: 2.9739 - val_dense_10_loss: 0.5763 - val_dense_5_loss: 0.1864 - val_dense_6_loss: 1.5180 - val_dense_7_loss: 0.6931\nEpoch 39/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2172 - dense_10_loss: 0.6886 - dense_5_loss: 0.2201 - dense_6_loss: 1.6140 - dense_7_loss: 0.6946 - val_loss: 2.9677 - val_dense_10_loss: 0.5720 - val_dense_5_loss: 0.1859 - val_dense_6_loss: 1.5167 - val_dense_7_loss: 0.6931\nEpoch 40/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2166 - dense_10_loss: 0.6888 - dense_5_loss: 0.2202 - dense_6_loss: 1.6131 - dense_7_loss: 0.6945 - val_loss: 2.9724 - val_dense_10_loss: 0.5770 - val_dense_5_loss: 0.1857 - val_dense_6_loss: 1.5166 - val_dense_7_loss: 0.6931\nEpoch 41/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2150 - dense_10_loss: 0.6880 - dense_5_loss: 0.2199 - dense_6_loss: 1.6126 - dense_7_loss: 0.6945 - val_loss: 2.9712 - val_dense_10_loss: 0.5733 - val_dense_5_loss: 0.1872 - val_dense_6_loss: 1.5177 - val_dense_7_loss: 0.6931\nEpoch 42/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2136 - dense_10_loss: 0.6866 - dense_5_loss: 0.2200 - dense_6_loss: 1.6124 - dense_7_loss: 0.6946 - val_loss: 2.9679 - val_dense_10_loss: 0.5742 - val_dense_5_loss: 0.1861 - val_dense_6_loss: 1.5144 - val_dense_7_loss: 0.6931\nEpoch 43/50\n1208965/1208965 [==============================] - 200s 166us/step - loss: 3.2136 - dense_10_loss: 0.6866 - dense_5_loss: 0.2196 - dense_6_loss: 1.6128 - dense_7_loss: 0.6945 - val_loss: 2.9656 - val_dense_10_loss: 0.5723 - val_dense_5_loss: 0.1862 - val_dense_6_loss: 1.5140 - val_dense_7_loss: 0.6931\nEpoch 44/50\n1208965/1208965 [==============================] - 202s 167us/step - loss: 3.2146 - dense_10_loss: 0.6878 - dense_5_loss: 0.2202 - dense_6_loss: 1.6121 - dense_7_loss: 0.6945 - val_loss: 2.9715 - val_dense_10_loss: 0.5778 - val_dense_5_loss: 0.1858 - val_dense_6_loss: 1.5149 - val_dense_7_loss: 0.6931\nEpoch 45/50\n1208965/1208965 [==============================] - 200s 166us/step - loss: 3.2136 - dense_10_loss: 0.6876 - dense_5_loss: 0.2200 - dense_6_loss: 1.6115 - dense_7_loss: 0.6946 - val_loss: 2.9694 - val_dense_10_loss: 0.5737 - val_dense_5_loss: 0.1856 - val_dense_6_loss: 1.5170 - val_dense_7_loss: 0.6931\nEpoch 46/50\n1208965/1208965 [==============================] - 199s 165us/step - loss: 3.2113 - dense_10_loss: 0.6858 - dense_5_loss: 0.2201 - dense_6_loss: 1.6109 - dense_7_loss: 0.6945 - val_loss: 2.9630 - val_dense_10_loss: 0.5712 - val_dense_5_loss: 0.1856 - val_dense_6_loss: 1.5131 - val_dense_7_loss: 0.6931\nEpoch 47/50\n1208965/1208965 [==============================] - 200s 166us/step - loss: 3.2134 - dense_10_loss: 0.6878 - dense_5_loss: 0.2198 - dense_6_loss: 1.6113 - dense_7_loss: 0.6946 - val_loss: 2.9669 - val_dense_10_loss: 0.5736 - val_dense_5_loss: 0.1858 - val_dense_6_loss: 1.5144 - val_dense_7_loss: 0.6931\nEpoch 48/50\n1208965/1208965 [==============================] - 199s 165us/step - loss: 3.2108 - dense_10_loss: 0.6859 - dense_5_loss: 0.2199 - dense_6_loss: 1.6105 - dense_7_loss: 0.6946 - val_loss: 2.9658 - val_dense_10_loss: 0.5740 - val_dense_5_loss: 0.1854 - val_dense_6_loss: 1.5133 - val_dense_7_loss: 0.6931\nEpoch 49/50\n1208965/1208965 [==============================] - 201s 166us/step - loss: 3.2109 - dense_10_loss: 0.6853 - dense_5_loss: 0.2199 - dense_6_loss: 1.6111 - dense_7_loss: 0.6945 - val_loss: 2.9638 - val_dense_10_loss: 0.5727 - val_dense_5_loss: 0.1856 - val_dense_6_loss: 1.5124 - val_dense_7_loss: 0.6931\nEpoch 50/50\n1208965/1208965 [==============================] - 202s 167us/step - loss: 3.2115 - dense_10_loss: 0.6867 - dense_5_loss: 0.2199 - dense_6_loss: 1.6104 - dense_7_loss: 0.6946 - val_loss: 2.9656 - val_dense_10_loss: 0.5727 - val_dense_5_loss: 0.1860 - val_dense_6_loss: 1.5138 - val_dense_7_loss: 0.6931\n\nEpoch 00050: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\nTrain on 133290 samples, validate on 33323 samples\nEpoch 1/50\n133290/133290 [==============================] - 24s 181us/step - loss: 3.9051 - dense_10_loss: 0.6110 - dense_5_loss: 0.3860 - dense_6_loss: 2.1763 - dense_7_loss: 0.7318 - val_loss: 3.4147 - val_dense_10_loss: 0.4713 - val_dense_5_loss: 0.3002 - val_dense_6_loss: 1.9429 - val_dense_7_loss: 0.7002\nEpoch 2/50\n133290/133290 [==============================] - 22s 162us/step - loss: 3.4118 - dense_10_loss: 0.4692 - dense_5_loss: 0.3097 - dense_6_loss: 1.9314 - dense_7_loss: 0.7015 - val_loss: 3.1655 - val_dense_10_loss: 0.3984 - val_dense_5_loss: 0.2689 - val_dense_6_loss: 1.8017 - val_dense_7_loss: 0.6965\nEpoch 3/50\n133290/133290 [==============================] - 22s 164us/step - loss: 3.2316 - dense_10_loss: 0.4164 - dense_5_loss: 0.2892 - dense_6_loss: 1.8269 - dense_7_loss: 0.6992 - val_loss: 3.0409 - val_dense_10_loss: 0.3593 - val_dense_5_loss: 0.2554 - val_dense_6_loss: 1.7318 - val_dense_7_loss: 0.6943\nEpoch 4/50\n133290/133290 [==============================] - 22s 163us/step - loss: 3.1200 - dense_10_loss: 0.3867 - dense_5_loss: 0.2754 - dense_6_loss: 1.7592 - dense_7_loss: 0.6987 - val_loss: 2.9427 - val_dense_10_loss: 0.3372 - val_dense_5_loss: 0.2395 - val_dense_6_loss: 1.6705 - val_dense_7_loss: 0.6954\nEpoch 5/50\n133290/133290 [==============================] - 22s 163us/step - loss: 3.0508 - dense_10_loss: 0.3716 - dense_5_loss: 0.2670 - dense_6_loss: 1.7138 - dense_7_loss: 0.6984 - val_loss: 2.8804 - val_dense_10_loss: 0.3198 - val_dense_5_loss: 0.2315 - val_dense_6_loss: 1.6338 - val_dense_7_loss: 0.6952\nEpoch 6/50\n133290/133290 [==============================] - 22s 168us/step - loss: 2.9953 - dense_10_loss: 0.3577 - dense_5_loss: 0.2599 - dense_6_loss: 1.6793 - dense_7_loss: 0.6984 - val_loss: 2.8193 - val_dense_10_loss: 0.2982 - val_dense_5_loss: 0.2299 - val_dense_6_loss: 1.5969 - val_dense_7_loss: 0.6943\nEpoch 7/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.9527 - dense_10_loss: 0.3469 - dense_5_loss: 0.2556 - dense_6_loss: 1.6520 - dense_7_loss: 0.6982 - val_loss: 2.7877 - val_dense_10_loss: 0.2944 - val_dense_5_loss: 0.2266 - val_dense_6_loss: 1.5717 - val_dense_7_loss: 0.6949\nEpoch 8/50\n133290/133290 [==============================] - 22s 165us/step - loss: 2.9105 - dense_10_loss: 0.3379 - dense_5_loss: 0.2510 - dense_6_loss: 1.6236 - dense_7_loss: 0.6980 - val_loss: 2.7608 - val_dense_10_loss: 0.2910 - val_dense_5_loss: 0.2177 - val_dense_6_loss: 1.5573 - val_dense_7_loss: 0.6948\nEpoch 9/50\n133290/133290 [==============================] - 22s 162us/step - loss: 2.8847 - dense_10_loss: 0.3312 - dense_5_loss: 0.2477 - dense_6_loss: 1.6077 - dense_7_loss: 0.6980 - val_loss: 2.7333 - val_dense_10_loss: 0.2817 - val_dense_5_loss: 0.2167 - val_dense_6_loss: 1.5392 - val_dense_7_loss: 0.6956\nEpoch 10/50\n133290/133290 [==============================] - 22s 164us/step - loss: 2.8587 - dense_10_loss: 0.3255 - dense_5_loss: 0.2452 - dense_6_loss: 1.5901 - dense_7_loss: 0.6979 - val_loss: 2.7165 - val_dense_10_loss: 0.2793 - val_dense_5_loss: 0.2134 - val_dense_6_loss: 1.5292 - val_dense_7_loss: 0.6946\nEpoch 11/50\n133290/133290 [==============================] - 22s 164us/step - loss: 2.8346 - dense_10_loss: 0.3208 - dense_5_loss: 0.2423 - dense_6_loss: 1.5737 - dense_7_loss: 0.6978 - val_loss: 2.6804 - val_dense_10_loss: 0.2701 - val_dense_5_loss: 0.2138 - val_dense_6_loss: 1.5023 - val_dense_7_loss: 0.6942\nEpoch 12/50\n133290/133290 [==============================] - 22s 164us/step - loss: 2.8180 - dense_10_loss: 0.3196 - dense_5_loss: 0.2409 - dense_6_loss: 1.5599 - dense_7_loss: 0.6977 - val_loss: 2.6699 - val_dense_10_loss: 0.2696 - val_dense_5_loss: 0.2097 - val_dense_6_loss: 1.4960 - val_dense_7_loss: 0.6945\nEpoch 13/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.7993 - dense_10_loss: 0.3141 - dense_5_loss: 0.2398 - dense_6_loss: 1.5477 - dense_7_loss: 0.6977 - val_loss: 2.6445 - val_dense_10_loss: 0.2647 - val_dense_5_loss: 0.2035 - val_dense_6_loss: 1.4814 - val_dense_7_loss: 0.6949\nEpoch 14/50\n133290/133290 [==============================] - 22s 165us/step - loss: 2.7818 - dense_10_loss: 0.3102 - dense_5_loss: 0.2373 - dense_6_loss: 1.5367 - dense_7_loss: 0.6976 - val_loss: 2.6425 - val_dense_10_loss: 0.2668 - val_dense_5_loss: 0.2075 - val_dense_6_loss: 1.4741 - val_dense_7_loss: 0.6941\nEpoch 15/50\n133290/133290 [==============================] - 22s 162us/step - loss: 2.7628 - dense_10_loss: 0.3042 - dense_5_loss: 0.2351 - dense_6_loss: 1.5260 - dense_7_loss: 0.6975 - val_loss: 2.6392 - val_dense_10_loss: 0.2612 - val_dense_5_loss: 0.2076 - val_dense_6_loss: 1.4765 - val_dense_7_loss: 0.6939\nEpoch 16/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.7508 - dense_10_loss: 0.3033 - dense_5_loss: 0.2333 - dense_6_loss: 1.5167 - dense_7_loss: 0.6975 - val_loss: 2.6121 - val_dense_10_loss: 0.2548 - val_dense_5_loss: 0.2053 - val_dense_6_loss: 1.4579 - val_dense_7_loss: 0.6939\nEpoch 17/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.7392 - dense_10_loss: 0.3006 - dense_5_loss: 0.2327 - dense_6_loss: 1.5084 - dense_7_loss: 0.6975 - val_loss: 2.6058 - val_dense_10_loss: 0.2599 - val_dense_5_loss: 0.2002 - val_dense_6_loss: 1.4516 - val_dense_7_loss: 0.6940\nEpoch 18/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.7279 - dense_10_loss: 0.2993 - dense_5_loss: 0.2318 - dense_6_loss: 1.4994 - dense_7_loss: 0.6974 - val_loss: 2.6000 - val_dense_10_loss: 0.2568 - val_dense_5_loss: 0.1983 - val_dense_6_loss: 1.4508 - val_dense_7_loss: 0.6941\nEpoch 19/50\n133290/133290 [==============================] - 21s 160us/step - loss: 2.7175 - dense_10_loss: 0.2973 - dense_5_loss: 0.2305 - dense_6_loss: 1.4925 - dense_7_loss: 0.6973 - val_loss: 2.6001 - val_dense_10_loss: 0.2631 - val_dense_5_loss: 0.2017 - val_dense_6_loss: 1.4415 - val_dense_7_loss: 0.6939\nEpoch 20/50\n133290/133290 [==============================] - 22s 162us/step - loss: 2.7055 - dense_10_loss: 0.2940 - dense_5_loss: 0.2292 - dense_6_loss: 1.4851 - dense_7_loss: 0.6972 - val_loss: 2.5709 - val_dense_10_loss: 0.2509 - val_dense_5_loss: 0.1953 - val_dense_6_loss: 1.4304 - val_dense_7_loss: 0.6943\nEpoch 21/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.6982 - dense_10_loss: 0.2920 - dense_5_loss: 0.2285 - dense_6_loss: 1.4805 - dense_7_loss: 0.6972 - val_loss: 2.5703 - val_dense_10_loss: 0.2483 - val_dense_5_loss: 0.1989 - val_dense_6_loss: 1.4295 - val_dense_7_loss: 0.6936\nEpoch 22/50\n133290/133290 [==============================] - 22s 161us/step - loss: 2.6852 - dense_10_loss: 0.2900 - dense_5_loss: 0.2264 - dense_6_loss: 1.4716 - dense_7_loss: 0.6971 - val_loss: 2.5585 - val_dense_10_loss: 0.2457 - val_dense_5_loss: 0.1971 - val_dense_6_loss: 1.4222 - val_dense_7_loss: 0.6934\nEpoch 23/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.6818 - dense_10_loss: 0.2913 - dense_5_loss: 0.2275 - dense_6_loss: 1.4659 - dense_7_loss: 0.6971 - val_loss: 2.5473 - val_dense_10_loss: 0.2451 - val_dense_5_loss: 0.1958 - val_dense_6_loss: 1.4127 - val_dense_7_loss: 0.6937\nEpoch 24/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.6736 - dense_10_loss: 0.2885 - dense_5_loss: 0.2256 - dense_6_loss: 1.4625 - dense_7_loss: 0.6970 - val_loss: 2.5486 - val_dense_10_loss: 0.2481 - val_dense_5_loss: 0.1948 - val_dense_6_loss: 1.4121 - val_dense_7_loss: 0.6936\nEpoch 25/50\n133290/133290 [==============================] - 22s 162us/step - loss: 2.6630 - dense_10_loss: 0.2868 - dense_5_loss: 0.2253 - dense_6_loss: 1.4539 - dense_7_loss: 0.6970 - val_loss: 2.5470 - val_dense_10_loss: 0.2422 - val_dense_5_loss: 0.1959 - val_dense_6_loss: 1.4151 - val_dense_7_loss: 0.6938\nEpoch 26/50\n133290/133290 [==============================] - 22s 164us/step - loss: 2.6598 - dense_10_loss: 0.2869 - dense_5_loss: 0.2237 - dense_6_loss: 1.4523 - dense_7_loss: 0.6969 - val_loss: 2.5321 - val_dense_10_loss: 0.2438 - val_dense_5_loss: 0.1941 - val_dense_6_loss: 1.4002 - val_dense_7_loss: 0.6939\nEpoch 27/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.6533 - dense_10_loss: 0.2849 - dense_5_loss: 0.2232 - dense_6_loss: 1.4484 - dense_7_loss: 0.6968 - val_loss: 2.5374 - val_dense_10_loss: 0.2402 - val_dense_5_loss: 0.1941 - val_dense_6_loss: 1.4086 - val_dense_7_loss: 0.6945\nEpoch 28/50\n133290/133290 [==============================] - 25s 189us/step - loss: 2.6428 - dense_10_loss: 0.2826 - dense_5_loss: 0.2222 - dense_6_loss: 1.4412 - dense_7_loss: 0.6968 - val_loss: 2.5357 - val_dense_10_loss: 0.2411 - val_dense_5_loss: 0.1916 - val_dense_6_loss: 1.4098 - val_dense_7_loss: 0.6932\nEpoch 29/50\n133290/133290 [==============================] - 22s 165us/step - loss: 2.6373 - dense_10_loss: 0.2824 - dense_5_loss: 0.2215 - dense_6_loss: 1.4367 - dense_7_loss: 0.6967 - val_loss: 2.5162 - val_dense_10_loss: 0.2358 - val_dense_5_loss: 0.1907 - val_dense_6_loss: 1.3954 - val_dense_7_loss: 0.6943\nEpoch 30/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.6280 - dense_10_loss: 0.2804 - dense_5_loss: 0.2206 - dense_6_loss: 1.4302 - dense_7_loss: 0.6967 - val_loss: 2.5055 - val_dense_10_loss: 0.2354 - val_dense_5_loss: 0.1886 - val_dense_6_loss: 1.3877 - val_dense_7_loss: 0.6938\nEpoch 31/50\n133290/133290 [==============================] - 22s 165us/step - loss: 2.6281 - dense_10_loss: 0.2820 - dense_5_loss: 0.2200 - dense_6_loss: 1.4294 - dense_7_loss: 0.6967 - val_loss: 2.5073 - val_dense_10_loss: 0.2400 - val_dense_5_loss: 0.1877 - val_dense_6_loss: 1.3858 - val_dense_7_loss: 0.6938\nEpoch 32/50\n133290/133290 [==============================] - 22s 163us/step - loss: 2.6202 - dense_10_loss: 0.2782 - dense_5_loss: 0.2205 - dense_6_loss: 1.4249 - dense_7_loss: 0.6966 - val_loss: 2.5021 - val_dense_10_loss: 0.2402 - val_dense_5_loss: 0.1856 - val_dense_6_loss: 1.3833 - val_dense_7_loss: 0.6930\nEpoch 33/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.6156 - dense_10_loss: 0.2778 - dense_5_loss: 0.2199 - dense_6_loss: 1.4213 - dense_7_loss: 0.6966 - val_loss: 2.4909 - val_dense_10_loss: 0.2326 - val_dense_5_loss: 0.1883 - val_dense_6_loss: 1.3762 - val_dense_7_loss: 0.6938\nEpoch 34/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.6141 - dense_10_loss: 0.2779 - dense_5_loss: 0.2194 - dense_6_loss: 1.4203 - dense_7_loss: 0.6965 - val_loss: 2.4926 - val_dense_10_loss: 0.2273 - val_dense_5_loss: 0.1912 - val_dense_6_loss: 1.3800 - val_dense_7_loss: 0.6941\nEpoch 35/50\n133290/133290 [==============================] - 22s 167us/step - loss: 2.6072 - dense_10_loss: 0.2789 - dense_5_loss: 0.2185 - dense_6_loss: 1.4132 - dense_7_loss: 0.6965 - val_loss: 2.4952 - val_dense_10_loss: 0.2317 - val_dense_5_loss: 0.1905 - val_dense_6_loss: 1.3794 - val_dense_7_loss: 0.6936\nEpoch 36/50\n133290/133290 [==============================] - 22s 168us/step - loss: 2.6043 - dense_10_loss: 0.2756 - dense_5_loss: 0.2183 - dense_6_loss: 1.4139 - dense_7_loss: 0.6965 - val_loss: 2.4788 - val_dense_10_loss: 0.2286 - val_dense_5_loss: 0.1881 - val_dense_6_loss: 1.3692 - val_dense_7_loss: 0.6929\nEpoch 37/50\n133290/133290 [==============================] - 22s 168us/step - loss: 2.5953 - dense_10_loss: 0.2733 - dense_5_loss: 0.2176 - dense_6_loss: 1.4079 - dense_7_loss: 0.6965 - val_loss: 2.4829 - val_dense_10_loss: 0.2300 - val_dense_5_loss: 0.1878 - val_dense_6_loss: 1.3723 - val_dense_7_loss: 0.6929\nEpoch 38/50\n133290/133290 [==============================] - 22s 168us/step - loss: 2.5948 - dense_10_loss: 0.2735 - dense_5_loss: 0.2178 - dense_6_loss: 1.4070 - dense_7_loss: 0.6964 - val_loss: 2.4816 - val_dense_10_loss: 0.2268 - val_dense_5_loss: 0.1884 - val_dense_6_loss: 1.3730 - val_dense_7_loss: 0.6933\nEpoch 39/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.5860 - dense_10_loss: 0.2714 - dense_5_loss: 0.2165 - dense_6_loss: 1.4018 - dense_7_loss: 0.6963 - val_loss: 2.4734 - val_dense_10_loss: 0.2272 - val_dense_5_loss: 0.1871 - val_dense_6_loss: 1.3652 - val_dense_7_loss: 0.6938\nEpoch 40/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.5877 - dense_10_loss: 0.2745 - dense_5_loss: 0.2168 - dense_6_loss: 1.3999 - dense_7_loss: 0.6964 - val_loss: 2.4743 - val_dense_10_loss: 0.2254 - val_dense_5_loss: 0.1872 - val_dense_6_loss: 1.3688 - val_dense_7_loss: 0.6929\nEpoch 41/50\n133290/133290 [==============================] - 22s 167us/step - loss: 2.5820 - dense_10_loss: 0.2721 - dense_5_loss: 0.2162 - dense_6_loss: 1.3974 - dense_7_loss: 0.6963 - val_loss: 2.4634 - val_dense_10_loss: 0.2221 - val_dense_5_loss: 0.1858 - val_dense_6_loss: 1.3620 - val_dense_7_loss: 0.6935\nEpoch 42/50\n133290/133290 [==============================] - 22s 167us/step - loss: 2.5781 - dense_10_loss: 0.2714 - dense_5_loss: 0.2164 - dense_6_loss: 1.3941 - dense_7_loss: 0.6963 - val_loss: 2.4831 - val_dense_10_loss: 0.2301 - val_dense_5_loss: 0.1885 - val_dense_6_loss: 1.3714 - val_dense_7_loss: 0.6930\nEpoch 43/50\n133290/133290 [==============================] - 22s 165us/step - loss: 2.5715 - dense_10_loss: 0.2685 - dense_5_loss: 0.2153 - dense_6_loss: 1.3913 - dense_7_loss: 0.6963 - val_loss: 2.4632 - val_dense_10_loss: 0.2290 - val_dense_5_loss: 0.1846 - val_dense_6_loss: 1.3563 - val_dense_7_loss: 0.6933\nEpoch 44/50\n133290/133290 [==============================] - 22s 167us/step - loss: 2.5718 - dense_10_loss: 0.2694 - dense_5_loss: 0.2152 - dense_6_loss: 1.3911 - dense_7_loss: 0.6962 - val_loss: 2.4681 - val_dense_10_loss: 0.2272 - val_dense_5_loss: 0.1853 - val_dense_6_loss: 1.3624 - val_dense_7_loss: 0.6931\nEpoch 45/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.5663 - dense_10_loss: 0.2688 - dense_5_loss: 0.2144 - dense_6_loss: 1.3870 - dense_7_loss: 0.6962 - val_loss: 2.4545 - val_dense_10_loss: 0.2291 - val_dense_5_loss: 0.1817 - val_dense_6_loss: 1.3510 - val_dense_7_loss: 0.6928\nEpoch 46/50\n133290/133290 [==============================] - 22s 165us/step - loss: 2.5623 - dense_10_loss: 0.2663 - dense_5_loss: 0.2147 - dense_6_loss: 1.3851 - dense_7_loss: 0.6962 - val_loss: 2.4469 - val_dense_10_loss: 0.2223 - val_dense_5_loss: 0.1855 - val_dense_6_loss: 1.3460 - val_dense_7_loss: 0.6931\nEpoch 47/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.5574 - dense_10_loss: 0.2654 - dense_5_loss: 0.2145 - dense_6_loss: 1.3814 - dense_7_loss: 0.6961 - val_loss: 2.4508 - val_dense_10_loss: 0.2230 - val_dense_5_loss: 0.1827 - val_dense_6_loss: 1.3522 - val_dense_7_loss: 0.6929\nEpoch 48/50\n133290/133290 [==============================] - 22s 166us/step - loss: 2.5594 - dense_10_loss: 0.2663 - dense_5_loss: 0.2153 - dense_6_loss: 1.3817 - dense_7_loss: 0.6961 - val_loss: 2.4418 - val_dense_10_loss: 0.2201 - val_dense_5_loss: 0.1812 - val_dense_6_loss: 1.3475 - val_dense_7_loss: 0.6930\nEpoch 49/50\n133290/133290 [==============================] - 22s 165us/step - loss: 2.5513 - dense_10_loss: 0.2665 - dense_5_loss: 0.2133 - dense_6_loss: 1.3754 - dense_7_loss: 0.6961 - val_loss: 2.4504 - val_dense_10_loss: 0.2277 - val_dense_5_loss: 0.1853 - val_dense_6_loss: 1.3444 - val_dense_7_loss: 0.6930\nEpoch 50/50\n133290/133290 [==============================] - 23s 171us/step - loss: 2.5519 - dense_10_loss: 0.2670 - dense_5_loss: 0.2136 - dense_6_loss: 1.3752 - dense_7_loss: 0.6961 - val_loss: 2.4489 - val_dense_10_loss: 0.2203 - val_dense_5_loss: 0.1845 - val_dense_6_loss: 1.3510 - val_dense_7_loss: 0.6931\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for mol_type in mol_types: \n",
        "    print(mol_type,\": cv score is \",cv_score[i])\n",
        "    i+=1\n",
        "print(\"total cv score is\",cv_score_total)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-14T16:24:05.176499Z",
          "iopub.execute_input": "2022-12-14T16:24:05.176917Z",
          "iopub.status.idle": "2022-12-14T16:24:05.187017Z",
          "shell.execute_reply.started": "2022-12-14T16:24:05.176840Z",
          "shell.execute_reply": "2022-12-14T16:24:05.185570Z"
        },
        "trusted": true,
        "id": "YzPyzEZDHN0E",
        "outputId": "d338d94e-4fe7-4468-86d2-53eab0e52b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1JHC : cv score is  0.6838147189441642\n2JHH : cv score is  -1.075825742098381\n1JHN : cv score is  -0.09774994985601333\n2JHN : cv score is  -1.2623536947927336\n2JHC : cv score is  -0.4908582351742244\n3JHH : cv score is  -1.0044649354273885\n3JHC : cv score is  -0.5574494767914225\n3JHN : cv score is  -1.5126281923627023\ntotal cv score is -0.6646894384448376\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}